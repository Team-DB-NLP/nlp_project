[
 {
  "repo": "AmitXShukla/Healthcare-Management-App-Flutter_Firebase",
  "language": "Dart",
  "readme_contents": "# Flutter FireBase Healthcare Management App\nComplete Healthcare Management (Patient, OPD, IPD, Rx, Lab) in Flutter Firebase App for iOS Android and Web\n\n```diff\n- If you like this project, please consider giving it a star (*) and follow me at GitHub & YouTube.\n```\n[<img src=\"https://github.com/AmitXShukla/AmitXShukla.github.io/blob/master/assets/icons/youtube.svg\" width=40 height=50>](https://youtube.com/AmitShukla_AI)\n[<img src=\"https://github.com/AmitXShukla/AmitXShukla.github.io/blob/master/assets/icons/github.svg\" width=40 height=50>](https://github.com/AmitXShukla)\n[<img src=\"https://github.com/AmitXShukla/AmitXShukla.github.io/blob/master/assets/icons/medium.svg\" width=40 height=50>](https://medium.com/@Amit_Shukla)\n[<img src=\"https://github.com/AmitXShukla/AmitXShukla.github.io/blob/master/assets/icons/twitter_1.svg\" width=40 height=50>](https://twitter.com/ashuklax)\n\n# Elish HMS\n\nElish Healthcare Management System App\n\n## Objective \nManage OPD, IPD, Pathology, WebMD, Rx, Patient Appointments<br/><br/>\n\n<i>Due to current Covid-19 situation,<br/>\nPatient's private data is not stored in app and location tracing functionality is not available with out government/authorities approval.</i>\n## Getting Started\n\nThis project is a community version and is absolutely free for private use.<br/>\n<a href=\"https://www.youtube.com/playlist?list=PLp0TENYyY8lHcc8mZiYG83sbsCea2xd3r\">click here for Demo & Video tutorials</a>\n\n## Technologies\n```sbtshell)\nFrontend: Flutter\nBackend:Google Firestore/Firebase\nMessages: LOOM SDK\nWebView: loom-app (using Angular version\n``` \n\n## Related Apps\n<ul>\n<li><a href=\"https://getcovidvaccine.web.app/\">Vaccine Distribution App</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=MkV413X2Kmw&list=PLp0TENYyY8lHL-G7jGbhpJBhVb2UdTOhQ&index=1&t=698s\">Pandemic Contact Tracing, Visitor Management, Mobile Assets/Employee Attendance App</a></li>\n</ul>\n\n## Features\n<ul>\n<li>Store millions of records with lightening fast data retrieval</li>\n<li>hands free /voice activated typing</li>\n<li>Secured App (Role based access with Admin panel)</li>\n<li>Local dictionary based auto-completion</li>\n<li>Global dictionary based auto-completion/auto-sync (Pro)</li>\n<li>GBs of pictures, documents, Lab reports, Receipts (Pro)</li>\n<li>Self learning (auto complete) data entry (Pro)</li>\n<li>Social authentication (Pro)</li>\n<li>SMS, EMAIL, WhatsAPP API (Pro)</li>\n</ul>\n<i>send email to info@elishcosulting.com for Pro version enquiries.</i>\n\n## Product Images\n\n![Pic 1](./images/hms_pic_1.png)\n![Pic 2](./images/hms_pic_2.png)\n![Pic 3](./images/hms_pic_3.png)\n![Pic 4](./images/hms_pic_4.png)\n\n\n\n## How to Install\n\n<ul>\n    <li>Install Flutter environment</li>\n    <li>Download This GitHub repository</li>\n    <li>install Flutter packages *pub get) and Flutter web -> Flutter create .</li>\n    <li>Setup firebase account/project</li>\n    <li>Copy Firebase Project Config settings and replace variable firebaseConfig at src/web/index.html</li>\n    <li>enable Firebase social authentications</li>\n    <li>update Firebase Rules</li>\n\n```sbtshell\n    rules_version = '2';\n    service cloud.firestore {\n    match /databases/{database}/documents {\n    match /{document=**} {\n      allow read, write: if false;\n    }\n    match /roles/{document} {\n    // fix this, anyone who is logged in, can read these document & passwords\n    //  allow read: if isSignedIn();\n  \tallow read, write: if false;\n    }\n    \n    match /users/{document} {\n    allow create: if true;\n    allow read : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow update: if isSignedIn() && isDocOwner() && onlyContentChanged();\n    allow update, delete: if isAdmin();\n    }\n    \n    match /person/{document=**} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /person/{document}/Vaccine/{doc=**} {\n    allow create: if true;\n    // allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    // fix this later\n    allow read, update : if true;\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /person/{document}/OPD/{doc} {\n    allow create: if true;\n    // allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    // fix this later\n    allow read, update : if true;\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /person/{document}/Lab/{doc} {\n    allow create: if true;\n    // allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    // fix this later\n    allow read, update : if true;allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /person/{document}/Rx/{doc} {\n    allow create: if true;\n    // allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    // fix this later\n    allow read, update : if true;\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /person/{document}/Messages/{doc} {\n    allow create: if true;\n    // allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    // fix this later\n    allow read, update : if true;\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /appointments/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /records/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    }\n    \n    match /vaccine/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    }\n    \n    match /purchase/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n\t\tmatch /msr/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /vendor/{document} {\n    allow create: if true;\n    allow read, update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    match /warehouse/{document} {\n    allow create: if true;\n    allow read: if isSignedIn()\n    allow update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    match /item/{document} {\n    allow create: if true;\n    allow read: if isSignedIn()\n    allow update : if isSignedIn() && (isDocOwner() || isAdmin());\n    allow delete : if isSignedIn() && isAdmin();\n    }\n    \n    // helper functions\n    function isSignedIn() {\n    return request.auth.uid != null;\n    }\n    \n    function onlyContentChanged() {\n    return request.resource.data.role == resource.data.role;\n    // make sure user is not signing in with any role or changin his role during update\n    }\n    function isDocOwner() {\n    return request.auth.uid == resource.data.author;\n    }\n    // function isDocCreater() {\n    // return request.auth.uid == request.resource.data.author;\n    // }\n    function isAdmin() {\n    return get(/databases/$(database)/documents/users/$(request.auth.uid)).data.role == \"admin\";\n    }\n    // function isEmployee() {\n    // return get(/databases/$(database)/documents/settings/$(request.auth.uid)).data.role == \"employee\";\n    // }\n    }\n    }\n```\n</ul>\n\n![Pic 4](./images/env_variable.png)\n![Pic 4](./images/social_auth.png)\n![Pic 4](./images/rules.png)"
 },
 {
  "repo": "IRCAD/sight",
  "language": "C++",
  "readme_contents": "# Sight\n\n| Branch |    Status |\n|--------|-----------|\n| Dev    | [![pipeline status](https://git.ircad.fr/Sight/sight/badges/dev/pipeline.svg)](https://git.ircad.fr/Sight/sight/commits/dev) |\n| Master | [![pipeline status](https://git.ircad.fr/Sight/sight/badges/master/pipeline.svg)](https://git.ircad.fr/Sight/sight/commits/master) |\n\n## Description\n[//]: # (cspell: disable)\n**Sight**, the **S**urgical **I**mage **G**uidance and **H**ealthcare **T**oolkit aims to ease the creation of\napplications based on medical imaging.\n[//]: # (cspell: enable)\n\nIt includes various functionalities such as 2D and 3D digital image processing, visualization, augmented reality and\nmedical interaction simulation. It runs on Microsoft Windows and Linux, is written in C++, and features rapid interface\ndesign using XML files. It is freely available under the LGPL.\n\n**Sight** is mainly developed by the Surgical Data Sciences Team of [IRCAD France](https://www.ircad.fr), where it is\nused everyday to develop innovative applications for the operating room and medical research centers.\n\nMany **tutorials** and **examples**, which can help you to learn smoothly how to use **Sight**, are located in the\n`tutorials` and `examples` directories.\nDetailed steps are described [here](https://sight.pages.ircad.fr/sight-doc/Tutorials/index.html).\n\n### Features\n\n- 2D/3D visualization of medical images, meshes, and many widgets.\n- Import / export medical data from various formats (DICOM, [VTK](https://www.vtk.org/), ...) and sources\n  (files, devices, PACS, ...).\n- Playing, recording, processing videos (webcams, network streams, Intel RealSense devices, ...).\n- Easy GUI configuration and customization (XML description and stylesheets support).\n- Timeline, allowing to store various data (video, matrices, markers, etc...) and synchronize these data across time.\n- Mono and stereo camera calibration,\n- [ArUco](https://sourceforge.net/projects/aruco/) optical markers tracking,\n- [openIGTLink](http://openigtlink.org/) support through client and server services,\n- Advanced memory management to support large data. Unused data can be offloaded to disk, saving memory for foreground\n  tasks.\n- Work session or any part of it, can be saved and restored on disk. The data itself can be encrypted using AES256 to\n  ensure a high level of security and privacy\n\n\n### Hardware / Operating System / Compiler support\n\n**Sight** is written in standard C++17 and use [CMake](https://cmake.org/) as its build system, which means that Sight\nshould at least compile on any operating system that provide support for a decent C++17 compiler, CMake, **AND** Sight's\ndependencies (see [Install](#install) for a list of dependencies for Linux platform). However, we currently have access\nto a limited set of hardware/OS/compiler combinations where the code is actually tested on a regular basis.\n\nSuch combination includes:\n-  [Debian 11 stable on AMD64 with GCC 10.2.1](https://www.debian.org/ports/amd64)\n-  [Ubuntu 21.04 on AMD64 with GCC 10.3.0 or CLang 12](https://releases.ubuntu.com/21.04/)\n-  [Microsoft Windows 10 on AMD64 with VisualStudio 2019](https://www.microsoft.com/windows/)\n\n> If your platform is not listed, that *doesn't* mean **Sight** will not work, just we cannot guarantee that it is well\n> tested. If you are on this kind of platform and are able to build and use **Sight**, feel free to share with us your\n> success !\n\n> We use some fine tuned compiler flags (like `/arch:AVX2`) to optimize and generate code specifically for CPUs that\n> were released around 2013 and later. It means, if your CPU is too old, **Sight** will crash at runtime because some\n> CPU instructions are not implemented. In such situation, you can modify hidden cmake variable `SIGHT_ARCH` at\n> configuring time or modify the default compiler flag directly in **Sight** CMake code.\n\n## Applications\n\n### SightViewer\n\n**SightViewer** is a full featured medical image and mesh viewer with advanced rendering features such as volume\nrendering. It supports most medical image formats, and can also retrieve DICOM files from a PACS. It demonstrates many\nuseful features of Sight.\n\n<div align=center style=\"text-align: center; display: flex; flex-flow: row wrap; justify-content: space-around;\">\n<figure>\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/SightViewer01.gif\">\n    <figcaption>\n        <b><i>MPR view of a medical 3D image with additional volume rendering</i></b>\n    </figcaption>\n</figure>\n<figure>\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/SightViewer02.gif\">\n    <figcaption>\n        <b><i>Volume rendering and transfer function tuning</i></b>\n    </figcaption>\n</figure>\n<figure>\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/mixed_vr_reconstructions.gif\">\n    <figcaption>\n        <b><i>Volume rendering mixed with 3D surfacic meshes</i></b>\n    </figcaption>\n</figure>\n</div>\n\n### DicomXplorer\n\n**DicomXplorer** is a simple medical image viewer that can connect to a PACS to retrieve DICOM data. It supports CT-scan\nand MRI images.\n\n<div align=center style=\"text-align: center; display: flex; flex-flow: row wrap; justify-content: space-around;\">\n<figure>\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/DicomXplorer01.gif\">\n    <figcaption>\n        <b><i>DICOM and medical image files navigation</i></b>\n    </figcaption>\n</figure>\n<figure>\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/DicomXplorer02.gif\">\n    <figcaption>\n        <b><i>MPR view of a medical 3D image</i></b>\n    </figcaption>\n</figure>\n</div>\n\n### SightCalibrator\n\n**SightCalibrator** is a user-friendly application to calibrate mono and stereo cameras.\nThis software is a must-have since camera calibration is a mandatory step in any AR application.\n\n<div align=center style=\"text-align: center; display: flex; flex-flow: row wrap; justify-content: space-around;\">\n<figure style=\"\">\n    <img src=\"https://git.ircad.fr/sight/sight-doc/-/raw/dev/Introduction/media/SightCalibrator01.gif\">\n    <figcaption>\n        <b><i>Intrinsic & extrinsic calibration of mono/stereo cameras with live reprojection error display</i></b>\n    </figcaption>\n</figure>\n</div>\n\n## Install\n\nSee [detailed install instructions](https://sight.pages.ircad.fr/sight-doc/Installation/index.html) for Windows and\nLinux.\n\n## Documentation\n\n* [Documentation](https://sight.pages.ircad.fr/sight-doc)\n* [Tutorials](https://sight.pages.ircad.fr/sight-doc/Tutorials/index.html)\n* [Doxygen](https://sight.pages.ircad.fr/sight)\n\n## Support\n\nPlease note that our GitLab is currently only available in read-only access for external developers and users. This is a\nrestriction because of the licensing model of GitLab. Since we use an EE version, we would be forced to pay for every\ncommunity user, and unfortunately we cannot afford it. This licensing model might change in the future\nhttps://gitlab.com/gitlab-org/gitlab-ee/issues/4382 though.\n\nUntil then, we gently ask our community users to use our GitHub mirror to\n[report any issues](https://github.com/IRCAD/sight/issues) or propose\n[contributions](https://github.com/IRCAD/sight/pulls).\n\nYou can also get live community support on the [gitter chat room](https://gitter.im/IRCAD-IHU/sight-support).\n\n"
 },
 {
  "repo": "hasyed/HealthCareApp",
  "language": "Java",
  "readme_contents": "HealthCareApp\n=============\n\nThe Mobile Solution to maintain consumption of daily use edibles and calorie balance with interactive user interfaces, Chart and Nutrition labels calculated in such a way that are readable to user.\n\nThis app takes an image of  nutrition chart of the product which user wants to intake. OpenCV is used to clean the captured image. Then the image goes to OCR(Optical Character Recognation) which extract the information from the image and added them to user database that he has consumed it, the information  includes Calories, Total Fats and other Nutritions. \n\nIf user want to gain or reduce weight, the formula in the main of the app helps user to input in how many days he want to reduce the weight and how much he wants to reduce. This will give him the Calories he wanted to take take everyday.\nUser can manually add products or exercise too. Products that he intake increase the calories and exercise that he do reduces the calories.\n\nThere is a chart that helps user to see his previouse performance regards calories as well.\n\nI have forked [android-ocr](https://github.com/rmtheis/android-ocr/tree/master/android/src) with some changes for openCV for cleansing the image for my help. \n##Requires\n[tess-two](https://github.com/rmtheis/tess-two/tree/master/tess-two)\n\n[OpenCV for android](http://opencv.org/)\n\n\n"
 },
 {
  "repo": "Qingbao/HealthCareStepCounter",
  "language": "Java",
  "readme_contents": "HealthCareStepCounter\n=====================\n\nComing soon: rewritten with React Native\n\n"
 },
 {
  "repo": "arvindsis11/Ai-Healthcare-Chatbot",
  "language": "CSS",
  "readme_contents": "# flask-chatbot\nBuilt on python 3.6\nFlask==0.11\nchatterbot==0.8.4\nSQLAlchemy==1.1.11\n\n#### A web implementation of [ChatterBot](https://github.com/gunthercox/ChatterBot) using Flask.\n\n## Local Setup:\n 1. Open command prompt and locate folder. run 'pip install -r requirements.txt'\n 2. Run *train.py*\n 3. Run *run.py*\n 4. Demo will be live at http://localhost:5000/\n \n ## Git push cmd- for reference\n ```java\n echo \"# MyRestApi all crud operations using spring boot framework\" >> README.md\ngit init\ngit add .\ngit commit -m \"initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/arvindsis11/MyRestApi.git\ngit push -u origin main\ngit rm -r --cached .\n////////////////////////////////////////\nor push an existing repository from the command line\ngit remote add origin https://github.com/arvindsis11/springJPAdemo.git\ngit branch -M main\ngit push -u origin main\nhttps://github.com/arvindsis11/angular-todomanagement-app.git\n/////////////////////////////////////\ncommon git error:\nuse this:\ngit pull --rebase origin main\ngit push origin main\nurl:https://stackoverflow.com/questions/24114676/git-error-failed-to-push-some-refs-to-remote\n ```\n\n## License\nThis source is free to use, but ChatterBot does have a license which still applies and can be found on the [LICENSE](https://github.com/gunthercox/ChatterBot/blob/master/LICENSE) page.\n"
 },
 {
  "repo": "microsoft/Healthcare-Blockchain-Solution-Accelerator",
  "language": "C#",
  "readme_contents": "# Blockchain Healthcare Solution Accelerator Guide\r\n\r\n## About this repository\r\nThis accelerator was built to provide developers with all of the resources needed to quickly build an initial Hyperledger Fabric Healthcare data transactionary solution. Use this accelerator to jump start your development efforts with Hyperledger and Azure.\r\n\r\nThis repository contains the steps, scripts, code, and tools to create a Hyperledger Fabric blockchain application. 00_Resource_Deployment will create the necessary supporting resources in Azure (Storage, Kubernetes, and Cosmos DB). 01_Hyperledger_Fabric_Deployment will configure and deploy a Hyperledger Fabric blockchain network using Helm Packages and Kubernetes. 02_Hyperledger_Fabric_Client will install the necessary chaincode. Finally 03_Application_Deployment will deploy and host your application either locally or in your subscription.\r\n\r\n## Prerequisites\r\nIn order to successfully complete your solution, you will need to have access to and or provisioned the following:\r\n1. Access to an Azure subscription\r\n2. Visual Studio 2017 or 2019\r\n3. Kubectl, Helm, and Docker Command Line Tools installed\r\n4. Service Fabric\r\n\r\nOptional\r\n1. Intellij CE\r\n\r\n## Azure and Blockchain\r\nThe directions provided for this repository assume fundemental working knowledge of Azure, Cosmos DB, Azure Storage, Hyperledger Fabric, Service Fabric, and Kubernetes.  \r\n\r\nFor additional training and support, please see:\r\n 1. [Kubernetes](https://kubernetes.io/)\r\n 2. [Hyperledger Fabric](https://hyperledger-fabric.readthedocs.io/en/release-1.4/)\r\n 3. [Service Fabric](https://azure.microsoft.com/en-us/services/service-fabric/)\r\n 4. [Cosmos DB](https://docs.microsoft.com/en-us/azure/cosmos-db/introduction)\r\n\r\n## Getting Started and Process Overview\r\nClone/download this repo onto your computer and then walk through each of these folders in order, following the steps outlined in each of the README files.  After completion of all steps, you will have a working end-to-end solution with the following architecture:\r\n\r\n![Microservices Architecture](./References/architecture.JPG)\r\n\r\n\r\n### [00 - Resource Deployment](./00_Resource_Deployment)\r\nThe resources in this folder can be used to deploy the required resources into your Azure Subscription. This can be done either via the [Azure Portal](https://portal.azure.com) or by using the [PowerShell script](./00_Resource_Deployment/deploy.ps1) included in the resource deployment folder.\r\n\r\nAfter deployed, you will have a Cosmos DB account and database, Azure storage, and Kubernetes cluster deployed in your specified resource group.\r\n\r\n### [01 - Hyperledger Fabric Deployment](./01_Hyperledger_Fabric_Deployment)\r\nThis folder contains the Hyperledger Fabric Deployment configuration files. To prepare the environment and deploy the infrastructer run the [deploy script](./01_Hyperledger_Fabric_Deployment/deploy.ps1).\r\n\r\nRunning this script will deploy a basic fabric network consisting of two organizations: one peer organization and one orderer organization. To read more about hyperledger reference the [Hyperledger Fabric Documentation](https://hyperledger-fabric.readthedocs.io/en/release-1.4/).\r\n\r\n### [02 - Hyperledger Fabric Client](./02_Hyperledger_Fabric_Client)\r\nThis folder contains the Kotlin Chaincode and Hyperledger Fabric server used to communicate with the blockchain network. The script in this folder will pull the Fabric Chaincode, Client, and gRPC server. This image will install the chaincode and allow the application to execute against the Blockchain network. Follow the provided instructions.\r\n\r\n### [03 - Application Deployment](./03_Application_Deployment)\r\nThis folder contains the .net services for the proof content storage service, transaction tracker, transaction indexer, and gRPC Fabric Client. The Angular web application is also started and hosted with these services. Service Fabric is used to host this application.\r\n\r\n## Links\r\nHosted Site: [Healthcare Blockchain Solution](http://healthcare-apphosting.southcentralus.cloudapp.azure.com/login)\r\n\r\n*Use income less than 11000 and a NY Zip Code for a profile to qualify*\r\n\r\nVideo: [Healthcare Blockchain Solution Video](https://msit.microsoftstream.com/video/7f62ce8c-39e1-40d6-8adb-cbf298f31dfe)\r\n\r\n*Or download [Healthcare Blockchain Solution Video](healthcare_solution_video.mp4)*\r\n\r\n\r\n## License\r\nCopyright (c) Microsoft Corporation\r\n\r\nAll rights reserved.\r\n\r\nMIT License\r\n\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"\"Software\"\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\r\n\r\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED AS IS, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE\r\n\r\n## Contributing\r\n\r\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\r\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\r\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\r\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\r\nprovided by the bot. You will only need to do this once across all repos using our CLA.\r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or\r\ncontact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\r\n"
 },
 {
  "repo": "openmrs/openmrs-module-radiology",
  "language": "Java",
  "readme_contents": "# openmrs-module-radiology\n\n[![Build Status](https://travis-ci.org/openmrs/openmrs-module-radiology.svg?branch=master)](https://travis-ci.org/openmrs/openmrs-module-radiology) [![Coverage Status](https://coveralls.io/repos/openmrs/openmrs-module-radiology/badge.svg?branch=master&service=github)](https://coveralls.io/github/openmrs/openmrs-module-radiology?branch=master) [![Codacy Badge](https://api.codacy.com/project/badge/grade/5e0137f0c916494eaa3ba7de43149ef7)](https://www.codacy.com/app/teleivo/openmrs-module-radiology_2) [![Dependency Status](https://www.versioneye.com/user/projects/57a194fb3d8eb6002f560778/badge.svg?style=flat)](https://www.versioneye.com/user/projects/57a194fb3d8eb6002f560778)\n\n####Table of Contents\n\n1. [Overview](#overview)\n2. [Build](#build)\n3. [Install](#install)\n  * [Docker](#docker-whale)\n  * [Demo data](#demo-data)\n4. [Documentation](#documentation)\n  * [Website](#website)\n  * [Developer guides](#developer-guides)\n  * [Wiki](#wiki)\n5. [Contributing](#contributing)\n  * [Code](#code)\n  * [Translation](#translation)\n6. [Issues](#issues)\n7. [Limitations](#limitations)\n8. [Community](#community)\n9. [Support](#support)\n10. [License](#license)\n\n## Overview\n\nOpenMRS module radiology (previously called radiologydcm4chee) is a module adding capabilities of a Radiology\nInformation System (RIS) onto OpenMRS.\n\n## Build\n\n### Prerequisites\n\nYou need to have installed\n\n* a Java JDK 8\n* the build tool [Maven](https://maven.apache.org/)\n\nYou need to configure Maven to use the JAVA JDK 8\n\n```bash\nmvn -version\n```\n\nShould tell you what version Maven is using.\n\nYou need to clone this repository:\n\n```bash\ngit clone https://github.com/openmrs/openmrs-module-radiology.git\n```\n\n### Command\n\nAfter you have taken care of the [Prerequisites](#prerequisites)\n\nExecute the following command:\n\n```bash\ncd openmrs-module-radiology\nmvn clean package\n```\n\nThis will generate the radiology module in `omod/target/radiology-{VERSION}.omod` which you will have to deploy into OpenMRS.\n\n## Install\n\nThe easiest way to install the module is to use [Docker](https://www.docker.com/).\n\n### Docker :whale:\n\nThis module can be baked into a Docker image so you can easily run and test it.\n\n#### Prerequisites\n\nAfter you have taken care of the [Build Prerequisites](#prerequisites)\n\nMake sure you have [Docker](https://docs.docker.com/) installed.\n\n#### Build\n\nBuild the Radiology Module and its Docker image:\n\n```bash\ncd openmrs-module-radiology\nmvn clean package docker:build\n```\n\n#### Run\n\nTo run an instance of the OpenMRS Radiology Module execute (assumes you have\ncreated a Docker image):\n\n```bash\ncd openmrs-module-radiology\nmvn docker:start\n```\n\nOpenMRS will be accessible at `http://<IP ADDRESS>:8080/openmrs`\n\n**NOTE: The IP address varies depending on your setup.**\n\nIf you are using [Docker machine](https://docs.docker.com/machine/) refer to its documentation on how to get the IP address.\nIf you are on Linux it will probably be will be `localhost`.\n\n#### Documentation\n\nPlease read the corresponding [DOCKER.md](docs/DOCKER.md) for more detailed\nexplanations on using Docker with the Radiology Module.\n\n### Demo data\n\nYou can import the demo data set [demo-data.sql](acceptanceTest/resources/demo-data.sql) into\nyour database which enables you to try out the modules features or test your\nchanges.\n\nPlease read the corresponding [DEMO-DATA.md](docs/DEMO-DATA.md).\n\n## Documentation\n\n### Website\n\nFor a detailed guide on ways to install and configure this module see\n\nhttp://teleivo.github.io/docs-openmrs-module-radiology/\n\n### Developer guides\n\nPlease check out the readme files at [docs](docs/).\n\n### Wiki\n\nFor some more background informations on the module see\n\nhttps://wiki.openmrs.org/display/docs/Radiology+Module\n\n## Contributing\n\nContributions are very welcome, we can definitely use your help!\n\n### Code\n\nCheck out our [contributing guidelines](CONTRIBUTING.md), read through the [Developer guides](#developer-guides).\n\nAfter you've read up :eyeglasses: [grab an issue](https://issues.openmrs.org/browse/RAD) that is `Ready For Development`.\n\n### Translation\n\nWe use\n\nhttps://www.transifex.com/openmrs/OpenMRS/radiology-module/\n\nto manage our translations.\n\nThe `messages.properties` file in this repository is our single source of\ntruth. It contains key, value pairs for the English language which is the\ndefault.\n\nTransifex fetches updates to this file every night which can then be translated\nby you and me on transifex website itself. At any time we can pull new translations from transifex\nback into this repository. Other languages like for ex. Spanish will then be in\nthe `messages_es.properties` file.\n\nIf you would like to know more about transifex from the coding side read\n\nhttps://wiki.openmrs.org/display/docs/Maintaining+OpenMRS+Module+Translations+via+Transifex\n\n## Issues\n\nTo file new issues or help to fix existing ones please check out\n\nhttps://issues.openmrs.org/browse/RAD\n\n## Limitations\n\nThis module is not yet officially released to the [openmrs modules](https://modules.openmrs.org/#/).\n\nThe API and UI are not yet stable and subject to frequent changes.\n\n:exclamation: ATTENTION :exclamation: radiology orders created via the module will not be sent to the PACS\nas HL7 order messages. This has previously been done in a hacky/synchronous way which was not fit for\nproduction and only messy code which had to be removed. A message queue which takes care of sending HL7\norder messages to the PACS once orders are created is needed. Such a queue would retry sending the order message\nin case the PACS is currently down. Unfortunately, OpenMRS does not provide such a message queue for outgoing HL7 messages.\nThis is THE big missing piece in the puzzle of the radiology module which until\nnow has been bridged with communication servers such as mirth.\n\nThe module depends on [OpenMRS Version 2.0.0](https://github.com/openmrs/openmrs-core) so it cannot\nrun on any version lower than that.\n\nThe module currently depends on [OpenMRS Legacy UI](https://github.com/openmrs/openmrs-module-legacyui)\nwhich provides the UI but it is [planned](https://issues.openmrs.org/browse/RAD-341)\nto extract the UI into a separate module so this module only provides the Java and\nREST API without forcing a specific UI onto anyone.\n\n## Community\n\n[![OpenMRS Talk](https://omrs-shields.psbrandt.io/custom/openmrs/talk/F26522?logo=openmrs)](http://talk.openmrs.org)\n[![OpenMRS IRC](https://img.shields.io/badge/openmrs-irc-EEA616.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSI2MTIiIGhlaWdodD0iNjEyIiB2aWV3Qm94PSIwIDAgNjEyIDYxMiI%2BPHBhdGggZD0iTTE1MyAyMjkuNWMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzEzMS44NjcgMzA2IDE1MyAzMDZjMjEuMTE0IDAgMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzE3NC4xMzMgMjI5LjUgMTUzIDIyOS41em0xNTMgMGMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzI4NC44NjcgMzA2IDMwNiAzMDZjMjEuMTE0IDAgMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzMyNy4xMzMgMjI5LjUgMzA2IDIyOS41em0xNTMgMGMtMjEuMTMzIDAtMzguMjUgMTcuMTE3LTM4LjI1IDM4LjI1UzQzNy44NjcgMzA2IDQ1OSAzMDZzMzguMjUtMTcuMTE3IDM4LjI1LTM4LjI1UzQ4MC4xMzMgMjI5LjUgNDU5IDIyOS41ek0zMDYgMEMxMzcuMDEyIDAgMCAxMTkuODc1IDAgMjY3Ljc1YzAgODQuNTE0IDQ0Ljg0OCAxNTkuNzUgMTE0Ljc1IDIwOC44MjZWNjEybDEzNC4wNDctODEuMzRjMTguNTUyIDMuMDYyIDM3LjYzOCA0Ljg0IDU3LjIwMyA0Ljg0IDE2OS4wMDggMCAzMDYtMTE5Ljg3NSAzMDYtMjY3Ljc1UzQ3NS4wMDggMCAzMDYgMHptMCA0OTcuMjVjLTIyLjMzOCAwLTQzLjkxLTIuNi02NC42NDMtNy4wMmwtOTAuMDQgNTQuMTI0IDEuMjA0LTg4LjdDODMuNSA0MTQuMTMzIDM4LjI1IDM0NS41MTMgMzguMjUgMjY3Ljc1YzAtMTI2Ljc0IDExOS44NzUtMjI5LjUgMjY3Ljc1LTIyOS41czI2Ny43NSAxMDIuNzYgMjY3Ljc1IDIyOS41UzQ1My44NzUgNDk3LjI1IDMwNiA0OTcuMjV6IiBmaWxsPSIjZmZmIi8%2BPC9zdmc%2B)](http://irc.openmrs.org)\n[![OpenMRS Telegram](https://img.shields.io/badge/openmrs-telegram-009384.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNDAgMjQwIj48ZGVmcz48bGluZWFyR3JhZGllbnQgaWQ9ImEiIHgxPSIuNjY3IiB5MT0iLjE2NyIgeDI9Ii40MTciIHkyPSIuNzUiPjxzdG9wIHN0b3AtY29sb3I9IiMzN2FlZTIiIG9mZnNldD0iMCIvPjxzdG9wIHN0b3AtY29sb3I9IiMxZTk2YzgiIG9mZnNldD0iMSIvPjwvbGluZWFyR3JhZGllbnQ%2BPGxpbmVhckdyYWRpZW50IGlkPSJiIiB4MT0iLjY2IiB5MT0iLjQzNyIgeDI9Ii44NTEiIHkyPSIuODAyIj48c3RvcCBzdG9wLWNvbG9yPSIjZWZmN2ZjIiBvZmZzZXQ9IjAiLz48c3RvcCBzdG9wLWNvbG9yPSIjZmZmIiBvZmZzZXQ9IjEiLz48L2xpbmVhckdyYWRpZW50PjwvZGVmcz48Y2lyY2xlIGN4PSIxMjAiIGN5PSIxMjAiIHI9IjEyMCIgZmlsbD0idXJsKCNhKSIvPjxwYXRoIGZpbGw9IiNjOGRhZWEiIGQ9Ik05OCAxNzVjLTMuODg4IDAtMy4yMjctMS40NjgtNC41NjgtNS4xN0w4MiAxMzIuMjA3IDE3MCA4MCIvPjxwYXRoIGZpbGw9IiNhOWM5ZGQiIGQ9Ik05OCAxNzVjMyAwIDQuMzI1LTEuMzcyIDYtM2wxNi0xNS41NTgtMTkuOTU4LTEyLjAzNSIvPjxwYXRoIGZpbGw9InVybCgjYikiIGQ9Ik0xMDAuMDQgMTQ0LjQxbDQ4LjM2IDM1LjczYzUuNTIgMy4wNDQgOS41IDEuNDY3IDEwLjg3Ni01LjEyNGwxOS42ODUtOTIuNzYzYzIuMDE2LTguMDgtMy4wOC0xMS43NDYtOC4zNTgtOS4zNWwtMTE1LjU5IDQ0LjU3MmMtNy44OSAzLjE2NS03Ljg0NCA3LjU2Ny0xLjQ0IDkuNTI4bDI5LjY2NCA5LjI2IDY4LjY3My00My4zMjZjMy4yNC0xLjk2NiA2LjIxNy0uOTEgMy43NzUgMS4yNTgiLz48L3N2Zz4%3D)](https://telegram.me/openmrs)\n[![OpenMRS Radiology Wiki](https://img.shields.io/badge/openmrs-wiki-5B57A6.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNjAiIGhlaWdodD0iMTQyIiB2aWV3Qm94PSIwIDAgMTYwIDE0MiI%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%3D%3D)](https://wiki.openmrs.org/display/docs/Radiology+Module)\n\n## Support\n\nAsk questions on [OpenMRS Talk](https://talk.openmrs.org/).\n\n## License\n\n[MPL 2.0 w/ HD](http://openmrs.org/license/) \u00a9 [OpenMRS Inc.](http://www.openmrs.org/)\n"
 },
 {
  "repo": "VectorInstitute/cyclops",
  "language": "Python",
  "readme_contents": "![cyclops Logo](https://github.com/VectorInstitute/cyclops/blob/main/docs/source/theme/static/cyclops_logo-dark.png?raw=true)\n\n--------------------------------------------------------------------------------\n\n[![PyPI](https://img.shields.io/pypi/v/pycyclops)](https://pypi.org/project/pycyclops)\n[![code checks](https://github.com/VectorInstitute/cyclops/actions/workflows/code_checks.yml/badge.svg)](https://github.com/VectorInstitute/cyclops/actions/workflows/code_checks.yml)\n[![integration tests](https://github.com/VectorInstitute/cyclops/actions/workflows/integration_tests.yml/badge.svg)](https://github.com/VectorInstitute/cyclops/actions/workflows/integration_tests.yml)\n[![docs](https://github.com/VectorInstitute/cyclops/actions/workflows/docs_deploy.yml/badge.svg)](https://github.com/VectorInstitute/cyclops/actions/workflows/docs_deploy.yml)\n[![codecov](https://codecov.io/gh/VectorInstitute/cyclops/branch/main/graph/badge.svg)](https://codecov.io/gh/VectorInstitute/cyclops)\n[![license](https://img.shields.io/github/license/VectorInstitute/cyclops.svg)](https://github.com/VectorInstitute/cyclops/blob/main/LICENSE)\n\n``cyclops`` is a framework for facilitating research and deployment of ML models\nin the health (or clinical) setting. It provides a few high-level APIs namely:\n\n\n* `query` - Querying EHR databases (such as MIMIC-IV)\n* `process` - Process static and temporal EHR data\n* `evaluate` - Evaluate models on clinical prediction tasks\n* `monitor` - Detect data drift relevant for clinical use cases\n\n``cyclops`` also provides a library of use-cases on clinical datasets. The implemented\nuse cases include:\n\n* Mortality decompensation prediction\n\n\n## \ud83d\udc23 Getting Started\n\n### Installing cyclops using pip\n\n```bash\npython3 -m pip install pycyclops\n```\n\nThe core package only includes support for the `process` API. To install support for\n`query`, `evaluate` and `monitor` APIs, install them as extra dependency installs.\n\nTo install with `query` API support,\n\n```bash\npython3 -m pip install 'pycyclops[query]'\n```\n\nTo install with `evaluate` API support,\n\n```bash\npython3 -m pip install 'pycyclops[evaluate]'\n```\n\nTo install with `monitor` API support,\n\n```bash\npython3 -m pip install 'pycyclops[monitor]'\n```\n\nMultiple extras could also be combined, for example to install with both `query` and\n`evaluate` API support:\n\n```bash\npython3 -m pip install 'pycyclops[query,evaluate]'\n```\n\n\n## \ud83e\uddd1\ud83c\udfff\u200d\ud83d\udcbb Developing\n\nThe development environment has been tested on ``python = 3.9``.\n\nThe python virtual environment can be set up using\n[poetry](https://python-poetry.org/docs/#installation). Hence, make sure it is\ninstalled and then run:\n\n```bash\npython3 -m poetry install\nsource $(poetry env info --path)/bin/activate\n```\n\nAPI documentation is built using [Sphinx](https://www.sphinx-doc.org/en/master/) and\ncan be locally built by:\n\n```bash\ncd docs\nmake html SPHINXOPTS=\"-D nbsphinx_allow_errors=True\"\n```\n\n### Contributing\nContributing to cyclops is welcomed.\nSee [Contributing](https://vectorinstitute.github.io/cyclops/api/intro.html) for\nguidelines.\n\n\n## \ud83d\udcda [Documentation](https://vectorinstitute.github.io/cyclops/)\n\n## \ud83d\udcd3 Notebooks\n\nTo use jupyter notebooks, the python virtual environment can be installed and\nused inside an IPython kernel. After activating the virtual environment, run:\n\n```bash\npython3 -m ipykernel install --user --name <name_of_kernel>\n```\n\nNow, you can navigate to the notebook's ``Kernel`` tab and set it as\n``<name_of_kernel>``.\n\n## \ud83c\udf93 Citation\nReference to cite when you use CyclOps in a project or a research paper:\n```\n@article {Krishnan2022.12.02.22283021,\n\tauthor = {Krishnan, Amrit and Subasri, Vallijah and McKeen, Kaden and Kore, Ali and Ogidi, Franklin and Alinoori, Mahshid and Lalani, Nadim and Dhalla, Azra and Verma, Amol and Razak, Fahad and Pandya, Deval and Dolatabadi, Elham},\n\ttitle = {CyclOps: Cyclical development towards operationalizing ML models for health},\n\telocation-id = {2022.12.02.22283021},\n\tyear = {2022},\n\tdoi = {10.1101/2022.12.02.22283021},\n\tpublisher = {Cold Spring Harbor Laboratory Press},\n\tURL = {https://www.medrxiv.org/content/early/2022/12/08/2022.12.02.22283021},\n\tjournal = {medRxiv}\n}\n```\n"
 },
 {
  "repo": "hapifhir/hapi-fhir",
  "language": "Java",
  "readme_contents": "HAPI FHIR\n=========\n\nHAPI FHIR - Java API for HL7 FHIR Clients and Servers\n\n[![License][Badge-License]][Link-License]\n\n## CI/CD\n| CI Status (master) | SNAPSHOT Pipeline | Current Release |\n| :---: | :---: | :---: |\n| [![Build Status][Badge-AzurePipelineMaster]][Link-AzurePipelinesMaster] | [![Build Status][Badge-AzureReleaseSnapshot]][Link-AzurePipelinesSnapshot] | [![Release Artifacts][Badge-MavenCentral]][Link-MavenCentral] |\n\n## Coverage and Quality\n\n[![codecov][Badge-CodeCov]][Link-CodeCov]\n[![Language grade: Java](https://img.shields.io/lgtm/grade/java/g/hapifhir/hapi-fhir.svg?logo=lgtm&logoWidth=18)](https://lgtm.com/projects/g/hapifhir/hapi-fhir/context:java)\n\n## Documentation and wiki\n\nComplete project documentation is available here:\nhttp://hapifhir.io\n\nA demonstration of this project is available here:\nhttp://hapi.fhir.org/\n\nThis project is Open Source, licensed under the Apache Software License 2.0.\n\nPlease see [this wiki page][Link-wiki] for information on where to get help with HAPI FHIR. \n\nPlease see [Smile CDR][Link-SmileCDR] for information on commercial support.\n\n[Link-AzurePipelines]: https://dev.azure.com/hapifhir/HAPI%20FHIR/_build\n[Link-AzurePipelinesMaster]: https://dev.azure.com/hapifhir/HAPI%20FHIR/_build?definitionId=2\n[Link-AzurePipelinesSnapshot]: https://dev.azure.com/hapifhir/HAPI%20FHIR/_build?definitionId=3\n[Link-MavenCentral]: http://search.maven.org/#search|ga|1|ca.uhn.hapi.fhir\n[Link-CodeCov]: https://codecov.io/gh/hapifhir/hapi-fhir\n[Link-wiki]: https://github.com/hapifhir/hapi-fhir/wiki/Getting-Help\n[Link-SmileCDR]: https://smilecdr.com\n[Link-License]: https://hapifhir.io/hapi-fhir/license.html\n\n[Badge-AzurePipelineMaster]: https://dev.azure.com/hapifhir/HAPI%20FHIR/_apis/build/status/hapifhir.hapi-fhir?branchName=refs%2Fpull%2F2319%2Fmerge\n[Badge-AzureReleaseSnapshot]: https://dev.azure.com/hapifhir/HAPI%20FHIR/_apis/build/status/SNAPSHOT%20pipeline?branchName=master\n[Badge-MavenCentral]: https://maven-badges.herokuapp.com/maven-central/ca.uhn.hapi.fhir/hapi-fhir-base/badge.svg\n[Badge-CodeCov]: https://codecov.io/gh/hapifhir/hapi-fhir/branch/master/graph/badge.svg?token=zHfnKfQB9X\n[Badge-License]: https://img.shields.io/badge/license-apache%202.0-60C060.svg\n\n\n"
 },
 {
  "repo": "aws-samples/amazon-sagemaker-healthcare-fraud-detection",
  "language": "Jupyter Notebook",
  "readme_contents": "\n\n# Introduction\n\nIDC study states that 40% of Enteprises in year 2019 will be working to include AI/ML as a part of their transformative strategy. Today, AI/ML is beyond the hype cycle and there are usecases that are providing real business value. \n\nIn this workshop, we will work on a healthcare insurance fraud identification usecase. We will apply machine learning to identify anomalous claims that require further investigation. The technique used in the workshop is broadly applicable to multiple problems fraud, abuse and waste.\n\n## **Launch an Amazon SageMaker Jupyter Notebook**\n\n### Prerequisites and assumptions\n1. To run this Jupyter Notebook, you need an personal Laptop and an AWS account that provides access to AWS services.\n\n### Steps\n1. Sign In to the [AWS Console](https://aws.amazon.com/)\n2. Click Services, search for **Amazon SageMaker** and Click **Amazon SageMaker** in the dropdown![Find SageMaker](./images/find-sagemaker.png)\n3. After you land on Amazon SageMaker console, click on **Notebook Instances**![SageMaker Console](./images/sagemaker-console.png)\n4. Click **Create Notebook**![Create Notebook](./images/create-notebook.png)\n5. Give Notebook a name you can remember and fill out configuration details as suggested in the screenshots below.![Create Notebook Instance](./images/create-notebook-instance.png)\n6. Select IAM Role if one already exists in the dropdown![Select Existing Role](./images/select-role.png)\n7. Create a new role if one doesn't exist. ![Create new role](./images/create-role.png)\n8. Provide  a path to clone public git repo that we will use today for our workshop to download data dictionary and Jupyter IPython Notebook ![Select Git Repo](./images/select-gitrepo.png) \n9. Provide the path of [Git repo](https://github.com/aws-samples/amazon-sagemaker-healthcare-fraud-detection.git)\n![Provide Git url](./images/clone-gitrepo.png) \n6. Click **Create Notebook Instance**\n8. In the Amazon SageMaker Console-->Notebook Instances, wait for your notebook instance to start. Observe change from Pending to In Service status.![Creation pending](./images/creation-pending.png)![Notebook In Service](./images/notebook-inservice.png)\n9. Remember the name of your notebook instance and Click **Open Jupyter** for your notebook.![Notebook In Service](./images/notebook-inservice.png)\n10. Validate your data and notebook cloned from Git Repo![Validate Git Clone](./images/validate-git-clone.png)\n\n## **Finish your Lab in Jupyter Notebook**\n1. Click on **healthcare-fraud-identification-using-PCA-anomaly-detection.ipynb** and start working. From here onwards all the instruction will be in the Jupyter Notebook. Come back after you have completed all the steps in the Jupyter Notebook and finish rest of the steps as suggested below.\n\n\n## Finish\n1. **Congratulations!** \n2. Please make sure to delete all resources as mentioned in the section below.\n\n\n## Cleanup Resources\n1. Go to Amazon SageMaker console to shutdown your Amazon SageMaker Jupyter Notebook Instance, select your instance from the list.Select **Stop** from the **Actions** drop down menu.\n![Stop Notebook Instance](./images/stop-notebook.png)\n2. After your notebook instance is completely **Stopped**, select **Delete** fron the **Actions** drop down menu to **delete** your notebook instance.![Delete Notebook Instance](./images/delete-notebook.png)\n4. Navigate to Amazon S3 Console. \n![S3 Console](./images/s3-console.png)\n5. Find Amazon S3 bucket created for training and click to list objects in the bucket.![Find Bucket](./images/search-s3-bucket.png)\n6. Navigate to the **model-tar.gz** and delete it by using **Actions** menu.![Delete Model](./images/delete-model.png) \n6. Navigate to the training data file **healthcare_fraud_identification_feature_store** and delete it by using **Actions** menu.![Delete Training Data](./images/delete-training-data.png)\n7. After all the objects are deleted in the bucket. Go ahead and delete the bucket using the Actions menu.![Delete Bucket](./images/delete-bucket.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n\n"
 },
 {
  "repo": "Sid1608/E-HealthCare-Management-System",
  "language": "Java",
  "readme_contents": "# E-HealthCare-Management-System\n<h2>Table Of Content</h2>\n<ol>\n  <li><a href=\"#description\">Description</a></li>\n  <li><a href=\"#lat\">Languages and Technology Used</a></li>\n  <li><a href=\"#Req\">Requirements</li>\n  <li><a href=\"#features\">Features</a></li>\n  <li><a href=\"#steps\">Steps to run the project in your machine</a></li>\n  <li ><a href=\"#ws\">Working-Snippets</a></li>\n  <li><a href=\"#cs\"> Database-Snippets</a></li>\n\n</ol>\n<h2 id=\"description\">Description</h2>\n E-HealthCare-Management-System is a console based application which is built using java.This application helps in management of  Patients, doctors, admin in a easy and comfortable way.using this Application patients can quickly Sign up, Login, view his/her profile, view doctors, book Appointment, view Report, choose doctor, view Appointments, give feedback, pay online and logout. Admin can add Doctors,view patients list, view Doctors list,remove doctors, see feedback given by patients,view reports,logout.Doctor can login, view profile, viewAppointments, Attend Patients and logout. \n \n <h2 id=\"lat\">Languages and Technology Used</h2>\n <ul>\n  <li>Java</li>\n  <li>MySql</li>\n  <li>Jdbc</li>\n</ul>\n<h2 id=\"Req\">Requirements</h2>\n<ul>\n  <li>Java [JDK 8+]</li>\n  <li>Eclipse</li>\n  <li>MySql</li>\n  <li>Jdbc Driver</li>\n  <li>MySql Connector</li>\n</ul>\n <h2 id=\"features\">Features</h2>\n <ul>\n  <li><a href=\"#login\">login</a></li>\n  <li><a href=\"#Admin\">Admin\u2019s DashBoard</a></li>\n  <li><a href=\"#Patient\">Patient\u2019s DashBoard</a></li>\n  <li><a href=\"#Doctor\">Doctor\u2019s DashBoard</a></li>\n  <li><a href=\"#Report\">Report-Table</a></li>\n  <li><a href=\"#Appointment\">Appointment-Table</a></li>\n  <li><a href=\"#feedback\">Feedback Form</a></li>\n  <li><a href=\"#Booking\">Booking Appointment</li>\n  <li ><a href=\"#choose\">Choosing Doctor</a></li>\n  <li ><a href=\"#Payment\">Payment Process</a></li>\n </ul>\n <h2 id=\"steps\">Steps to run the project in your machine</h2>\n <ol>\n   <li>Download and install Eclipse in your machine</li>\n    <li>Clone or download the repository</li>\n    <li>Extract all the files and move it in your eclipse directory.Open EHMS Folder.</li>\n   <li>Open EHMS.sql in your MySql workbench.download MySql connector(\u201cmysql-connector-java-8.0.22.jar\" ) </li>\n   <li>Now Open .classpath file in EHMS folder,in Line - 9 of This file Change the  path with the path where your .jar file is being downloaded. </li>\n   <li>Open ConnectionProvider.java file and change the uname(username) and pass(password) variable according the user name and password of your MySql database</li>\n   <li>Now it is ready to Run</li>\n </ol>\n <h2 id=\"ws\">Working-Snippets</h2>\n<img id=\"login\" src=\"snippets/Login.png\">\n<img id=\"Admin\" src=\"snippets/Admin.png\">\n<img id=\"Patient\" src=\"snippets/Patient.png\">\n<img id=\"Doctor\" src=\"snippets/Doctor.png\">\n<img id=\"Report\" src=\"snippets/Report.png\">\n<img id =\"Appointment\" src=\"snippets/Appointment.png\">\n<img id=\"feedback\" src=\"snippets/Feedback.png\">\n<img id=\"Booking\" src=\"snippets/BookingAppointment.png\">\n<img id=\"choose\" src=\"snippets/choosingDoctor.png\">\n<img id=\"Payment\" src=\"snippets/paymentProcess.png\">\n<h2 id=\"cs\">Database-Snippets</h2>\n<img id=\"a\" src=\"snippets/Database/Schema.png\">\n<img id=\"b\" src=\"snippets/Database/User-PatientTable.png\">\n<img id=\"c\" src=\"snippets/Database/Doctor-AppointmentTable.png\">\n<img id=\"d\" src=\"snippets/Database/ExampleDoctorInputs.png\">\n<img id=\"e\" src=\"snippets/Database/Report-FeebackTable.png\">\n"
 },
 {
  "repo": "GeneSourceCodeChain/AI_Components",
  "language": "C",
  "readme_contents": "# AI-components\n### Introduction\nAI Components of GeneSourceCode project is a subproject taking charge of machine learning related tasks. AI Components aim to make full use of gene and other medical data with the facilities of modern AI technologies. AI Components currently focus on \r\n\n1.Prediction of diseases and traits directly from raw DNA sequence.\r\nWe will test both traditional classification/regression algorithm and popular deep neural network ways such as LSTM, p-LSTM, IndRNN, attention model and so on to process raw DNA sequential data.\r\n\n2.Prediction of diseases and traits from hand-designed feature.\r\nThe hand-designed feature extracted from raw DNA, RNA or histone sometime may be discriminative enough to make prediction task viable. We will try to extract and learn on features this way.\r\n\n3.Medical application based on visual clues.\r\nComputer Vision has become a reliable way of prediction after deep learning prevails. Medical scientists have adopted this method to various applications such as predicting or detecting certain diseases, image processing on X ray pictures, and so on. We will implement all these applications in this subproject and make them optional service modules.\r\n\n4.Mining fitness status on physical examination and motion data,\r\nWe will also mining data provided by users to detect potential fitness problem or reveal healthy status. \r\n\n### Components\n1.Prediction of diseases and traits directly from raw DNA sequence.\n\n(1)rawDNA/LSTM: classification base on DNA subsequence:\n\nYou can train a classifier with train_LSTM. The dataset generation tools will be released soon.\n\n2.Prediction of disease and trais from hand-designed feature.\n\n(1)extractedDNA/AllelesClassifier: classification base on Alleles\n\nYou can train a classifier on polymorphic alleles.\n\n3.Medical appliation based on visual clues.\n\n(1)visual/facial: classification based on facial images:\n\n(2)visual/iris: biometric identification and illness detection according to visual information from iris.\n\nYou can train a classifier with train_facial_classifier. The dataset generation tools will be released soon.\n\n4.Mining fitness status on physical examination and motion data.\n"
 },
 {
  "repo": "christian-posta/healthcare-poc",
  "language": "Java",
  "readme_contents": "# HL7 use cases with JBoss Fuse\n\n[HL7 over MLLP](http://www.hl7.org) is a very common transport mechanisms for systems that can speak the HL7 protocol format. [JBoss Fuse](http://www.jboss.org/products/fuse/download/) is a very powerful microservices-style integration platform and has a proven track record for building flexibile, resilient, highly available integration scenarios for critical health-care providers. Additionally, replacing legacy vendors like SeeBeyond on JCAPS is the sweet spot for these types of Fuse implementations. \n\n## Criticality of integrations\nThe integrations that get deployed as part of a Fuse implementation that support health-care usecases, including HL7 integrations, are typically part of Tier 1 applications with utmost uptime and resilience requirements. These applications include, but not limited, patient admission, scheduling, lab results, and even the critical of all critical use cases: transmitting patient vitals in real time. Additionally, high levels of throughput and performance are expected.   \n\n## Overall architecture\nThis POC divides a typical flow into 3 individually deployable microservices:\n\n* [hl7-ingress](hl7-ingress) - an MLLP/HL7 collector of events\n* [hl7-transform-1](hl7-transform-1) - able to transform HL7 payloads from one message to another\n* [hl7-consumer-1](hl7-consumer-1) - able to marshal HL7 payloads and send to downstream systems, EHR, etc\n\nWe also leverage [ActiveMQ](http://activemq.apache.org) to provide resilient/guaranteed messaging in a Staged Event Driven Architecture pattern. \n \nWith these building blocks, we can build a powerful physical deployment that has proven to withstand faults, invalid formats, network connectivity issues, failover, and perform well above expected performance (or legacy performance) metrics. \n\n## JBoss Fuse \nFor this POC, we will build out the following architecture locally (on our laptops) but do so using process-isolation constructs to illustrate a physical deployment. Physical deployments can very based on resources you have (VMs, CPU//mem, etc). For illustration purposes, this is the architecture we will start with for this POC:\n\n![sample architecture](docs/images/example-arch.png)\n\nIn this architecture we see these relevant components:\n\n* 3 fuse instances, isolated at the process level\n* 2 ActiveMQ brokers, in a master/slave set up\n* 1 Fabric8 node which manages deployments, master/slave elections, versions, service discovery, etc.\n\nNote, that this is the use case depicted in this POC, though is intended to help the reader understand the components and concepts at a high level. A typical deployment in a production-like setting is NOT being depicted above, however, you may be able to deduct what a more resilient environment may look like based on the pieces. Also note, with Fuse and how we've architected these services, we can choose *how* we want to deploy. In this POC we've chosen to deploy the components into individual processes but this is not a technical rule. We can deploy them all into the same process as well (though it may or may not be recommended depending on your desired architecture).\n\n### Fuse insight!\nAnother alternative deployment depicted by this POC is the following:\n\n![sample architecture](docs/images/insight-arch.png)\n\nIn this depiction, we have the same above deployment of Fuse and ActiveMQ, but we also have 3 additional nodes which provide a highly-scalable, centralized logging and insight framework built on top of [Elasticsearch](https://github.com/elastic/elasticsearch). With Fuse, we can spin up \"Fuse Insight\" nodes and have all logging dumped into one spot and then use the Fuse web console to query, chart, and graph the results of calls/transactions that have propogated through the platform including debugging and SLA diagnosis. \n\n## Getting Started\nTo get started learning about how this POC is put together, [jump to the Getting Started docs](docs/getting-started.md)\n\n"
 },
 {
  "repo": "GoogleCloudPlatform/healthcare-federated-access-services",
  "language": "Go",
  "readme_contents": "# `healthcare-federated-access-services`\n\n## Purpose\n\nThe Global Alliance for Genomics and Health (\"GA4GH\") has [launched](https://www.ga4gh.org/news/ga4gh-passports-and-the-authorization-and-authentication-infrastructure/) an open standard for requesting and granting access to genomic datasets, known as the \"GA4GH Passport\". This allows different identity providers and data hosts to interact with each other, independent of their hosting platform and identity provider. For example, the owner of a genomic dataset hosted on Google Cloud (e.g. a national genomics institute) can grant access to a researcher with an organizational identity (e.g. an academic or corporate email address) via a GA4GH passport.\n\nThe GA4GH Passport specification is a technology to eliminate barriers between users and data, even in complex multi-cloud and hybrid-cloud environments, while still adhering to data consents and strict sharing policies between the parties involved.\n\n### Data Access Manager\n\nThis repository contains the Data Access Manager (\"DAM\"), which performs the role of a [GA4GH Passport Clearinghouse](http://bit.ly/ga4gh-passport-v1#passport-clearinghouse).\n\n#### The problem\n\nSensitive data is often organized in controlled-access datasets where only qualified individuals or organizations should have access. Data controllers must identify these data accessors ahead of time, and then configure their datasets to permit access. This manual and error-prone process slows down collaboration and can make some use-cases impossible.\n\n#### The solution\n\nGA4GH Passports are a standard way to securely communicate information between data controllers and data accessors. The Data Access Manager (DAM) enables data controllers to seamlessly leverage GA4GH passports to make their data accessible, but also secure.\n\nDAM enables the translation of abstract qualifications (e.g. I am a physician, I am an academic researcher, etc) into platform-specific access management configurations (e.g. I can access this file, I can run this operation). Once an administrator configures DAM with policies describing how qualifications should translate into data access (e.g. academic researchers should have access to files A and B, but not C), verification of those qualifications and the resulting reconfiguration of underlying permissions will occur automatically as data access requests are received.\n\nDAM evaluates identities against policies in real-time, which means data controllers do not need to have a relationship with data accessors \u2013 in fact, data controllers and accessors do not need to know one another exist prior to a transaction. DAM provides the option for data accessors to be billed directly for expenses associated with their requests, rather than those costs being incurred by the data controller. DAM is designed to work as a component within a broader data hosting platform, and also as a standalone service.\n\n### Identity Concentrator\n\nThis repository contains the Identity Concentrator (\"IC\"), which performs the role of a [GA4GH Passport Broker](http://bit.ly/ga4gh-passport-v1#passport-broker)\n\n#### The problem\n\nIn order to access controlled-access datasets, data accessors must prove to data controllers that they have the qualifications required by the data controller. This is done by submitting an application to the data controller who manually reviews the information provided. If acceptable, the data controller adds the data accessor to an allowlist or other static access control mechanism. The data accessor must then use the specific identity (e.g. a Google Cloud credential) for which the access was granted. The data accessor must repeat this process for each dataset that they wish to work with. This results in data accessors accumulating many disparate identities, each specific to a different data controller.\n\n#### The solution\n\nThe IC is an open-source service that securely combines identity qualifications (e.g. I am an academic researcher, I am a physician, I have taken ethics training XYZ, etc) collected from disparate sources into a single identity that can be used to access controlled-access datasets. Without the IC, data accessors must obtain and manage identities that are specific to a given data controller (e.g. a data controller hosting data on Google Cloud may have required data accessors to obtain Google Cloud credentials rather than using their existing corporate or academic credential).\n\nBecause data accessors often require access to data siloed across many locations, data accessors must shift between identities to obtain the data that they need. This makes running complex workflows that depend on data from diverse sources challenging and unreliable. With IC, data accessors (and the workbench platforms that they use) are able to combine relevant identities before executing a given workflow. This enables the workflow to leverage all data that the data accessor is permitted to access, regardless of how fragmented their identity qualifications may be. IC is designed to work as a component within a broader platform, but can also be deployed as a standalone service.\n\nSome datasets will have visa requirements that can be collected from multiple sources, but need to be presented on one passport. The IC can combine lists of visas pertaining to one user from various visa sources.\n\nFor more information, visit:\n\n*  [GA4GH Overview of Passports](http://bit.ly/ga4gh-passport-v1#overview) and\n   [GA4GH Researcher Identity Introduction](http://bit.ly/ga4gh-ri-intro).\n*  [GA4GH Passport v1.0](http://bit.ly/ga4gh-passport-v1) full specification.\n*  [GA4GH AAI OpenID Connect Profile v1.0](http://bit.ly/ga4gh-aai-profile) specification.\n*  [GA4GH](https://www.ga4gh.org/)\n\n## Contributing to the repository\n\nFor information on how to contribute to the repository, see [How to Contribute](CONTRIBUTING.md).\n\n## Notice\n\nThis is not an officially supported Google product.\n\n## How to Deploy\n\nFor information on how to deploy Federated Access, see [How To Deploy a\nFederated Access Playground](docs/playground/deploy.md).\nThe `deploy.bash` script is designed to get a test environment up and running\nquickly and make it easy to develop services that use them in a non-sensitive\nenvironment.\n\nWhen planning the next phase where these services need to be prepared for a\nproduction environment with live, sensitive data, the [productionization\ndocumentation](docs/shared/admin/productionization.md) can be helpful.\n\n## Troubleshooting\n\nSee the [how-to](docs/shared/admin/howto.md) guide.\n\n## Configuration\n\nFor DAM:\n*  **Documentation**: [DAM Configuration](docs/dam/admin/README.md)\n*  **Example Configurations**: see [deploy/config/dam-template](deploy/config/dam-template)\n*  **Config Definitions**: see [DamConfig](proto/dam/v1/dam_service.proto)\n\nFor IC:\n*  **Documentation**: [IC Configuration](docs/ic/admin/README.md)\n*  **Example Configurations**: [deploy/config/ic-template](deploy/config/ic-template)\n*  **Config Definitions**: see [IcConfig](proto/ic/v1/ic_service.proto)\n\n## Test Personas\n\nTest Personas are a means to create mock test users that are defined to hold a set of visas. The DAM can use test personas to verify that access privileges behave as expected for users with such visas. Each test persona reports an \"access list\" that describes the resources and roles their visas provide access to.\n\nA playground environment includes a Test Persona Broker (\"Persona Broker\") that allows users to impersonate Test Personas. If the DAM and IC are both set up to trust a Persona Broker, then end to end tests and training can be conducted.\n\n**Note:** Production deployments of DAM and IC should never be configured to trust Persona Brokers. However, production DAMs can still use Test Personas to verify access without allowing users to impersonate them.\n\n## APIs\n\nFor information about API endpoints available in Federated Access components,\nplease refer to [API documentation](apis.md).\n\n## Bugs, feature requests and general feedback\n\nPlease consult the open [issues](https://github.com/GoogleCloudPlatform/healthcare-federated-access-services/issues), or file a new issue. Your feedback is appreciated!\n"
 },
 {
  "repo": "nhs-r-community/NHSRdatasets",
  "language": "R",
  "readme_contents": "---\noutput: github_document\n---\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\n\n# NHS R-community Datasets <a href='https://nhsrcommunity.com/'><img src='https://nhs-r-community.github.io/assets/logo/nhsr-logo.png' align=\"right\" height=\"80\" /></a>\n\n\n<!-- badges: start -->\n[![R-CMD-check](https://github.com/nhs-r-community/NHSRdatasets/workflows/R-CMD-check/badge.svg)](https://github.com/nhs-r-community/NHSRdatasets/actions)\n[![Project Status: Active \u2013 The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)\n[![CRAN version](https://www.r-pkg.org/badges/version/NHSRdatasets)](https://cran.r-project.org/package=NHSRdatasets)\n[![Downloads](https://cranlogs.r-pkg.org/badges/grand-total/NHSRdatasets)](https://cran.r-project.org/package=NHSRdatasets)\n<!-- badges: end -->\n \n<br><br>\n \nPlease visit: [nhsrcommunity.com](https://nhsrcommunity.com/)\n\n## Datasources for reuse\n\nThis package has been created to help NHS, Public Health and related analysts/data scientists learn to use `R`. It contains several free datasets (just one at the moment), help files explaining their structure, and `vignette` examples of their use.  We encourage contributions to the package, both to expand the set of training material, but also as development for newer `R`/github users as a first contribution.  Please add relevant free, open source data sets that you think may benefit the NHS R-community.\n\n## Installation instructions\n\nThis packages is available on CRAN or the development version can be installed from source, via this Github repository.  You will need [`Rtools`](https://cran.r-project.org/bin/windows/Rtools/) installed to build the package, and the `remotes` package.\n\n```{r install, eval=FALSE}\nremotes::install_github(\"https://github.com/nhs-r-community/NHSRdatasets\")\n```\n\n## Contribution\n\nPlease contribute to this repository, and please cite it when you use it in training or publications.\n\n__To contribute, please:__\n\n* Fork the repository.\n* Add your dataset in the `data` folder, in `.rda` format.  The best way to do this is with the `usethis` package with \"gzip\" compression:  `usethis::use_data(data, compress=\"gzip\")` \n* Please add a minimal `R` function to act as a help file. You can use the `LOS_model` as a guide.\n* Please add a `vignette` demonstrating how the data has been/can be used.\n* Create a pull request, detailing your additions, and we will review it before merging.\n\n<br>\n___When contributing a dataset, the contributor certifies that:___\n\n* They are the data owner, or are authorised to republish the dataset in question.\n* The dataset does not contain real patient-level data.\n* Where based on patient data, the contributor takes full responsibility for sharing the data and certifies that. it is has been processed, anonymised, aggregated or otherwise protected in accordance with all legal requirements under General Data Protection Regulation (GDPR), or other relevant legislation.\n\n\nPlease note that the 'NHSRdatasets' project is released with a\n[Contributor Code of Conduct](https://github.com/nhs-r-community/NHSRdatasets/blob/master/CODE_OF_CONDUCT.md).\nBy contributing to this project, you agree to abide by its terms.\n\n"
 },
 {
  "repo": "RasaHQ/medicare_locator",
  "language": "Python",
  "readme_contents": "\ufeff# Medicare Locator built with the Rasa Stack\n\n## \ud83c\udfe5 Introduction\n\nThis is an open source starter pack for developers to show how to automate full conversations in healthcare sector.\n\nIt supports the following user goals:\n\n- Searching for a hospital, nursing home or home health agency in a US city.\n- Handling basic chitchat.\n\n## \ud83d\udcbe How to install and setup Medicare Locator\n\n**Step 1**: To install Medicare Locator, please clone the repo:\n```\ngit clone https://github.com/RasaHQ/medicare_locator.git\ncd medicare_locator\n```\nThe Medicare Locator uses **Python 3.5 and 3.6** and has not been tested with other versions.\nUse the requirements.txt file to install the appropriate dependencies\nvia pip. If you do not have pip installed yet first do:\n```\nsudo easy_install pip\n```\notherwise move to the next step directly.\n\n**Step 2**: Install requirements:\n```\npip install -r requirements.txt\n```\n\n**Step 3**: Install the spaCy English language model by running:\n```\npython3 -m spacy download en\n```\n\nThis will install the bot and all of its requirements.\n\n## \ud83e\udd16 How to run Medicare Locator\n\n**Step 1**: Train the core model by running:\n```\nmake train-core\n```\nThis will train the Rasa Core model and store it inside the `/models/current/dialogue` folder of your project directory.\n\n**Step 2**: Train the NLU model by running:\n```\nmake train-nlu\n```\nThis will train the NLU model and store it inside the `/models/current/nlu` folder of your project directory.\n\n**Step 3**: In a new terminal start the server for the custom action by running:\n```\nmake action-server\n```\n\n**Step 4**: Now to test the Medicare Locator with both these models you can run:\n```\nmake cmdline\n```\nAfter the bot has loaded you can start chatting to it. If you start by saying `Hi` for example,\nthe bot will reply by asking you what you are looking for and show you a number of options in form of buttons.\nSince those buttons do not show when testing the bot in the command line, you can imitate a button click by copy\nand pasting the intent of the button of your choice as your input.\n\nAn example conversation in the command line could look something like this:\n```\nYour input ->  Hi\nHi. What are you looking for ?\nButtons:\n1: Hospital (/inform{\"selected_type_slot\": \"rbry-mqwu\"})\n2: Nursing Home (/inform{\"selected_type_slot\": \"b27b-2uc7\"})\n3: Home Health Agency (/inform{\"selected_type_slot\": \"9wzi-peqs\"})\nYour input ->  /inform{\"selected_type_slot\": \"rbry-mqwu\"}\nWhat is you current city?\nYour input ->  Seattle\n...\n```\n\nTry out different conversations and see what the current state of the bot can do!\nAfter playing around a bit you can try to modify and extend the bot by adding custom actions and intents for example.\nFind help for this in the [Rasa Docs](https://rasa.com/docs/).\n\nA helpful option to extend training data and get to know your bot is interactive learning,\nhere you can correct your bot at every step in the conversation and automatically save the data for future training.\n\n**Step 5**: To run Medicare Locator in interactive learning mode run:\n```\nmake interactive\n```\n\n## \ud83d\udcf1 Use Telegram as Chat platform\nIn order to chat to the Medicare Locator through Telegram you can do the following:\n\n**Step 1**: First if you don't already use Telegram, download it and set it up with your phone.\nOnce you are registered with Telegram you start by setting up a Telegram bot.\n\n**Step 2**: To setup your own bot go to the [Telegram BotFather](https://web.telegram.org/#/im?p=@BotFather),\nenter `/newbot` and follow the instructions.\nYou should get your `access_token`, and the username you set will be your `verify`. Save this information as you will need it later.\n\n**Step 3**: Now you will need to connect to Telegram via a webhook. To create a local webhook from your machine you can use [Ngrok](https://ngrok.com/). Follow the instructions on their site to\nset it up on your computer. Move `ngrok` to your working directory and in a new terminal run:\n```\n./ngrok http 5005\n```\nNgrok will create a https address for your computer. For Telegram you need the address in this format:\n`https://xxxxxx.ngrok.io/webhooks/telegram/webhook`\n\n**Step 4**: Go to the *credentials.yml* file that you downloaded from the repo and input your personal `access_token`, `verify` and `webhook_url`.\nYou will have to update the `webhook_url` everytime you do redo Step 3, the `access_token` and `verify` will stay the same.\n\n**Step 5**: In a new terminal start the server for the custom action by running:\n```\nmake action-server\n```\n\n**Step 6**: In a new terminal connect to Telegram by running:\n```\nmake telegram\n```\n\n**Step 7**: Now you and anyone on Telegram are able to chat to your bot. You can find it by searching for its name on Telegram.\n\nDetailed information about this can also be found in the [Rasa Docs](https://rasa.com/docs/core/connectors/#telegram-connector).\n\n\n## More about the Medicare Locator demo bot\nThere are some custom actions that require connections to external services,\nspecifically `FacilityForm` and `FindHealthCareAddress`. These two actions\nconnect to Medicare APIs. These APIs do not require tokens or any form of authentication.\n\nFor more information about Medicare APIs please visit [data.medicare.gov](https://data.medicare.gov/)\n\nIf you would like to run Medicare Locator on your website, follow the instructions\n[here](https://github.com/mrbot-ai/rasa-webchat) to place the chat widget on\nyour website.\n\n\n## \ud83d\udc69\u200d\ud83d\udcbb Overview of the files\n\n`data/core/` - contains stories for Rasa Core\n\n`data/nlu_data.md` - contains example NLU training data\n\n`actions.py` - contains custom action/api code\n\n`domain.yml` - the domain file for Core\n\n`nlu_config.yml` - the NLU config file\n\n`core_config.yml` - the Core config file\n\n`credentials.yml` - contains credentials for the use with Telegram\n\n`endpoints.yml` - contains url for endpoint\n\n## \ud83d\udee0 Makefile overview\nRun `make help` to see an overview of all make commands available.\n\n`train-nlu` - Train the NLU model.\n\n`train-core` - Train the Core model.\n\n`interactive` - Run the Medicare Locator interactive learning mode.\n\n`cmdline` - Run the bot on the command line.\n\n`action-server` - Start the action server.\n\n`telegram` - Run the bot in the Telegram channel.\n\n## :gift: License\nLicensed under the GNU General Public License v3. Copyright 2019 Rasa Technologies\nGmbH. [Copy of the license](https://github.com/RasaHQ/rasa-demo/blob/master/LICENSE).\nLicensees may convey the work under this license. There is no warranty for the work.\n"
 },
 {
  "repo": "ZainMustafaaa/HealthCare-Scan-Nearby-Hospital-Locations",
  "language": "Java",
  "readme_contents": "# HealthCare-Nearby-Hospital-Locations\nI developed this android application to help beginner developers to know how to use google maps API and how to convert JSON data into Java object.\n## Functionality\n1. Hard-Coded solutions for common medical problems\n2. Scan nearby hospital locations from your current location by using GPS and Google Maps API.\n## Getting Started\nThis application is uploaded for learning purpose you just clone this project to your android studio and make sure to add all dependencies  to your project file.\n## Manifest\n  1. uses-permission android:name=\"android.permission.INTERNET\"\n  2. uses-permission android:name=\"android.permission.ACCESS_COARSE_LOCATION\"\n  3. uses-permission android:name=\"android.permission.ACCESS_FINE_LOCATION\"\n  4. uses-permission android:name=\"android.permission.ACCESS_NETWORK_STATE\"\n## Dependencies\n  1. compile 'com.google.android.gms:play-services-location:11.0.1'\n  2. compile 'com.google.android.gms:play-services:11.0.2'\n  3. compile files('libs/json-20170516.jar')\n## Built with\nAndroid Studio 2.3.3\n"
 },
 {
  "repo": "leonibr/healthcaredemo",
  "language": "C#",
  "readme_contents": "[![Gitpod ready-to-code](https://img.shields.io/badge/Gitpod-ready--to--code-blue?logo=gitpod)](https://gitpod.io/#https://github.com/leonibr/healthcaredemo)\n\n### Realtime - HealthCare Central\n\nStart by opening multiple browser windows and see all being updated accordingly\n\n## [Online demo here](https://healthcare.marques.top)\n\n### Quickstart\n\n```bash\ngit clone https://github.com/leonibr/healthcaredemo\n```\n\n```bash\ncd healthcaredemo\n```\n\nNow there is two options:\n\n- Using docker:\n\n```bash\ndocker-compose run --service-ports healthcare_demo\n# it uses port 5015\n```\n\n- Or using dotnet cli:\n\n```bash\n dotnet run --project Host\\Host.csproj\n# it uses port 5005\n# watch out for folder navigation in your OS if not Windows\n```\n\n## Or check the screenshots:\n\n1. Home Screen\n   ![homescreen.png](homescreen.png)\n   ![rate](ocuprate.png)\n\n1. Place patients by dragging and droping\n   ![healthcare_centrarl.gif](healthcare_central.gif)\n\n#### Objective:\n\n- simulate realtime administration of hospitals\n\n#### Scenario:\n\n- Operators can place or remove patients on hospital beds\n- Operators do not register patients to waiting list, to simulate new patient to the list click: \"Add Patient to Waiting List\"\n- Operators get always updated data whenever is ivalidated by other operator (or other browser window)\n- The time is show as duration\n\n#### Constraints:\n\n- Dual mode (WebAssembly and SSB)\n- Dragging and Droping without external lib.\n\n#### Modify whatever you like! but please share if you can! \ud83d\ude09\n\n- It was built on top of Fusion's MudBlazor template.\n\n### Changes:\n\n    - updated to .net 7 and Fusion 4.1.38\n\n### Road map:\n\n- Maybe a dashboard\n\n## License\n\n- MIT\n"
 },
 {
  "repo": "swar-it/D3.js-Dash",
  "language": null,
  "readme_contents": "# D3.js and SQLite #\n## Data visualization and Proof of Concept ##\nA simple hospital dashboard illustrating concurrent clinical operations. Written predominately in Javascript and conforming to MVC, it uses a SQLite data store and some simple PHP accessors.\n[![image](./Screen Shot 2014-06-24 at 3.04.52 PM.png)](http://colinwhite.net/Dash2.5/) \n\nBased on the following Creative Commons and Open Source prior work .\n\n#### HTML5 Boilerplate ####\nby Catalin Maris [https://github.com/alrra](https://github.com/kamisama/cal-heatmap) \n\n#### D3.js Data Driven Documents ####\nby Mike Bostock\n\n[https://github.com/mbostock/d3](https://github.com/mbostock/d3)\n\n[https://github.com/mbostock/queue](https://github.com/mbostock/queue)\n\n[https://github.com/mbostock/topojson](https://github.com/mbostock/topojson)\n\n[https://github.com/d3/d3-plugins/tree/master/sankey](https://github.com/d3/d3-plugins/tree/master/sankey)\n\n#### D3.js Tool Tips ####\nby Justin Palmer [https://github.com/Caged/d3-tip](https://github.com/Caged/d3-tip)\n\n#### Cal-HeatMap ####\nby Wan Qi Chen [https://github.com/kamisama/cal-heatmap](https://github.com/kamisama/cal-heatmap) \n\n#### Special thanks...####\nLars Kuttohff\n\n[http://stackoverflow.com/users/1172002/lars-kotthoff](http://stackoverflow.com/users/1172002/lars-kotthoff)"
 },
 {
  "repo": "advikmaniar/ML-Healthcare-Web-App",
  "language": "Python",
  "readme_contents": "# ML-Healthcare-Web-App\n\nThis is an interactive Machine Learning Web App \"ML in Healthcare\" developed using Python and StreamLit. It uses ML algorithms to build powerful and accurate models to predict the risk (High / Low) of the user of having a Heart Attack or Breast Cancer based on the user's specific attributes like age, sex, heart rate, blood sugar, etc.\n\n<h2><b> View App Here: </b></h2>\n\n[![StreamLit App](https://static.streamlit.io/badges/streamlit_badge_white.svg)](https://share.streamlit.io/advikmaniar/ml-healthcare-web-app/main/ML_Healthcare.py)\n\n<hr>\n\nThis applications has two basic sections:\n\n<h2>1) - Model Building </h2>\nIn this section 7 different models are built using different ML algorithms. They are: \n\n```\n1. Logistic Regression \n2. KNN\n3. SVM \n4. Decision Trees \n5. Random Forest \n6. Gradient Boosting \n7. XGBoost\n```\nThe models are trained using data from https://archive.ics.uci.edu/ml/index.php, particularly the [Heart Attack Prediction](https://github.com/advikmaniar/ML-Heathcare-Web-App/blob/main/Data/heart.csv) and [Breast Cancer (Wisconsin)](https://github.com/advikmaniar/ML-Heathcare-Web-App/blob/main/Data/BreastCancer.csv) datasets.\n\nAn interactive side-dashboard is created using the streamlit `st.sidebar` call which enables the user to do the following:\n1. Choose dataset - `Heart Attack / Breast Cancer`\n2. Choose algorithm - `Logistic Regression , KNN , SVM , Decision Trees , Random Forest , Gradient Boosting , XGBoost.`\n3. Change the important parameters for each model - `Learning Rate, Random State, Regularization Coeff, Gamma, Kernel, n_estimators` etc. \n\nAfter training using the parameters selected by the user, the tuned model is built and ready to be tested on our testing data. The classification plot and confusion matrix is displayed for the model selected along with the model metrics: `Accuracy, Precision, Recall, F1-Score, Mean Squared Error, Execution Time`. The user can observe real-time changes in the plots and metrics as they change the model parameters further. \n> **This is a great way to understand the different ML algorithms and how they are influenced by tuning the hyperparameters.**\n> \n![image](https://user-images.githubusercontent.com/72503778/123002403-85b73700-d3cf-11eb-80a1-71262561b9c8.png)\n\nThe 7 models (optimum tuning) performed as follows: <br>\n`Criterion: Accuracy`\nModel | Accuracy (Heart Attack / Breast Cancer)\n------------ | -------------\nLogistic Regression | **91.803% / 100.0%**\nKNN | **86.89% / 96.49%**\nSVM | **93.44% / 100.0%**\nDecision Trees | **52.56% / 60.53%**\nRandom Forest | **90.164% / 98.24%**\nGradient Boosting | **88.53% / 96.49%**\nXGBoost | **95.08% / 94.737%**\n\n<h2>2) - User Prediction </h2>\nIn this section, the user can use any model developed above to predict their status (High Risk / Low Risk) using their own values. (Either for Heart Attack or Breast Cancer)\n\n![image](https://user-images.githubusercontent.com/72503778/123003157-6d93e780-d3d0-11eb-81fc-8dd6abe89efa.png)\n\n![image](https://user-images.githubusercontent.com/72503778/123003260-93b98780-d3d0-11eb-9ff0-bb27da6a105e.png)\n\n\nView the final video [here](https://github.com/advikmaniar/ML-Healthcare-Web-App/blob/main/Results/Video.mp4).\n<hr>\n\n<h1> Thank You! </h1>\n\n<hr>\n\n\n\n"
 },
 {
  "repo": "no13bus/baymax",
  "language": "Python",
  "readme_contents": "baymax\r\n========\r\n\r\nBuild your personal life database, including of internet, taxi, sports and health data analysis.\r\n\r\n[\u4e2d\u6587\u6587\u6863](https://github.com/no13bus/baymax/blob/master/README_CN.md)\r\n\r\n# Let's start\r\n- pip install -r requirements.txt\r\n- Modify the config file, such as celery, MySQL configuration, APP callback address and apps' client_id.\r\n- python manage.py initdb\r\n- python manage.py insert\r\n- python manage.py run\r\n\r\n# Deployment\r\n- nginx+gunicorn+supervisor\r\n\r\n# Which app we support now\r\n- [GitHub](http://github.com)\r\n- [Health App: Fitbit](https://dev.fitbit.com)\r\n- [rescuetime](https://www.rescuetime.com/developers)\r\n\r\n# Which app we will support\r\n- [ledongli](http://ledongli.cn)\r\n- [bong](http://www.bong.cn/)\r\n- [xiaomi](http://www.mi.com/shouhuan)\r\n- [Nike+](https://developer.nike.com/index.html)\r\n- [Moves](https://dev.moves-app.com/)\r\n- [Withings](http://oauth.withings.com/api)\r\n- [Uber](http://uber.com)\r\n- ..............\r\n\r\n# Mechanism\r\n- Users first use GitHub account to sign in, after we get the authentication for the apps, \r\nthe backend will to grab the app life data every day.\r\n- And then baymax will display the life data, including of walking, running, surfing the net time distribution, \r\nGitHub code submitted times and statistics.\r\n\r\n\r\n# The tech we use\r\n- Flask\r\n- sqlalchemy\r\n- Bootstrap\r\n- celery\r\n- redis\r\n\r\n# change log\r\n### version 0.1\r\n- update the framework version. Flask2.x Python3.x\r\n\r\n\r\n# Screen\r\n![1](https://raw.githubusercontent.com/no13bus/baymax/master/screen/1.png)\r\n![2](https://raw.githubusercontent.com/no13bus/baymax/master/screen/2.png)\r\n![3](https://raw.githubusercontent.com/no13bus/baymax/master/screen/3.png)\r\n![4](https://raw.githubusercontent.com/no13bus/baymax/master/screen/4.png)\r\n![5](https://raw.githubusercontent.com/no13bus/baymax/master/screen/5.png)\r\n![6](https://raw.githubusercontent.com/no13bus/baymax/master/screen/6.png)\r\n![7](https://raw.githubusercontent.com/no13bus/baymax/master/screen/7.png)\r\n"
 },
 {
  "repo": "OpenConceptLab/oclapi",
  "language": "Python",
  "readme_contents": "# This project has been deprecated. Please see https://github.com/OpenConceptLab/oclapi2 for the latest version.\n\nOCL API\n======\n\n## Contributing\n\nWe welcome all pull requests. Before starting any work, please check https://github.com/OpenConceptLab/ocl_issues/issues if the change you want to work on has been already reported there. If it's not there, create a new issue so that it can be discussed. We should triage the issue and get back to you in a few days.\n\nAll pull requests should contain a single commit (unless you have a very good reason to include multiple commits). A commit message should be in the following format: `OpenConceptLab/ocl_issues#id Short title`, where `id` is the issue number e.g. 170. Please always rebase your commit on the master branch and make sure all tests pass, before creating a pull request.\n\n## What you'll need:\n* git\n* docker-compose\n\nSource for the Open Concept Lab APIs\n\n## Docker Environment Setup (preferred)\n\nFork the repo on github and clone your fork:\n````sh\ngit clone https://github.com/{youruser}/oclapi\n````\n\nAdd a remote repo to upstream in order to be able to fetch updates:\n````sh\ngit remote add upstream https://github.com/OpenConceptLab/oclapi\n````\n\nGo to:\n````sh\ncd oclapi\n````\n\nBuild containers explicitly (only the first time to go around oclapi:dev not found):\n````sh\ndocker-compose build\n````\n\nFire up containers:\n````sh\ndocker-compose up\n````\n\nYou can access the API at http://localhost:8000\n\nThe root password and the API token can be found in docker-compose.yml under api/environment.\n\n### Docker Environment Settings\n\nDocker `.env` file should be located under the root project folder. On development environment you don't need this file.\n\n#### .env file details\n\n`ENVIRONMENT=` Python module for environment, e.g. production, staging, local, qa\n\n`AWS_ACCESS_KEY_ID=` Amazon Web Services access key.\n\n`AWS_SECRET_ACCESS_KEY=` Amazon Web Services secret key.\n\n`AWS_STORAGE_BUCKET_NAME=` Amazon Web Services bucket name.\n\n`ROOT_PASSWORD=` API root user password.\n\n`OCL_API_TOKEN=` API root token.\n\n`SECRET_KEY=` DJANGO secret key.\n\n`EMAIL_HOST_PASSWORD=` no-reply@openconceptlab.org password.\n\n`SENTRY_DSN=` Sentry unique URL for the given environment.\n\n`IMPORT_DEMO_DATA=` Set to 'true' to import ~2k concepts from the CIEL demo data.\n\n`FLOWER_USER=` Flower user (Default value - floweruser).\n\n`FLOWER_PWD=` Flower password (Default value - Flower123).\n\n### Running commands in a container\n\nYou can run any command in a running container. Open up a new terminal and run for example:\n````sh\ndocker-compose exec api python manage.py syncdb\n````\n\n### Running tests in a container\nYou can run tests in a container as any other command.\n\n#### Unit Tests\n\n````sh\ndocker-compose run --rm api python manage.py run_test --configuration=Dev\n````\n\n#### Integration Tests\n\nSee integration-tests/README.md\n\nTo run locally:\n````sh\ndocker-comopse up -d\ndocker build integration-tests/. --network=\"host\" --build-arg CACHEBUST=$(date +%s)\n````\n\nDeprecated integration tests can be run with:\n````sh\ndocker-compose run --rm api python manage.py test integration_tests --configuration=Dev\n````\n\n#### Rebuilding SOLR index\n\nIf the SOLR index gets out of sync, you can run the following command:\n````sh\ndocker-compose run --rm -d api python manage.py rebuild_index --batch-size 100 --workers 4 --verbosity 2 --noinput\n````\nIt's asynchronous. To follow logs run:\n````sh\ndocker logs -f oclapistg_api_run_1\n````\n, where oclapistg_api_run_1 is the container id returned by the `run` command.\n\n### Backups\n\nBy default backups are taken every night at midnight. You can trigger a manual backup by running:\n````sh\ndocker-compose run --rm backup bash backup.sh\n````\nBackups are saved in a backup directory configured via the BACKUP_DIR env property (./backups by default).\nYou can restore a particular backup by running:\n````sh\ndocker-compose run --rm backup bash restore.sh 2017-09-27_00-00-01\n````\n### Connecting to mongo in container\n\n````sh\ndocker-compose exec mongo mongo\n````\n\n### Debugging in container\n\nTo setup debugging PyCharm Professional Edition is required.\n\nDocker-compose up starts the server in a development mode by default. It exposes all services on the host machine as well as enables SSH to the API service.\n\nIn Pycharm IDE open oclapi project and go to `Settings-> Project: oclapi -> Project Interpreter`\n\nClick on gear icon and choose `Add Remote` option\n\nConfigure interpreter with SSH credentials as in the image (default password is `Root123`):\n\n![alt](img/remote_interpreter_config.png)\n\nThere will be warnings about unknown host etc. but don't don't worry, just confirm.\n\nSetup django debug configuration as in the image (Path mapping should be `absolute path to project directory=/code`):\n\n![alt](img/docker_debug_config.png)\n\nRun your configuration! Debugging server will run on [http://0.0.0.0:8001/](http://0.0.0.0:8001/)\n\nIn case of any problems with `.pycharm_helpers` just delete remote interpreter and create new with same configuration, it will write pycharm helpers in Your ocl container again.\n\n## Continuous Integration\n\nThe project is built by CI at https://ci.openmrs.org/browse/OCL\n\nYou can see 3 plans there:\n* OCL API\n* OCL WEB\n* OCL QA UI Tests\n\nOCL API and OCL WEB are triggered by commits to respective repos. First docker images are built and pushed with a nightly tag to dockerhub at https://hub.docker.com/u/openconceptlab/dashboard/. Next unit and integration tests are being run. Finally a qa tag is being pushed to dockerhub and deployed to https://ocl-qa.openmrs.org/. On each deployment data is wiped out of the qa environment. You can login to the server using username 'admin' and password 'Admin123'.\n\n### Deploying to staging and production\n\nIf you want to deploy to staging or production, you need to be logged in to Bamboo. Please request access via helpdesk@openmrs.org\n\n1. Go to https://ci.openmrs.org/browse/OCL and click the cloud icon next to the project you want to deploy.\n2. Click the related deployment plan.\n3. Click the cloud icon next in the actions column for the chosen environment.\n4. Choose whether to create a new release from build result or redeploy an existing release. You will choose the latter when promoting a release from staging to production, downgrading to a previous release or restarting services.\n5. When creating a new release, choose the build result, which you want to deploy (usually the latest successful build). Leave the release title unchanged and click the Start deployment button.\n6. Wait for the release to complete.\n\n### Importing CIEL to staging and production\n\nIn order to import a newer version of the CIEL dictionary you need to have an SSH root access to staging.openconceptlab.org and openconceptlab.org.\nDownload the zip file with concepts and mappings in the OCL format and run the following commands for staging:\n```sh\nsudo -s\ncd /root/docker/oclapi-stg \nunzip /path/to/zip/ciel_20180223.zip\ndocker-compose run -d --rm -v /root/docker/oclapi-stg:/ciel api python manage.py import_concepts_to_source --source 57cd60e2ba0d489c55039465 --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_concepts.json\ndocker logs -f oclapistg_api_run_1\ndocker-compose run -d --rm -v /root/docker/oclapi-stg:/ciel api python manage.py import_mappings_to_source --source 57cd60e2ba0d489c55039465 --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_mappings.json\ndocker logs -f oclapistg_api_run_2\n```\n\nOr for production:\n```sh\nsudo -s\ncd /root/docker/oclapi-prd \nunzip /path/to/zip/ciel_20180223.zip\ndocker-compose run -d --rm -v /root/docker/oclapi-prd:/ciel api python manage.py import_concepts_to_source --source 5821b7a564d700001440f44a --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_concepts.json\ndocker logs -f oclapiprd_api_run_1\ndocker-compose run -d --rm -v /root/docker/oclapi-prd:/ciel api python manage.py import_mappings_to_source --source 5821b7a564d700001440f44a --token REPLACE_WITH_ROOT_API_TOKEN --retire-missing-records /ciel/ciel_20180223_mappings.json\ndocker logs -f oclapiprd_api_run_2\n```\n\nImports run in background so you can disconnect from the server any time, but note that you must wait for concepts to be imported before importing mappings. You can get back to logs at any point by running: `docker logs -f CONTAINER_NAME`.\n\n## Manual Environment Setup (on a Mac)\n\nFollow this [guide](http://docs.python-guide.org/en/latest/starting/install/osx/) to install Python 2.7\nand set up a virtual environment.  You may wish to name your virtual environment something more descriptive,\nfor example replace:\n\n    virtualenv venv\n\nWith:\n\n    virtualenv oclenv\n\nAnd then run:\n\n    source oclenv/bin/activate\n\n### Mongo\n\nThe OCL API uses MongoDB as its backend datastore.  If you don't have it already, use Homebrew to install it:\n\n    brew install mongodb\n\nOnce installed, use the `mongod` command to start a local instance of the MongoDB server.\nThen, in a separate console window, run `mongo` to start the interactive command-line client.\nUsing the Mongo command-line, create a database named `ocl`:\n\n     > use ocl\n\n### Solr 4.9.0\n\nSolr is used to support searching across OCL API entities.  To download Solr 4.9.0, visit the Solr [mirrors](http://www.apache.org/dyn/closer.cgi/lucene/solr/4.9.0) page and select a mirror.  Then download solr-4.9.0.tgz (NOT solr-4.9.0-src.tgz).\n\nChoose an install directory (e.g. `~/third-party`, henceforth `$INSTALL_DIR`) and extract the tarball there.  You will then need to set 2 environment variables:\n\n       export SOLR_ROOT=$INSTALL_DIR/solr-4.9.0\n       export SOLR_HOME=$OCLAPI_ROOT/solr\n\n`$OCLAPI_ROOT` refers to your Git project root (i.e. the location of this Readme file).\n\nThis should enable you to run `$OCLAPI_ROOT/run_solr.sh`, which starts Solr in a Jetty instance listening on port 8983.  Verify this by visiting:\n\n     http://localhost:8983/solr\n\n### The Django Project\n\nClone this repository, and `cd` into the `ocl` directory.\nBefore you can run the server, you will need to execute the following steps:\n\n1. Install the project dependencies:\n\n    pip install -r requirements.txt\n\n2. Use `syncdb` to create your backing Mongo collections.\n\n   ```sh\n   ./manage.py syncdb\n   ```\n\n   If you are starting with a clean Mongo database, `syncdb` will prompt you to create a superuser.\n   Follow that prompt.\n\n   If you are not prompted to create a superuser, or wish to do so later, you can also use the command:\n\n   ```sh\n   ./manage.py createsuperuser\n   ```\n   \n3. Verify your superuser and make note of your token.\n\n   ```sh\n   $ mongo\n   > use ocl\n   > db.auth_user.find({'is_superuser':true})\n   ```\n\n   This should revel the superuser you just created.  Note the user's _id (e.g. `ObjectId(\"528927fb2f3e986be1627d6d\")`),\n   and use it to locate your token:\n\n   ```sh\n   > db.authtoken_token.find({'user_id': ObjectId(\"528927fb2f3e986be1627d6d\")})[0]\n   ```\n\n   Make note of the token `_id` (e.g. `\"20e6ac8fe09129debac2929f4a20a56bea801165\"`).  You will need this to access your endpoints\n   once you start up your server.\n\n4. Run the lightweight web server that ships with Django.\n\n   ./manage.py runserver\n\n   The OCL API should now be running at `http://localhost:8000`.\n\n5. Test an endpoint.\n   \n   Remember, the API uses token-based authentication, so you can't just plug an endpoint into a browser and hit Return.\n   You'll need to use a tool that allows you to specify a header with your request.  One simple example is `curl`:\n\n   ```sh   \n   curl -H \"Authorization: Token c1328d443285f2c933775574e83fe3abfe6d7c0d\" http://localhost:8000/users/\n   ```\n\n   I recommend using the [Advanced REST Client](https://chrome.google.com/webstore/detail/advanced-rest-client/hgmloofddffdnphfgcellkdfbfbjeloo?hl=en-US) app for Chrome.\n   This provides you with a nice editor for passing parameters along with your `POST` and `PUT` requests.\n\n6. Create an API user.\n   \n   Your superuser is not a valid API user, because it was not created via the `POST /users/` operation.\n   However, you can use your superuser to access that endpoint and _create_ an API user:\n\n   ```sh\n   curl -H \"Authorization: Token c1328d443285f2c933775574e83fe3abfe6d7c0d\" -H \"Content-Type: application/json\" -d '{\"username\":\"test\",\"email\":\"test@test.com\", \"name\":\"TestyMcTest\"}' http://localhost:8000/users/   \n   ```\n\n7. (Optional) Make your API user an admin (staff) user.\n\n   Log into the Django admin console with the superuser credentials you established in step 4:\n\n   ```sh\n   http://localhost:8000/admin/\n   ```\n\n   Then navigate to the user list:\n\n   ```sh\n   http://localhost:8000/admin/auth/user/\n   ```\n\n   Select the user you just created, and check the box next to \"staff status\".  Now your user is an admin within the context of the OCL API.\n   \n   \n\n## Data Import Before Concept Creation\nWe need to have data before we go on creating a concept. \n\nThe dropdowns that require preloaded data are Concept Class, Datatype, Name/Description Type, Locale, Map Type. \n\n\n### How to import Data\n1. Create a new org `OCL`. \n2. Create a new user source `Classes` under org `OCL`. This will be be used for Concept Class dropdown.\n3. Import the data as concepts in `Classes` from https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Classes/classes.json .\n\nFollow https://github.com/OpenConceptLab/oclapi/wiki/Bulk-Importing#how-to-import to know how to import concepts in a source.\n\nProceed in same fashion for rest of the dropdown fields. Create sources `Datatypes`, `NameTypes`, `DescriptionTypes`, `Locales`, `MapTypes` under org `OCL`. \n\nRefer to following files for data: \n\nDatatypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Datatypes/datatypes_fixed.json\n\nNameTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_NameTypes/nametypes_fixed.json\n\nDescriptionTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_DescriptionTypes/description_types.json\n\nLocales: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_Locales/locales.json\n\nMapTypes: https://github.com/OpenConceptLab/ocl_import/blob/master/OCL_MapTypes/maptypes_fixed.json\n\n\n---------------------------------------------------------------------\nCopyright (C) 2016 Open Concept Lab. Use of this software is subject\nto the terms of the Mozille Public License v2.0. Open Concept Lab is\nalso distributed under the terms the Healthcare Disclaimer\ndescribed at http://www.openconceptlab.com/license/.\n---------------------------------------------------------------------\n"
 },
 {
  "repo": "aws-quickstart/quickstart-tableau-server-healthcare",
  "language": "Python",
  "readme_contents": "# quickstart-tableau-server-healthcare\n## Tableau Server for Healthcare on the AWS Cloud\n\n**Important note:** You must have an AWS Business Associate Addendum (BAA) in place, and follow its configuration requirements, before running protected health information (PHI) workloads on AWS. For details, see the [deployment guide](https://aws-quickstart.s3.amazonaws.com/quickstart-tableau-server-healthcare/doc/tableau-server-for-healthcare-on-the-aws-cloud.pdf).\n\nThis Quick Start helps you deploy a Tableau Server standalone environment on the AWS Cloud, following best practices from AWS and Tableau Software. Specifically, this environment can help organizations with workloads that fall within the scope of the U.S. Health Insurance Portability and Accountability Act (HIPAA). The Quick Start addresses certain technical requirements in the Privacy, Security, and Breach Notification Rules under the HIPAA Administrative Simplification Regulations (45 C.F.R. Parts 160 and 164). \n\nThe Quick Start includes AWS CloudFormation templates, which automatically configure the Tableau Server environment in less than an hour. The [security controls reference](https://aws-quickstart.s3.amazonaws.com/quickstart-tableau-server-healthcare/assets/HIPAA-Security-Controls-Mapping_TableauServer_181022.xlsx) (Microsoft Excel spreadsheet) shows how Quick Start architecture decisions, components, and configurations map to HIPAA regulatory requirements. The Quick Start also includes a [deployment guide](https://aws-quickstart.s3.amazonaws.com/quickstart-tableau-server-healthcare/doc/tableau-server-for-healthcare-on-the-aws-cloud.pdf), which describes the reference architecture in detail and provides step-by-step instructions for deploying, configuring, and validating the AWS environment.\n\nThe Quick Start offers two deployment options:\n\n- Deploying Tableau Server for healthcare into a new virtual private cloud (VPC) on AWS\n- Deploying Tableau Server for healthcare into an existing VPC on AWS\n\nYou can also use the AWS CloudFormation templates as a starting point for your own implementation.\n\n![Quick Start architecture for Tableau Server for healthcare on AWS](https://d0.awsstatic.com/partner-network/QuickStart/datasheets/tableau-server-healthcare-architecture-on-aws.png)\n\n\nTo post feedback, submit feature ideas, or report bugs, use the **Issues** section of this GitHub repo.\nIf you'd like to submit code for this Quick Start, please review the [AWS Quick Start Contributor's Kit](https://aws-quickstart.github.io/). \n"
 },
 {
  "repo": "codefordenver/encorelink",
  "language": "JavaScript",
  "readme_contents": "# EncoreLink\n\nA web application to connect musicians with healthcare centers.\n\n[![Stories Ready to Work On](https://badge.waffle.io/codefordenver/music-volunteers.svg?label=ready&title=Cards%20Ready%20To%20Work%20On)](https://waffle.io/codefordenver/music-volunteers)\n[![Build Status](https://travis-ci.org/codefordenver/encorelink.svg?branch=master)](https://travis-ci.org/codefordenver/encorelink)\n\n![](./docs/screenshot_2018-11-05.png)\n\n## General Documentation\nThe documents we created when building EncoreLink are [here](https://drive.google.com/drive/folders/0BzPSX8eOfTADckNXd3VIc1U3UUE).\nProbably the most helpful will be:\n\n* [EncoreLink Specification](https://docs.google.com/document/d/1Mwo-pOyveza1XXKrpr966admHFanHzy5Hn2r6ewE3vk/edit#heading=h.6qqugcr09y1p) - this is our current spec\n\n## Developer Documentation\nDeveloper documentation currently lives [here](/docs/DEVDOCS.md)\n\n## Getting Started\n\n### Basic Setup\n\n1. Install [Node.js](https://nodejs.org/) (The official node verion for this app is 6.11, although other modern versions will probably work fine)\n2. Clone this repo\n3. Navigate to your repo folder\n4. Run `npm install`\n5. Get a Google api key with the Google Maps Embed API and the Google Maps JavaScript API enabled (you can probably get this from one of the team members on the project)\n6. Copy `.env.sample` to `.env` and set the `REACT_APP_GOOGLE_API_KEY` environment variable\n7. Run `npm start`\n\nIf this doesn't open a browser for you, you can navigate to http://localhost:3000 to view the app.\nThere is also an api explorer available at http://localhost:54321/explorer/\n\n### Advanced Setup - Postgres\n\nThis is an _optional_ setup to run the app against postgres. This is good if you want to persist data or test how the app interacts with the actual database that we use in production, however, it can be a bit more complicated, so we don't recommend it generally.\n\n1. Install [PostgreSQL](https://www.postgresql.org/download/) (for mac we'd recommend installing http://postgresapp.com/)\n2. Create a new database in postgres (you can call it `encorelink`)\n3. Copy the [server/datasources.local.example.json](server/datasources.local.example.json) to `server/datasources.local.json` and update the config for `postgres` in that file to match the info for that postgres database (i.e. setting the `host`, `port`, `database`, `username`, and `password`)\n\n## Usage\n\n### Linting\n\nThis project uses [eslint](http://eslint.org/) for checking coding practices and standards.\nIt is expected that any opened pull requests have a passing eslint run.\n\nIt is _highly recommended_ that you configure your editor to run eslint on the fly\nwhile you code.\n\nAdditionally, you can run eslint on the command line with `npm run lint` (or\n`npm run lint -- --fix` to have eslint attempt to fix some of the issues it finds)\n\n### Testing\n\nThis project uses Facebook's [jest](https://facebook.github.io/jest/) library for testing,\nand takes advantage of their [snapshot testing](https://facebook.github.io/jest/docs/tutorial-react.html#snapshot-testing).\n\nFor development run:\n\n```bash\nnpm test -- --watch\n```\n\nThis will run the tests in an interactive mode, where tests will automatically be\nre-run when files are changed, and snapshots can be updated on the fly.\n\n### Things work locally but not on Heroku...\nWe run the app with a different configuration for local development than we do\nfor deploying. If things work when running locally, but fail when deploying,\nrun `npm prune --production` to set your `node_modules` to match production and\nrun `npm run heroku` to emulate the config used for production (visible on\nlocalhost:3000).\n\nYou might also want to make sure you have the same npm modules that will be\ninstalled on heroku (this can be done with `rm -rf node_modules && npm install\n--production`). In this case, you'll have to start the app with\n`NODE_ENV=production npm start` (or otherwise export `NODE_ENV=production`\nbefore starting).\n\n### Client App State\nThis project uses Redux for managing state on the client. There is a neat\n[Redux DevTools browser extension](http://extension.remotedev.io/) that can help with\nunderstanding and debugging what is happening in the app as a user interacts with the client.\n\nThe [Chrome version of the plugin can be found here](https://chrome.google.com/webstore/detail/redux-devtools/lmhkpmbekcpmknklioeibfkpmmfibljd)\n\n### React Storybook\n\nWe have integrated [React Storybook](https://getstorybook.io) for development of React components\nin isolation independent of having to wire up data from the app.\n\nTo run react storybook, run `npm run storybook`\nThis will start storybook running at http://localhost:9001\n\nStories are defined for components in [client/stories/index.js](client/stories/index.js).\nFor docs on how to write stories see the [storybook docs](https://getstorybook.io/docs/react-storybook/basics/writing-stories).\n\n## Built With\n\n* [Node.js](https://nodejs.org/en/about/) - Executes JavaScript code server-side\n* [Loopback](https://loopback.io/) - Node.js API framework\n* [Netlify](https://www.netlify.com/docs/) - Frontend hosting; platform for automated deployment of static webpages\n* [Heroku](https://www.heroku.com/platform) - Backend hosting; Cloud application platform for web application deployment\n* [PostgreSQL](https://www.postgresql.org/about/) - An object-relational database\n* [React Storybook](https://github.com/storybooks/storybook) - Development environment for UI components\n* [Redux](https://redux.js.org/) - Manages state on the client\n"
 },
 {
  "repo": "terraform-google-modules/terraform-google-healthcare",
  "language": "HCL",
  "readme_contents": "# terraform-google-healthcare\n\nThis module handles opinionated Google Cloud Platform Healthcare datasets and stores.\n\n## Usage\n\nBasic usage of this module is as follows:\n\n```hcl\nmodule \"healthcare\" {\n  source  = \"terraform-google-modules/healthcare/google\"\n  version = \"~> 2.3\"\n\n  project  = \"<PROJECT_ID>\"\n  name     = \"example-dataset\"\n  location = \"us-central1\"\n  dicom_stores = [{\n    name = \"example-dicom-store\"\n    iam_members = [\n      { role = \"roles/healthcare.dicomEditor\", member = \"user:example@domain.com\" }\n    ]\n  }]\n  fhir_stores = [{\n    name         = \"example-fhir-store\"\n    version      = \"R4\"\n    notification_config = {\n      pubsub_topic = \"projects/<PROJECT_ID>/topics/example_topic\"\n    }\n  }]\n}\n```\n\nFunctional examples are included in the\n[examples](./examples/) directory.\n\n## Requirements\n\nThese sections describe requirements for using this module.\n\n### Software\n\nThe following dependencies must be available:\n\n- [Terraform][terraform] v0.13\n- [Terraform Provider for GCP][terraform-provider-gcp] plugin v4.39.0\n\n### Service Account\n\nA service account with the following roles must be used to provision\nthe resources of this module:\n\n- Healthcare Dataset Admin: `roles/healthcare.datasetAdmin`\n- Healthcare DICOM Admin: `roles/healthcare.dicomStoreAdmin`\n- Healthcare FHIR Admin: `roles/healthcare.fhirStoreAdmin`\n- Healthcare HL7 V2 Admin: `roles/healthcare.hl7V2StoreAdmin`\n- Healthcare Consent Admin: `roles/healthcare.ConsentStoreAdmin`\n\nThe [Project Factory module][project-factory-module] and the\n[IAM module][iam-module] may be used in combination to provision a\nservice account with the necessary roles applied.\n\nTo allow messages to be published from the Cloud Healthcare API to Pub/Sub,\nyou must add the `roles/pubsub.publisher` role to your project's [Cloud Healthcare\nService Agent service account](https://cloud.google.com/healthcare/docs/how-tos/controlling-access-other-products#the_cloud_healthcare_service_agent).\n\n### APIs\n\nA project with the following APIs enabled must be used to host the\nresources of this module:\n\n- Google Cloud Healthcare API: `healthcare.googleapis.com`\n\nTo allow messages to be published from the Cloud Healthcare API to Pub/Sub,\nthe following API also needs to be enabled:\n- Google Pub/Sub API: `pubsub.googleapis.com`\n\nThe [Project Factory module][project-factory-module] can be used to\nprovision a project with the necessary APIs enabled.\n\n<!-- BEGINNING OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n## Inputs\n\n| Name | Description | Type | Default | Required |\n|------|-------------|------|---------|:--------:|\n| consent\\_stores | Datastore that contain all information related to the configuration and operation of the Consent Management API (https://cloud.google.com/healthcare/docs/how-tos/consent-managing). | `any` | `[]` | no |\n| dicom\\_stores | Datastore that conforms to the DICOM (https://www.dicomstandard.org/about/) standard for Healthcare information exchange. | `any` | `[]` | no |\n| fhir\\_stores | Datastore that conforms to the FHIR standard for Healthcare information exchange. | `any` | `[]` | no |\n| hl7\\_v2\\_stores | Datastore that conforms to the HL7 V2 (https://www.hl7.org/hl7V2/STU3/) standard for Healthcare information exchange. | `any` | `[]` | no |\n| iam\\_members | Updates the IAM policy to grant a role to a new member. Other members for the role for the dataset are preserved. | <pre>list(object({<br>    role   = string<br>    member = string<br>  }))</pre> | `[]` | no |\n| location | The location for the Dataset. | `string` | n/a | yes |\n| name | The resource name for the Dataset. | `string` | n/a | yes |\n| project | The ID of the project in which the resource belongs. | `string` | n/a | yes |\n| time\\_zone | The default timezone used by this dataset. | `string` | `null` | no |\n\n## Outputs\n\nNo output.\n\n<!-- END OF PRE-COMMIT-TERRAFORM DOCS HOOK -->\n\n## Contributing\n\nRefer to the [contribution guidelines](./CONTRIBUTING.md) for\ninformation on contributing to this module.\n\n[project-factory-module]: https://registry.terraform.io/modules/terraform-google-modules/project-factory/google\n[terraform-provider-gcp]: https://www.terraform.io/docs/providers/google/index.html\n[terraform]: https://www.terraform.io/downloads.html\n"
 },
 {
  "repo": "opencasestudies/ocs-healthexpenditure",
  "language": "HTML",
  "readme_contents": "---\noutput: github_document\n---\n\n<!-- README.md is generated from README.Rmd. Please edit that file -->\n\n# OpenCaseStudies\n\n<!-- badges: start -->\n[![Travis-CI Build Status](https://travis-ci.com/muschellij2/ocs-healthexpenditure.svg?branch=master)](https://travis-ci.com/muschellij2/ocs-healthexpenditure)\n<!-- badges: end -->\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\",\n  fig.path = \"man/figures/README-\",\n  out.width = \"100%\"\n)\n```\n\n\n### License \n\nThis case study is part of the [OpenCaseStudies]() project. This work is licensed under the Creative Commons Attribution-NonCommercial 3.0 ([CC BY-NC 3.0](https://creativecommons.org/licenses/by-nc/3.0/us/)) United States License.\n\n### Citation \n\nTo cite this case study:\n\nKuo, Pei-Lun and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2019, February 14). opencasestudies/ocs-healthexpenditure: Exploring Health Expenditure using State-level data in the United States (Version v1.0.0). Zenodo. http://doi.org/10.5281/zenodo.2565307\n\n[![DOI](https://zenodo.org/badge/151142096.svg)](https://zenodo.org/badge/latestdoi/151142096)\n\n### Title \n\nExploring health expenditure using state-level data in the United States\n\nHealth policy in the United States is complicated, and several \nforms of healthcare coverage exist, including both coverage by federal \ngoverment-led healthcare policy, and by private insurance companies.\nBefore making any inference about the relationship between \nhealth condition and health policy, it is important for us to \nhave a general idea about healthcare economics in the United \nStates. Thus, We are interested in getting sense of healthcare \ncoverage and healthcare spending across the United States.  \n\n### Motivating questions\n\n1. Is there a relationship between healthcare coverage and\nhealthcare spending in the United States?   \n2. How does the spending distribution change across geographic \nregions in the Unied States?  \n3. Does the relationship between healthcare coverage and \nhealthcare spending in the United States change from 2013 to 2014?  \n\n### Data\n\nThe data for this demonstration come from\n[Henry J Kaiser Family Foundation (KFF)](https://www.kff.org). \n\n- [Health Insurance Coverage of the Total Population](https://www.kff.org/other/state-indicator/total-population/) (Includes years 2013-2016)\n  \n- [Health Care Expenditures by State of Residence (in millions)](https://www.kff.org/other/state-indicator/health-care-expenditures-by-state-of-residence-in-millions/) (Includes years 1991-2014)\n \nFor educational purposes, the data have been downloaded \nand relative paths are used for this demonstration.\n**Note**: If students are not familiar with relative paths,\nit will be helpful to briefly introduce the idea for absolute\npaths and relative paths.\n\nWe also introduce `library(datasets)` for States information.\n\n### Learning Objetives\n\nThe skills, methods, and concepts that students will be familiar with by the end of this case study are:\n\n<u>**Data Science Learning Objectives:**</u>\n\n1. Load data from a package (`datasets`)  \n2. Import data from a csv (`readr`)  \n3. View, filter, join, and summarize data (`dplyr`)   \n4. Reshape data into different formats (`tidyr`)  \n5. Create data visualizations (`ggplot2`) with labels (`ggrepel`) and facets for different groups    \n\n#### Data Import \n\nWe use the R package `library(readr)` for data import in this tutorial.  \n\n#### Data wrangling \n\nTwo R package `library(tidyr)`, `library(dplyr)` are used for data wrangling in this tutorial.  \n\nWe explain what tidy data is, and further introduce the concepts of \"wide format\" \nand \"long format.\" We also demonstrate how to convert from one format to the other using \n`gather()` and `spread()`.\n\nWe also demonstrate some other useful functions for data wrangling, including \nselecting columns using `select()`, \nSelecting rows using `filter()`, \narranging or re-orderomg rows using `arrange()`, \njoining two datasets using `join()`, \nadding columns using `mutate()`, \ncreating summaries of columns using `summarize()`, \nand grouping operations using `group_by()`. \n\n#### Data exploration (exploratory analysis)   \n\nFor exploratory analysis, we use data visulization for exploratory analysis. `ggplot2` is the R package \nwe demonstrate in this tutorial. \n\nWe explain how to create plots using `ggplot()` with basic syntax for `ggplot2`. \nWe also demonstrate how to create scatter plots using `geom_point()`,\nhow to add layers of text using `geom_text()`, \nhow to facet across a variable using `facet_wrap()`, \nhow to create boxplots using `geom_boxplot()`, \nand how to facet by two variables using `facet_grid`. \n\n#### Summary   \n\nThe total healthcare expenditure is associated with \nthe population. To make a fair comparison, \nwe create \"healthcare expenditure per capita.\" \nFurther, the exploratory analysis via data visualization showed \nhigher speding in healthcare per capita \nis positively associated with higher \nemployer coverage proportion and is \nnegatively associated with the porportion \nof uninsured population across the States. \n\n### Other notes and resources \n\n<u>**Packages used in this case study:** </u>\n\n Package   | Use in this case study                                                                       \n---------- |-------------\n[datasets](https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html){target=\"_blank\"} | to get the state data\n[tibble](https://tibble.tidyverse.org/){target=\"_blank\"} | to create tibbles (the tidyverse version of a data frame)\n[readr](https://readr.tidyverse.org/){target=\"_blank\"} |  to read in the data from the csv files\n[tidyr](https://tidyr.tidyverse.org/){target=\"_blank\"}      | to change the shape or format of tibbles to wide and long  \n[dplyr](https://dplyr.tidyverse.org/){target=\"_blank\"}      | to subset and filter the data for specific groups, to summarize the data\n[ggplot2](https://ggplot2.tidyverse.org/){target=\"_blank\"}      | to create plots  \n[ggrepel](https://cran.r-project.org/web/packages/ggrepel/ggrepel.pdf){target=\"_blank\"} | to add labels that do not overlap to plots\n\nIn order to run this code please ensure you have these packages installed.\n\n### For instructors:  \n\nThe objective of this tutorial is for student to get familiar with important skills in data science, including data import (`readr`), data wrangling (`dplyr`), and data visualization (`ggplot2`). This material is designed for 4.5 teaching hours. (One potential way to teach this tutorial is to divide the material into three 1.5 hour sessions. The first session focuses on data import, the second session focuses on data wrangling, and the third portion focuses on visualization.) The session starting with (*) can be made as exercise for students' practice.\n\n \n"
 },
 {
  "repo": "Dana-Farber/automl-in-healthcare-review",
  "language": null,
  "readme_contents": "# AutoML in Healthcare Review\nAutomated machine learning: Review of the state-of-the-art and opportunities for healthcare\n\nSelected highlights from the 2020 AutoML Review [https://doi.org/10.1016/j.artmed.2020.101822] that reviewed over **2,160 works related to the field of automated machine learning**. \n\n## The curated list of automated feature engineering tools for Automated Machine Learning\nFull details in https://www.sciencedirect.com/science/article/pii/S0933365719310437?via%3Dihub#tbl0005\n| Method                  | Work                                                                                                                                                                                                                | Feature Engineering Technique | Used by how many works |\n|-------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------|---------------------------------------|\n| Deep Feature Synthesis  | [LINK](https://dai.lids.mit.edu/wp-content/uploads/2017/10/DSAA_DSM_2015.pdf)                                                                                                                                      | Expand-Reduce                 | 151                                   |\n| Explore Kit             | [LINK](http://people.eecs.berkeley.edu/~dawnsong/papers/icdm-2016.pdf)                                                                                                                                               | Expand-Reduce                 | 53                                    |\n| One Button Machine      | [LINK](https://arxiv.org/pdf/1706.00327.pdf)                                                                                                                                                                         | Expand-Reduce                 | 32                                    |\n| AutoLearn               | [LINK](http://web2py.iiit.ac.in/research_centres/publications/download/inproceedings.pdf.88535e0ea3a74e72.4943444d2d20323031372e706466.pdf)                                                                          | Expand-Reduce                 | 16                                    |\n| GP Feature Construction | [LINK](https://homepages.ecs.vuw.ac.nz/~xuebing/Papers/GPFCFSmemeticComputing.pdf)                                                                                                                                   | Genetic Programming           | 68                                    |\n| Cognito                 | [LINK](https://ieeexplore.ieee.org/abstract/document/7836821)                                                                                                                                                        | Hierarchical Greedy Search    | 38                                    |\n| RLFE                    | [LINK](https://arxiv.org/pdf/1709.07150.pdf)                                                                                                                                                                         | Reinforcement Learning        | 21                                    |\n| LFE                     | [LINK](https://www.researchgate.net/profile/Udayan_Khurana/publication/318829821_Learning_Feature_Engineering_for_Classification/links/5a13e08a0f7e9b1e5730a735/Learning-Feature-Engineering-for-Classification.pdf) | Meta-Learning                 | 34                                    |\n\n\n## Automated machine learning pipeline optimizers\nFull details in https://www.sciencedirect.com/science/article/pii/S0933365719310437?via%3Dihub#tbl0010\n| Method                    | Work                                                                                                           | Optimization Algorithm                               | Data Pre-Processing | Feature Engineering | Model Selection    | Hyperparameter Optimization | Ensemble Learning  | Meta-Learning      | Used by how many works |\n|---------------------------|-----------------------------------------------------------------------------------------------------------------|------------------------------------------------------|---------------------|---------------------|--------------------|-----------------------------|--------------------|--------------------|---------------------------------------|\n| Auto-Weka                 | [LINK](https://arxiv.org/pdf/1208.3719.pdf)                                                                     | Bayesian Optimization (SMAC)                         | :heavy_check_mark:  |                     | :heavy_check_mark: | :heavy_check_mark:          |                    |                    | 703                                   |\n| Auto-Sklearn              | [LINK](http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)                    | Joint Bayesian Optimization and Bandit Search (BOHB) | :heavy_check_mark:  |                     | :heavy_check_mark: | :heavy_check_mark:          | :heavy_check_mark: | :heavy_check_mark: | 542                                   |\n| TPOT                      | [LINK](https://arxiv.org/pdf/1601.07925.pdf)                                                                    | Evolutionary Algorithm                               | :heavy_check_mark:  | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark:          |                    |                    | 84                                    |\n| TuPAQ                     | [LINK](https://thisisdhaas.com/papers/SOCC2015TuPAQ.pdf)                                                        | Bandit Search                                        |                     |                     | :heavy_check_mark: | :heavy_check_mark:          |                    |                    | 94                                    |\n| ATM                       | [LINK](http://www.thswear.com/files/SwearingenEtAl-ATM-BigData2017.pdf)                                         | Joint Bayesian Optimization and Bandit Search        |                     | :heavy_check_mark:  |                    | :heavy_check_mark:          |                    | :heavy_check_mark: | 29                                    |\n| Automatic Frankensteining | [LINK](https://www.ismll.uni-hildesheim.de/pub/pdfs/wistuba_et_al_SDM_2017.pdf)                                 | Bayesian Optimization                                |                     |                     | :heavy_check_mark: | :heavy_check_mark:          | :heavy_check_mark: |                    | 12                                    |\n| ML-Plan                   | [LINK](https://link.springer.com/article/10.1007/s10994-018-5735-z)                                             | Hierarchical Task Networks (HTN)                     | :heavy_check_mark:  |                     | :heavy_check_mark: | :heavy_check_mark:          |                    |                    | 24                                    |\n| Autostacker               | [LINK](https://arxiv.org/pdf/1803.00684.pdf)                                                                    | Evolutionary Algorithm                               |                     |                     | :heavy_check_mark: | :heavy_check_mark:          | :heavy_check_mark: |                    | 18                                    |\n| AlphaD3M                  | [LINK](https://www.cs.columbia.edu/~idrori/AlphaD3M.pdf)                                                        | Reinforcement Learning/Monte Carlo Tree Search       | :heavy_check_mark:  |                     | :heavy_check_mark: | :heavy_check_mark:          |                    |                    | 8                                     |\n| Collaborative Filtering   | [LINK](https://papers.nips.cc/paper/7595-probabilistic-matrix-factorization-for-automated-machine-learning.pdf) | Probabilistic Matrix Factorization                   | :heavy_check_mark:  |                     | :heavy_check_mark: | :heavy_check_mark:          |                    | :heavy_check_mark: | 29                                    |\n\n## Neural Architecture Search algorithms, based on performance on the CIFAR-10 dataset\nFull details in https://www.sciencedirect.com/science/article/pii/S0933365719310437?via%3Dihub#tbl0015\n| NAS Algorithm           | Work                                                                                                                                             | Search Space            | Search Strategy                             | Performance Estimation Strategy              | Number of Parameters | Search Time (GPU-days) | Test Error (%) |\n|-------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------|---------------------------------------------|----------------------------------------------|----------------------|------------------------|----------------|\n| Large-scale Evolution   | [LINK](https://arxiv.org/pdf/1703.01041.pdf)                                                                                                      | Feed-Forward Networks   | Evolutionary Algorithm                      | Naive Training and Validation                | 5.4M                 | 2600                   | 5.4            |\n| EAS                     | [LINK](https://arxiv.org/pdf/1707.04873.pdf)                                                                                                      | Feed-Forward Networks   | Reinforcement Learning and Network Morphism | Short Training and Validation                | 23.4M                | 10                     | 4.23           |\n| Hierarchical Evolution  | [LINK](https://arxiv.org/pdf/1711.00436.pdf)                                                                                                      | Cell Motifs             | Evolutionary Algorithm                      | Training and Validation on proposed CNN Cell | 15.7M                | 300                    | 3.75           |\n| NAS v3                  | [LINK](https://arxiv.org/pdf/1611.01578.pdf)                                                                                                      | Multi-branched Networks | Reinforcement Learning                      | Naive Training and Validation                | 37.4M                | 22400                  | 3.65           |\n| PNAS                    | [LINK](https://openaccess.thecvf.com/content_ECCV_2018/papers/Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper.pdf)                     | Cell Motifs             | Sequential Model-Based Optimization (SMBO)  | Performance Prediction                       | 3.2M                 | 225                    | 3.41           |\n| ENAS                    | [LINK](https://arxiv.org/pdf/1802.03268.pdf)                                                                                                      | Cell Motifs             | Reinforcement Learning                      | One Shot                                     | 4.6M                 | 0.45                   | 2.89           |\n| ResNet + Regularization | [LINK](https://arxiv.org/pdf/1705.07485.pdf)                                                                                                      | HUMAN BASELINE          | HUMAN BASELINE                              | HUMAN BASELINE                               | 26.2M                | -                      | 2.86           |\n| DARTS                   | [LINK](https://arxiv.org/pdf/1806.09055.pdf)                                                                                                      | Cell Motifs             | Gradient-Based Optimization                 | Training and Validation on proposed CNN Cell | 3.4M                 | 4                      | 2.83           |\n| NASNet-A                | [LINK](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zoph_Learning_Transferable_Architectures_CVPR_2018_paper.pdf)                       | Cell Motifs             | Reinforcement Learning                      | Naive Training and Validation                | 3.3M                 | 2000                   | 2.65           |\n| EENA                    | [LINK](https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_EENA_Efficient_Evolution_of_Neural_Architecture_ICCVW_2019_paper.pdf) | Cell Motifs             | Evolutionary Algorithm                      | Performance Prediction                       | 8.5M                 | 0.65                   | 2.56           |\n| Path-Level EAS          | [LINK](https://arxiv.org/pdf/1806.02639.pdf)                                                                                                      | Cell Motifs             | Reinforcement Learning                      | Short Training and Validation                | 14.3M                | 200                    | 2.30           |\n| NAO                     | [LINK](http://papers.nips.cc/paper/8007-neural-architecture-optimization.pdf)                                                                     | Cell Motifs             | Gradient-Based Optimization                 | Performance Prediction                       | 128M                 | 200                    | 2.11           |\n"
 },
 {
  "repo": "pik1989/EDAforHealthcare",
  "language": "Jupyter Notebook",
  "readme_contents": "# EDA for Healthcare in under 30 minutes :)\n\n\ud83d\udd34 Follow this video for the code walkthrough: (Link to be uploaded)\n\n\ud83d\udd34 Download the code from here: https://github.com/pik1989/EDAforHealthcare/**\n"
 },
 {
  "repo": "johnuberbacher/flutter_medical",
  "language": "Dart",
  "readme_contents": "# flutter_medical\n\nFunctioning Doctor/Healthcare Catalog & Scheduling App created using Dart with Flutter. \nStores and loads data from noSQL Firebase. \n\n![](https://img.shields.io/badge/Dart-0175C2?style=for-the-badge&logo=dart&logoColor=white) ![](https://img.shields.io/badge/Flutter-02569B?style=for-the-badge&logo=flutter&logoColor=white) ![](https://img.shields.io/badge/firebase-%23039BE5.svg?style=for-the-badge&logo=firebase)\n\n\n![Screenshot](https://i.imgur.com/ebfJCdt.jpg)\n\n### Installation\n```\ngit clone https://github.com/johnuberbacher/flutter_medical.git\n\nflutter pub get\n\nflutter run\n```\n\n### TO-DO\n\n- [x] Create Profile page\n- [ ] MyHealth edit profile details\n- [ ] Fix Top Doctors \"view more\"\n- [ ] Firebase Storage for user profile images and office gallery photos\n\n### Meta\n\nJohn Uberbacher \u2013 [johnuberbacher.com](https://johnuberbacher.com)\n"
 },
 {
  "repo": "google/healthcare-text-annotation",
  "language": null,
  "readme_contents": "# Healthcare Text Annotation Guidelines\n\nMedical annotations capture a structured representation of knowledge stored in unstructured text. The task of mapping text to structured knowledge is done with the end goal of feeding the annotations into a machine learning algorithm that learns how to automatically extract the medical knowledge contained in the text. The guidelines defined below establish an annotation standard to be followed by human annotators. \n\n\nThe Guidelines cover the annotation methodology, including recommendations for training and organizing the annotations team, as well as specific guidelines and examples for annotating medical entities, modifiers, entity temporal and certainty assessments, and entity relations. \n\n\nBy sharing this document with the broader community we encourage researchers to follow the standardized approach for data annotation, hence producing high-quality medical text annotations. \n\n\n\n[1. Annotation Methodology](methodology/annotation-methodology.md)\n\n\n[2. Annotation Guidelines (introduction)](guidelines/annotation-guidelines.md)\n\n[- Defining Entity Categories](guidelines/defining-entity-categories.md)\n\n[- Defining Entity Temporal Assessment](guidelines/defining-entity-temporal-assessment.md)\n\n[- Defining Entity Certainty Assessment](guidelines/defining-entity-certainty-assessment.md)\n\n[- Defining Entity Subject](guidelines/defining-entity-subject.md)\n\n[- Rules for Annotating Temporal Assessment, Certainty Assessment, Subject](guidelines/annotating-rules.md)\n\n[- Defining Entity Relations](guidelines/defining-entity-relations.md)\n\n"
 },
 {
  "repo": "XindiWu/Awesome-Machine-Learning-in-Biomedical-Healthcare-Imaging",
  "language": null,
  "readme_contents": "# Awesome Machine Learning in Biomedical(Healthcare) Imaging\n> \ud83c\udf08A list of awesome selected resources towards the application of machine learning in Biomedical/Healthcare Imaging, inspired by [awesome-php](https://github.com/ziadoz/awesome-php).\n\nIf you also want to contribute to this list, feel free to send me a pull request or contact me\ud83d\ude4c[@XindiWu](mailto:wuxindi0709@gmail.com).\n\n\n## Table of Contents\n\n\n## Courses\n\n### Bioimage Analysis\n\nHarvard: [Bio-Image Analysis Course](https://iccb.med.harvard.edu/bio-image-analysis-course)\n\nStanford: [Introduction to Bioimaging](https://web.stanford.edu/class/ee169/EE_169.html)\n\nCMU: [Bioimage Informatics](https://www.andrew.cmu.edu/course/42-731/index.html)\n\nCaltech: [Data Analysis in the Biological Sciences](http://bebi103.caltech.edu.s3-website-us-east-1.amazonaws.com/2018/)\n\n### Biomedical Image Analysis\n\nUniversity of Nebraska: [Introduction to Biomedical Imaging and Image Analysis](https://www.unmc.edu/radiology/education/biomedical-imaging.html)\n\nStanford: [Computational Methods for Biomedical Image Analysis and Interpretation](https://canvas.stanford.edu/courses/98045)\n\nDartmouth College: [Medical Image Visualization and Analysis](https://engineering.dartmouth.edu/academics/courses/engg199-03)\n\nUCLA: [Signal and Image Processing for Biomedicine](https://sa.ucla.edu/ro/Public/SOC/Results/ClassDetailterm_cd=16W&subj_area_cd=PBMED%20%20&crs_catlg_no=0209%20%20%20%20&class_id=825054200&class_no=%20001%20%20)\n\nMIT: [Biomedical Signal and Image Processing](https://ocw.mit.edu/courses/health-sciences-and-technology/hst-582j-biomedical-signal-and-image-processing-spring-2007/index.htm)\n\nNYU: [Biomedical Imaging II](http://bulletin.engineering.nyu.edu/preview_course_nopop.php?catoid=11&coid=27762)\n\nUCSD: [Introduction to biomedical imaging and sensing](http://circuit.ucsd.edu/~zhaowei/ECE187/info.php)\n\nILLINOIS: [Biomedical imaging](https://ece.illinois.edu/academics/courses/profile/ECE380)\n\n### Medical image analysis\n\nJohns Hopkins: [Applied Medical Image Processing](https://ep.jhu.edu/programs-and-courses/585.703-applied-medical-image-processing)\n\nUCB: [Medical Imaging Signals and Systems](https://www2.eecs.berkeley.edu/Courses/EEC261/)\n\nBrown University: [Medical Image Analysis](http://vision.lems.brown.edu/content/engn-2500-medical-image-analysis)\n\nMIT: [Principles of Medical Imaging](https://ocw.mit.edu/courses/nuclear-engineering/22-058-principles-of-medical-imaging-fall-2002/)\n\nPurdue: [MEDICAL IMAGING DIAGNOSTIC TECHNOLOGIES](https://engineering.purdue.edu/ProEd/courses/medical-imaging-diagnostic-technologies)\n\nUniversity of Michigan Ann Arbor: [MEDICAL IMAGING SYSTEMS](https://bme.umich.edu/course/biomede-516/)\n\nUniversity of Florida: [Introduction to Biomedical Image Analysis and Imaging Informatics](https://www.bme.ufl.edu/labs/yang/pdf/Syllabus_Yang_BME6938-Revise.pdf)\n\nCMU: [Methods In (Bio)Medical Image Analysis](https://www.cs.cmu.edu/~galeotti/methods_course/)\n\n\n\n\n\n\n## Conferences\n\n2019 MICCAI: [22nd International Conference on Medical Image Computing and Computer Assisted Intervention](https://www.miccai2019.org/)\n\n2019 AIME: [Artificial Intelligence in Medicine](http://aime19.aimedicine.info/)\n\n2019 AMIA: [AMIA Symposium](https://www.amia.org/amia2019)\n\n2020 RECOMB: [International Conference on Research in Computational Molecular Biology](https://www.recomb2020.org/)\n\n2020 PSB: [Pacific Symposium on Biocomputing](https://psb.stanford.edu/)\n\n2020 ICHI: [IEEE International Conference on Healthcare Informatics](https://hs-heilbronn.de/ichi2020) \n\n2020 CMIMI: [Conference on Machine Intelligence in Medical Imaging](https://siim.org/page/2019cmimi) \n\n2019 ISBI: [The IEEE International Symposium on Biomedical Imaging](https://biomedicalimaging.org/2019/)\n\n## Journal Collections\n\nBMC: [Artificial intelligence in biomedical imaging](https://www.biomedcentral.com/collections/ai)\n\nJAMA: [Deep Learning and Artificial Intelligence in Health Care](https://sites.jamanetwork.com/machine-learning/)\n\nPLOS: [Machine Learning in Health and Biomedicine](https://collections.plos.org/mlforhealth)\n\nNature Medicine: [Digital Medicine](https://www.nature.com/collections/egjifhdcih)\n\nIEEE: [Journal of Biomedical and Health Informatics](https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=6221020)\n\n\n\n## Datasets\n\nExternal Link Collections: https://github.com/beamandrew/medical-data\n\nKaggle: [Medical Image](https://www.kaggle.com/datasets?search=Medical+image)\n\nKaggle: [Biology Image](https://www.kaggle.com/datasets?search=biology+image)\n\n[The Cancer Imaging Archive (TCIA)](https://www.cancerimagingarchive.net/)\n\n### Radiology (Ultrasound, Mammographs, X-Ray, CT, MRI, fMRI, etc.)\n\n[Center for Invivo Microscopy (CIVM), Embrionic and Neonatal Mouse (H&E, MR)](http://www.civm.duhs.duke.edu/devatlas/)\n[user guide](http://www.civm.duhs.duke.edu/devatlas/UserGuide.pdf)\n\n[Collaborative Informatics and Neuroimaging Suite (COINS)](https://portal.mrn.org/micis/index.php?subsite=dx)\n\n[Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI)](http://adni.loni.ucla.edu/)\n\n[The Open Access Series of Imaging Studies (OASIS)](http://www.oasis-brains.org/)\n\n[Breast Cancer Digital Repository](https://bcdr.eu/)\n\n[DDSM: Digital Database for Screening Mammography](http://marathon.csee.usf.edu/Mammography/Database.html)\n\n[The Mammographic Image Analysis Society (MIAS) mini-database](http://peipa.essex.ac.uk/info/mias.html)\n\n[Mammography Image Databases 100 or more images of mammograms with ground truth](http://marathon.csee.usf.edu/Mammography/Database.html)\n\n[NLM HyperDoc Visible Human Project color, CAT and MRI image samples - over 30 images](http://www.nlm.nih.gov/research/visible/visible_human.html)\n\n[CT Scans for Colon Cancer](https://wiki.cancerimagingarchive.net/display/Public/CT+COLONOGRAPHY#e88604ec5c654f60a897fa77906f88a6)\n\n\n\n\n### Microscopy (Cell, Cytology, Biology, Protein, Molecular, Fluorescence, etc.)\n\n[BDGP images from the FlyExpress database](www.flyexpress.net)\n\n[The UCSB Bio-Segmentation Benchmark dataset](http://www.bioimage.ucsb.edu/research/biosegmentation)\n\n[Pap Smear database](http://labs.fme.aegean.gr/decision/downloads)\n\n[BIICBU Biological Image Repository](http://ome.grc.nia.nih.gov/iicbu2008/)\n\n[RNAi dataset](http://ome.grc.nia.nih.gov/iicbu2008/rnai/index.html)\n\n[Chinese Hamster Ovary cells (CHO) dataset](http://ome.grc.nia.nih.gov/iicbu2008/hela/index.html#cho)\n\n[Locate Endogenus mouse sub-cellular organelles (END) databaset](http://locate.imb.uq.edu.au/)\n\n[2D HeLa dataset (HeLa) datgaset](http://ome.grc.nia.nih.gov/iicbu2008/hela/index.html)\n\n[Allen Brain Atlas](http://www.brain-map.org/)\n\n[1000 Functional Connectomes Project](http://fcon_1000.projects.nitrc.org/)\n\n[The Cell Centered Database (CCDB)]( http://ccdb.ucsd.edu/index.shtm)\n\n[The Encyclopedia of DNA Elements (ENCODE)](http://genome.ucsc.edu/ENCODE/http://www.plosbiology.org/article/info:doi/10.1371/journal.pbio.1001046)\n\n[The Human Protein Atlas](http://www.proteinatlas.org/)\n\n[DRIVE: Digital Retinal Images for Vessel Extraction](http://www.isi.uu.nl/Research/Databases/DRIVE) / [Ground truth](http://www.cs.rug.nl/~imaging/databases/retina_database/retinalfeatures_database.html)\n\n[El Salvador Atlas of Gastrointestinal VideoEndoscopy Images and Videos of his-res of studies taken from Gastrointestinal](http://www.gastrointestinalatlas.com)\n\n### Histology and Histopathology\n\n[The Histology Image Dataset (histologyDS)](http://www.informed.unal.edu.co/histologyDS)\n\n[The Cancer Genome Atlas (TCGA)](http://cancergenome.nih.gov)\n\n[International Cancer Genome Consortium](http://icgc.org) / [Data portal](http://dcc.icgc.org/)\n\n[Stanford Tissue Microarray Database (TMA)](http://tma.im)\n\n[MITOS dataset](http://ipal.cnrs.fr/ICPR2012/)\n\n[DPA\u2019s Whole Slide Imaging Repository](https://digitalpathologyassociation.org/whole-slide-imaging-repository)\n\n[Atlas of bleast Histology](http://www.webmicroscope.net/atlases/breast/brcatlas_start.asp)\n\n[ITK Analysis of Large Histology Datasets](http://www.na-mic.org/Wiki/index.php/ITK_Analysis_of_Large_Histology_Datasets)\n\n[Histology Photo Album](http://www.histology-world.com/photoalbum/thumbnails.php?album=52)\n\n[Slide Library of Virtual pathology, University of Leeds](http://www.virtualpathology.leeds.ac.uk/)\n\n[Tissue Acquisition and Banking Services (TABS) of the NYU Experimental Pathology Core Facilities](http://pathology.med.nyu.edu/research/core-laboratories/tissue-banking)\n\n[Aperio Images](http://images2.aperio.com/)\n\n[HAPS Histology Image Database](http://hapshistology.wikifoundry.com/)\n\n[NIH: chest x-ray datasets](https://nihcc.app.box.com/v/ChestXray-NIHCC)\n\n\n\n## Tasks:  (Link: https://paperswithcode.com/area/medical)\n\nMedical Image Segmentation\n\nEEG\n\nElectrocardiography (ECG)\n\nDrug Discovery\n\nCancer\n\nSleep Quality\n\nMedical Image Registration\n\nDisease Prediction\n\nMortality Prediction\n\nMedical Image Generation\n\nProtein Secondary Structure Prediction\n\nMedical Diagnosis\n\nLength-of-stay prediction\n\nSeizure Detection\n\nSkin\n\nHistopathological Image Classification\n\nMitosis Detection\n\nComputational Phenotyping\n\nEpidemiology\n\nLung Disease Classification\n\nDiabetic Retinopathy Detection\n\nX-Ray\n\nMedical Relation Extraction\n\nMetal Artifact Reduction\n\nPhotoplethysmography (PPG)\n\nLung Nodule Classification\n\nPneumonia Detection\n\nSurgical Skills Evaluation\n\nReadmission Prediction\n\nAutomatic Sleep Stage Classification\n\nEeg Decoding\n\nSkull Stripping\n\nParticipant Intervention Comparison Outcome Extraction\n\nPatient Outcomes\n\nMedical Report Generation\n\nKnee Osteoarthritis Prediction\n\nMulti-Label Classification Of Biomedical Texts\n\nbreast density classification\n\nMedical Super-Resolution\n\nMolecule Interpretation\n\nMammogram\n\nCancer Metastasis Detection\n\nPain Intensity Regression\n\nElectromyography (EMG)\n\nSurgical Gesture Recognition\n\nProtein Function Prediction\n\nepilepsy prediction\n\nSeizure prediction\n\nWhite Matter Fiber Tractography\n\nSingle-cell modeling\n\nAge-Related Macular Degeneration Classification\n\nSequential Diagnosis\n\nEcg Risk Stratification\n\nDiabetes Prediction\n\nMagnetic Resonance Fingerprinting\n\nTomography\n\nAtrial Fibrillation\n\nMalaria Risk Exposure Prediction\n\nMuscular Movement Recognition\n\nMedical Code Prediction\n\nMotion Correction In Multishot Mri\n\nChemical Reaction Prediction\n\nUltrasound\n\n## Competitions:\n\n[Grand Challenges in Biomedical Image Analysis](https://grand-challenge.org/)\n\n[The Cancer Imaging Archive (TCIA) Public Access](https://wiki.cancerimagingarchive.net/display/Public/Challenge+competitions)\n\n[Competitions in Kaggle](https://www.kaggle.com/competitions?sortBy=relevance&group=general&search=medical&page=1&pageSize=20&turbolinks%5BrestorationIdentifier%5D=34d9733a-6ecc-4581-a23d-cc00703b91c8)  \n\n[The MICCAI 2014 Machine Learning Challenge](https://competitions.codalab.org/competitions/1471)\n\n[The ISBI 2019 Challenges](https://biomedicalimaging.org/2019/challenges/)\n\n[Medical Segmentation Decathlon](http://medicaldecathlon.com/)\n\n\n\n\n"
 },
 {
  "repo": "MIT-LCP/eicu-code",
  "language": "Jupyter Notebook",
  "readme_contents": "# eICU Collaborative Research Database Code Repository [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1249016.svg)](https://doi.org/10.5281/zenodo.1249016)\n\nThis is a repository of code shared by the research community. The repository is intended to be a central hub for sharing, refining, and reusing code used for analysis of the [eICU Collaborative Research Database](http://eicu-crd.mit.edu). To find out more about database, please see: http://eicu-crd.mit.edu\n\n## How to contribute\n\nOur team has worked hard to create and share the eICU Collaborative Research Database. We encourage you to share the code that you use for data processing and analysis. Sharing code helps to make studies reproducible and promotes collaborative research. To contribute, please:\n\n- Fork the repository using the following link: https://github.com/mit-lcp/eicu-code/fork. For a background on GitHub forks, see: https://help.github.com/articles/fork-a-repo/\n- Commit your changes to the forked repository.\n- Submit a pull request to the [eICU code repository](https://github.com/mit-lcp/eicu-code), using the method described at: https://help.github.com/articles/using-pull-requests/\n\nWe encourage users to share concepts they have extracted by writing code which generates a materialized view. These materialized views can then be used by researchers around the world to speed up data extraction.\n\n## License\n\nBy committing your code to the [eICU Code Repository](https://github.com/mit-lcp/eicu-code) you agree to release the code under the [MIT License attached to the repository](https://github.com/mit-lcp/eicu-code/blob/master/LICENSE).\n\n## Coding style\n\nPlease refer to the [style guide](https://github.com/mit-lcp/eicu-code/blob/master/styleguide.md) for guidelines on formatting your code for the repository.\n"
 },
 {
  "repo": "shantanu-deshmukh/vhealth-gatsby",
  "language": "TypeScript",
  "readme_contents": "<h1 align=\"center\">\n  vHealth\n</h1>\n\nOpen Source web template for a Healthcare Startup. It's built using gatsby but can be easily ported to create-react-app.\n\n## Preview\n\n![Demo vHealth](demo.gif)\n\n[See Live Preview >> ](https://vhealth.openthemes.dev)\n\n## \ud83d\ude80 Get Up and Running in 5 Minutes\n\n1. **Install the Gatsby CLI.**\n\n   ```bash\n   npm install -g gatsby-cli\n   ```\n\n2. **Download and install dependencies.**\n\n   Clone this repo and install required dependencies:\n\n   ```bash\n   git clone https://github.com/shantanu-deshmukh/vhealth-gatsby.git\n   cd vhealth-gatsby\n   yarn install\n   #or `npm install` if you prefer npm\n   ```\n\n3. **Start the site in `develop` mode.**\n\n   Start the site in develop mode and make changes as per your requirement\n\n   ```bash\n   gatsby develop\n   ```\n\n4. **Deploy**\n\n   Just build and deploy the `public` directory to a service that serves HTML pages.\n\n   ```bash\n   gatsby build\n   ```\n\n## \ud83e\udd1d Contributing\n\nPull requests are welcome. For major changes, please open an issue first to discuss what you would like to change.\n\n## \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb Author\n\n[Shantanu Deshmukh](https://shantanudeshmukh.com)\n\nFull-stack developer with experience in building complete Web & mobile apps from scratch.\n\n[Linkedin](https://www.linkedin.com/in/shantanud/)\n/ [Twitter](https://twitter.com/askshantanu) / [AngelList](https://angel.co/u/dshantanu)\n\n## \ud83d\udc9c Thanks\n\nSpecial thanks to [SLAB Design Studio](https://dribbble.com/slabdsgn) for letting me use their design for this template.\n"
 },
 {
  "repo": "TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials",
  "language": "Python",
  "readme_contents": "# NEW LIST 2023 - 2024: Machine-Learning / Deep-Learning / AI + Web3 -Tutorials\n\nHi - Thanks for dropping by!<br>\n<br>\nI will be updating this tutorials site on a <b>daily basis</b> adding all relevant topcis for 2022 - 2024 especially pertaining to **GPU programming, Data Centric AI, Emerging topics like Sustainable AI with Web3AI.js (DeFI, DAO, NFT) and much more**.<br>\n\n**NOTE: All these tutorials are supported and accelerated on NVIDIA GPUs**\n<br>\nMore importantly the applications of ML/DL/AI into industry areas such as Transportation, Medicine/Healthcare etc. will be something I'll watch with keen interest and would love to share the same with you.\n<br>\nFinally, it is **YOUR** help I will seek to make it more useful and less boring, so please do suggest/comment/contribute!\n<p align=\"center\">\n  <img src=\"https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/images/DK.png\">\n</p>\n\n## Index\n\n* [deep-learning](#deep-learning)\n   * [UBER | Pyro](#uber-pyro-probabalistic-tutorials)\n   * [Netflix | VectorFlow](#netflix-vectorflow-tutorials)\n   * [PyTorch](#pytorch-tutorials)\n   * [tensorflow](#tensor-flow-tutorials)\n   * [theano](#theano-tutorials)\n   * [keras](#keras-tutorials)\n   * [caffe](#deep-learning-misc)\n   * [Torch/Lua]()\n   * [MXNET]()\n   \n* [scikit-learn](#scikit-learn)\n* [statistical-inference-scipy](#statistical-inference-scipy)\n* [pandas](#pandas)\n* [matplotlib](#matplotlib)\n* [numpy](#numpy)\n* [python-data](#python-data)\n* [kaggle-and-business-analyses](#kaggle-and-business-analyses)\n* [spark](#spark)\n* [mapreduce-python](#mapreduce-python)\n* [amazon web services](#aws)\n* [command lines](#commands)\n* [misc](#misc)\n* [notebook-installation](#notebook-installation)\n* [Curated list of Deep Learning / AI blogs](#curated-list-of-deeplearning-blogs)\n* [credits](#credits)\n* [contributing](#contributing)\n* [contact-info](#contact-info)\n* [license](#license)\n\n## deep-learning\n\nIPython Notebook(s) and other programming tools such as Torch/Lua/D lang in demonstrating deep learning functionality.\n\n### uber-pyro-probabalistic-tutorials\n<p align=\"center\">\n  <img src=\"https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/images/pyro.png\">\n</p>\n\nAdditional PyRo tutorials:\n\n* [pyro-examples/full examples](http://pyro.ai/examples/)\n* [pyro-examples/Variational Autoencoders](http://pyro.ai/examples/vae.html)\n* [pyro-examples/Bayesian Regression](http://pyro.ai/examples/bayesian_regression.html)\n* [pyro-examples/Deep Markov Model](http://pyro.ai/examples/dmm.html)\n* [pyro-examples/AIR(Attend Infer Repeat)](http://pyro.ai/examples/air.html)\n* [pyro-examples/Semi-Supervised VE](http://pyro.ai/examples/ss-vae.html)\n* [pyro-examples/GMM](http://pyro.ai/examples/gmm.html)\n* [pyro-examples/Gaussian Process](http://pyro.ai/examples/gp.html)\n* [pyro-examples/Bayesian Optimization](http://pyro.ai/examples/bo.html)\n* [Full Pyro Code](https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/tree/master/deep-learning/UBER-pyro)\n\n\n\n### netflix-vectorflow-tutorials\n<p align=\"center\">\n  <img src=\"https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/images/VectorFlow.png\">\n</p>\n\n* [MNIST Example, running with Dlang](https://github.com/Netflix/vectorflow/tree/master/examples)\n\n### pytorch-tutorials\n<p align=\"center\">\n  <img src=\"https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/images/PyTorch.png\">\n</p>\n\n| Level | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [Beginners/Zakizhou](https://github.com/pytorch/tutorials/tree/master/beginner_source) | Learning the basics of PyTorch from Facebook. |\n| [Intermedia/Quanvuong](https://github.com/pytorch/tutorials/tree/master/intermediate_source) | Learning the intermediate stuff about PyTorch of from Facebook. |\n| [Advanced/Chsasank](https://github.com/pytorch/tutorials/tree/master/advanced_source) | Learning the advanced stuff about PyTorch of from Facebook. |\n| [Learning PyTorch by Examples - Numpy, Tensors and Autograd](https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/tree/master/pytorch) | At its core, PyTorch provides two main features an n-dimensional Tensor, similar to numpy but can run on GPUs AND automatic differentiation for building and training neural networks. |\n| [PyTorch - Getting to know autograd.Variable, Gradient, Neural Network](https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/pytorch/PyTorch%20NN%20Basics%20-%20Autograd%20Gradient%20Neural%20Network%20Loss%20Backprop.ipynb) | Here we start with ultimate basics of Tensors, wrap a Tensor with Variable module, play with nn.Module and implement forward and backward function. |\n\n\n### tensor-flow-tutorials\n<br/>\n<p align=\"center\">\n  <img src=\"https://avatars0.githubusercontent.com/u/15658638?v=3&s=100\">\n</p>\nAdditional TensorFlow tutorials:\n\n* [pkmital/tensorflow_tutorials](https://github.com/pkmital/tensorflow_tutorials)\n* [nlintz/TensorFlow-Tutorials](https://github.com/nlintz/TensorFlow-Tutorials)\n* [alrojo/tensorflow-tutorial](https://github.com/alrojo/tensorflow-tutorial)\n* [BinRoot/TensorFlow-Book](https://github.com/BinRoot/TensorFlow-Book)\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-basics](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/1_intro/basic_operations.ipynb) | Learn basic operations in TensorFlow, a library for various kinds of perceptual and language understanding tasks from Google. |\n| [tsf-linear](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/linear_regression.ipynb) | Implement linear regression in TensorFlow. |\n| [tsf-logistic](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/logistic_regression.ipynb) | Implement logistic regression in TensorFlow. |\n| [tsf-nn](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/2_basic_classifiers/nearest_neighbor.ipynb) | Implement nearest neighboars in TensorFlow. |\n| [tsf-alex](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/alexnet.ipynb) | Implement AlexNet in TensorFlow. |\n| [tsf-cnn](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/convolutional_network.ipynb) | Implement convolutional neural networks in TensorFlow. |\n| [tsf-mlp](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/multilayer_perceptron.ipynb) | Implement multilayer perceptrons in TensorFlow. |\n| [tsf-rnn](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/3_neural_networks/recurrent_network.ipynb) | Implement recurrent neural networks in TensorFlow. |\n| [tsf-gpu](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/4_multi_gpu/multigpu_basics.ipynb) | Learn about basic multi-GPU computation in TensorFlow. |\n| [tsf-gviz](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/graph_visualization.ipynb) | Learn about graph visualization in TensorFlow. |\n| [tsf-lviz](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-examples/notebooks/5_ui/loss_visualization.ipynb) | Learn about loss visualization in TensorFlow. |\n\n### tensor-flow-exercises\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [tsf-not-mnist](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/1_notmnist.ipynb) | Learn simple data curation by creating a pickle with formatted datasets for training, development and testing in TensorFlow. |\n| [tsf-fully-connected](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/2_fullyconnected.ipynb) | Progressively train deeper and more accurate models using logistic regression and neural networks in TensorFlow. |\n| [tsf-regularization](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/3_regularization.ipynb) | Explore regularization techniques by training fully connected networks to classify notMNIST characters in TensorFlow. |\n| [tsf-convolutions](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/4_convolutions.ipynb) | Create convolutional neural networks in TensorFlow. |\n| [tsf-word2vec](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/5_word2vec.ipynb) | Train a skip-gram model over Text8 data in TensorFlow. |\n| [tsf-lstm](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/tensor-flow-exercises/6_lstm.ipynb) | Train a LSTM character model over Text8 data in TensorFlow. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://www.deeplearning.net/software/theano/_static/theano_logo_allblue_200x46.png\">\n</p>\n\n### theano-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [theano-intro](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/theano-tutorial/intro_theano/intro_theano.ipynb) | Intro to Theano, which allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation. |\n| [theano-scan](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/theano-tutorial/scan_tutorial/scan_tutorial.ipynb) | Learn scans, a mechanism to perform loops in a Theano graph. |\n| [theano-logistic](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/theano-tutorial/intro_theano/logistic_regression.ipynb) | Implement logistic regression in Theano. |\n| [theano-rnn](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/theano-tutorial/rnn_tutorial/simple_rnn.ipynb) | Implement recurrent neural networks in Theano. |\n| [theano-mlp](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/theano-tutorial/theano_mlp/theano_mlp.ipynb) | Implement multilayer perceptrons in Theano. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"http://i.imgur.com/L45Q8c2.jpg\">\n</p>\n\n### keras-tutorials\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| keras | Keras is an open source neural network library written in Python. It is capable of running on top of either Tensorflow or Theano. |\n| [setup](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/README.md) | Learn about the tutorial goals and how to set up your Keras environment. |\n| [intro-deep-learning-ann](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.%20ANN/1.1%20Introduction%20-%20Deep%20Learning%20and%20ANN.ipynb) | Get an intro to deep learning with Keras and Artificial Neural Networks (ANN). |\n| [Perceptrons and Adaline](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.%20ANN/1.1.1%20Perceptron%20and%20Adaline.ipynb) | Implement Peceptron and adaptive linear neurons. |\n| [MLP and MNIST Data](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.%20ANN/1.1.2%20MLP%20and%20MNIST.ipynb) | Classifying handwritten digits,implement MLP, train and debug ANN |\n| [theano](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.2%20Introduction%20-%20Theano.ipynb) | Learn about Theano by working with weights matrices and gradients. |\n| [keras-otto](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.3%20Introduction%20-%20Keras.ipynb) | Learn about Keras by looking at the Kaggle Otto challenge. |\n| [ann-mnist](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/1.4%20(Extra)%20A%20Simple%20Implementation%20of%20ANN%20for%20MNIST.ipynb) | Review a simple implementation of ANN for MNIST using Keras. |\n| [conv-nets](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/2.1%20Supervised%20Learning%20-%20ConvNets.ipynb) | Learn about Convolutional Neural Networks (CNNs) with Keras. |\n| [conv-net-1](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/2.2.1%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20I.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 1. |\n| [conv-net-2](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/2.2.2%20Supervised%20Learning%20-%20ConvNet%20HandsOn%20Part%20II.ipynb) | Recognize handwritten digits from MNIST using Keras - Part 2. |\n| [keras-models](http://nbviewer.ipython.org/github/leriomaggio/deep-learning-keras-tensorflow/blob/master/2.3%20Supervised%20Learning%20-%20Famous%20Models%20with%20Keras.ipynb) | Use pre-trained models such as VGG16, VGG19, ResNet50, and Inception v3 with Keras. |\n| [auto-encoders](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/6.%20AutoEncoders%20and%20Embeddings/6.1.%20AutoEncoders%20and%20Embeddings.ipynb) | Learn about Autoencoders with Keras. |\n| [rnn-lstm](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/7.%20Recurrent%20Neural%20Networks/7.1%20RNN%20and%20LSTM.ipynb) | Learn about Recurrent Neural Networks (RNNs) with Keras. |\n| [lstm-sentence-gen](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/7.%20Recurrent%20Neural%20Networks/7.2%20LSTM%20for%20Sentence%20Generation.ipynb) |  Learn about RNNs using Long Short Term Memory (LSTM) networks with Keras. |\n| [nlp-deep-learning](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/6.%20AutoEncoders%20and%20Embeddings/6.2%20NLP%20and%20Deep%20Learning.ipynb) | Learn about NLP using ANN (Artificial Neural Networks. |\n| [hyperparamter-tuning](https://github.com/leriomaggio/deep-learning-keras-tensorflow/blob/master/5.%20HyperParameter%20Tuning%20and%20Transfer%20Learning/5.1%20HyperParameter%20Tuning.ipynb) | Hyperparamters tuning using keras-wrapper.scikit-learn |\n\n### deep-learning-misc\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [deep-dream](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/deep-learning/deep-dream/dream.ipynb) | Caffe-based computer vision program which uses a convolutional neural network to find and enhance patterns in images. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scikitlearn.png\">\n</p>\n\n## scikit-learn\n\nIPython Notebook(s) demonstrating scikit-learn functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [intro](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-intro.ipynb) | Intro notebook to scikit-learn.  Scikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [knn](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-intro.ipynb#K-Nearest-Neighbors-Classifier) | Implement k-nearest neighbors in scikit-learn. |\n| [linear-reg](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-linear-reg.ipynb) | Implement linear regression in scikit-learn. |\n| [svm](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-svm.ipynb) | Implement support vector machine classifiers with and without kernels in scikit-learn. |\n| [random-forest](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-random-forest.ipynb) | Implement random forest classifiers and regressors in scikit-learn. |\n| [k-means](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-k-means.ipynb) | Implement k-means clustering in scikit-learn. |\n| [pca](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-pca.ipynb) | Implement principal component analysis in scikit-learn. |\n| [gmm](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-gmm.ipynb) | Implement Gaussian mixture models in scikit-learn. |\n| [validation](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scikit-learn/scikit-learn-validation.ipynb) | Implement validation and model selection in scikit-learn. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/scipy.png\">\n</p>\n\n## statistical-inference-scipy\n\nIPython Notebook(s) demonstrating statistical inference with SciPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| scipy | SciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data. |\n| [effect-size](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scipy/effect_size.ipynb) | Explore statistics that quantify effect size by analyzing the difference in height between men and women.  Uses data from the Behavioral Risk Factor Surveillance System (BRFSS) to estimate the mean and standard deviation of height for adult women and men in the United States. |\n| [sampling](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scipy/sampling.ipynb) | Explore random sampling by analyzing the average weight of men and women in the United States using BRFSS data. |\n| [hypothesis](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/scipy/hypothesis.ipynb) | Explore hypothesis testing by analyzing the difference of first-born babies compared with others. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/pandas.png\">\n</p>\n\n## pandas\n\nIPython Notebook(s) demonstrating pandas functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [pandas](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/pandas.ipynb) | Software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series. |\n| [github-data-wrangling](https://github.com/donnemartin/viz/blob/master/githubstats/data_wrangling.ipynb) | Learn how to load, clean, merge, and feature engineer by analyzing GitHub data from the [`Viz`](https://github.com/donnemartin/viz) repo. |\n| [Introduction-to-Pandas](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.00-Introduction-to-Pandas.ipynb) | Introduction to Pandas. |\n| [Introducing-Pandas-Objects](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.01-Introducing-Pandas-Objects.ipynb) | Learn about Pandas objects. |\n| [Data Indexing and Selection](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.02-Data-Indexing-and-Selection.ipynb) | Learn about data indexing and selection in Pandas. |\n| [Operations-in-Pandas](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.03-Operations-in-Pandas.ipynb) | Learn about operating on data in Pandas. |\n| [Missing-Values](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.04-Missing-Values.ipynb) | Learn about handling missing data in Pandas. |\n| [Hierarchical-Indexing](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.05-Hierarchical-Indexing.ipynb) | Learn about hierarchical indexing in Pandas. |\n| [Concat-And-Append](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.06-Concat-And-Append.ipynb) | Learn about combining datasets: concat and append in Pandas. |\n| [Merge-and-Join](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.07-Merge-and-Join.ipynb) | Learn about combining datasets: merge and join in Pandas. |\n| [Aggregation-and-Grouping](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.08-Aggregation-and-Grouping.ipynb) | Learn about aggregation and grouping in Pandas. |\n| [Pivot-Tables](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.09-Pivot-Tables.ipynb) | Learn about pivot tables in Pandas. |\n| [Working-With-Strings](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.10-Working-With-Strings.ipynb) | Learn about vectorized string operations in Pandas. |\n| [Working-with-Time-Series](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.11-Working-with-Time-Series.ipynb) | Learn about working with time series in pandas. |\n| [Performance-Eval-and-Query](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/pandas/03.12-Performance-Eval-and-Query.ipynb) | Learn about high-performance Pandas: eval() and query() in Pandas. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/matplotlib.png\">\n</p>\n\n## matplotlib\n\nIPython Notebook(s) demonstrating matplotlib functionality.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [matplotlib](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/matplotlib.ipynb) | Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. |\n| [matplotlib-applied](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/matplotlib-applied.ipynb) | Apply matplotlib visualizations to Kaggle competitions for exploratory data analysis.  Learn how to create bar plots, histograms, subplot2grid, normalized plots, scatter plots, subplots, and kernel density estimation plots. |\n| [Introduction-To-Matplotlib](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.00-Introduction-To-Matplotlib.ipynb) | Introduction to Matplotlib. |\n| [Simple-Line-Plots](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.01-Simple-Line-Plots.ipynb) | Learn about simple line plots in Matplotlib. |\n| [Simple-Scatter-Plots](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.02-Simple-Scatter-Plots.ipynb) | Learn about simple scatter plots in Matplotlib. |\n| [Errorbars.ipynb](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.03-Errorbars.ipynb) | Learn about visualizing errors in Matplotlib. |\n| [Density-and-Contour-Plots](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.04-Density-and-Contour-Plots.ipynb) | Learn about density and contour plots in Matplotlib. |\n| [Histograms-and-Binnings](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.05-Histograms-and-Binnings.ipynb) | Learn about histograms, binnings, and density in Matplotlib. |\n| [Customizing-Legends](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.06-Customizing-Legends.ipynb) | Learn about customizing plot legends in Matplotlib. |\n| [Customizing-Colorbars](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.07-Customizing-Colorbars.ipynb) | Learn about customizing colorbars in Matplotlib. |\n| [Multiple-Subplots](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.08-Multiple-Subplots.ipynb) | Learn about multiple subplots in Matplotlib. |\n| [Text-and-Annotation](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.09-Text-and-Annotation.ipynb) | Learn about text and annotation in Matplotlib. |\n| [Customizing-Ticks](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.10-Customizing-Ticks.ipynb) | Learn about customizing ticks in Matplotlib. |\n| [Settings-and-Stylesheets](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.11-Settings-and-Stylesheets.ipynb) | Learn about customizing Matplotlib: configurations and stylesheets. |\n| [Three-Dimensional-Plotting](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.12-Three-Dimensional-Plotting.ipynb) | Learn about three-dimensional plotting in Matplotlib. |\n| [Geographic-Data-With-Basemap](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.13-Geographic-Data-With-Basemap.ipynb) | Learn about geographic data with basemap in Matplotlib. |\n| [Visualization-With-Seaborn](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/matplotlib/04.14-Visualization-With-Seaborn.ipynb) | Learn about visualization with Seaborn. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/numpy.png\">\n</p>\n\n## numpy\n\nIPython Notebook(s) demonstrating NumPy functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [numpy](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/numpy.ipynb) | Adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays. |\n| [Introduction-to-NumPy](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.00-Introduction-to-NumPy.ipynb) | Introduction to NumPy. |\n| [Understanding-Data-Types](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.01-Understanding-Data-Types.ipynb) | Learn about data types in Python. |\n| [The-Basics-Of-NumPy-Arrays](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.02-The-Basics-Of-NumPy-Arrays.ipynb) | Learn about the basics of NumPy arrays. |\n| [Computation-on-arrays-ufuncs](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.03-Computation-on-arrays-ufuncs.ipynb) | Learn about computations on NumPy arrays: universal functions. |\n| [Computation-on-arrays-aggregates](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.04-Computation-on-arrays-aggregates.ipynb) | Learn about aggregations: min, max, and everything in between in NumPy. |\n| [Computation-on-arrays-broadcasting](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.05-Computation-on-arrays-broadcasting.ipynb) | Learn about computation on arrays: broadcasting in NumPy. |\n| [Boolean-Arrays-and-Masks](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.06-Boolean-Arrays-and-Masks.ipynb) | Learn about comparisons, masks, and boolean logic in NumPy. |\n| [Fancy-Indexing](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.07-Fancy-Indexing.ipynb) | Learn about fancy indexing in NumPy. |\n| [Sorting](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.08-Sorting.ipynb) | Learn about sorting arrays in NumPy. |\n| [Structured-Data-NumPy](http://nbviewer.jupyter.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/numpy/02.09-Structured-Data-NumPy.ipynb) | Learn about structured data: NumPy's structured arrays. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/python.png\">\n</p>\n\n## python-data\n\nIPython Notebook(s) demonstrating Python functionality geared towards data analysis.\n\n| Notebook | Description |\n|-----------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|\n| [data structures](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/structs.ipynb) | Learn Python basics with tuples, lists, dicts, sets. |\n| [data structure utilities](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/structs_utils.ipynb) | Learn Python operations such as slice, range, xrange, bisect, sort, sorted, reversed, enumerate, zip, list comprehensions. |\n| [functions](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/functions.ipynb) | Learn about more advanced Python features: Functions as objects, lambda functions, closures, *args, **kwargs currying, generators, generator expressions, itertools. |\n| [datetime](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/datetime.ipynb) | Learn how to work with Python dates and times: datetime, strftime, strptime, timedelta. |\n| [logging](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/logs.ipynb) | Learn about Python logging with RotatingFileHandler and TimedRotatingFileHandler. |\n| [pdb](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/pdb.ipynb) | Learn how to debug in Python with the interactive source code debugger. |\n| [unit tests](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/python-data/unit_tests.ipynb) | Learn how to test in Python with Nose unit tests. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/kaggle.png\">\n</p>\n\n## kaggle-and-business-analyses\n\nIPython Notebook(s) used in [kaggle](https://www.kaggle.com/) competitions and business analyses.\n\n| Notebook | Description |\n|-------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n| [titanic](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/kaggle/titanic.ipynb) | Predict survival on the Titanic.  Learn data cleaning, exploratory data analysis, and machine learning. |\n| [churn-analysis](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/analyses/churn.ipynb) | Predict customer churn.  Exercise logistic regression, gradient boosting classifers, support vector machines, random forests, and k-nearest-neighbors.  Includes discussions of confusion matrices, ROC plots, feature importances, prediction probabilities, and calibration/descrimination.|\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/spark.png\">\n</p>\n\n## spark\n\nIPython Notebook(s) demonstrating spark and HDFS functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [spark](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/spark/spark.ipynb) | In-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms. |\n| [hdfs](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/spark/hdfs.ipynb) | Reliably stores very large files across machines in a large cluster. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/mrjob.png\">\n</p>\n\n## mapreduce-python\n\nIPython Notebook(s) demonstrating Hadoop MapReduce with mrjob functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|\n| [mapreduce-python](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/mapreduce/mapreduce-python.ipynb) | Runs MapReduce jobs in Python, executing jobs locally or on Hadoop clusters. Demonstrates Hadoop Streaming in Python code with unit test and [mrjob](https://github.com/Yelp/mrjob) config file to analyze Amazon S3 bucket logs on Elastic MapReduce.  [Disco](https://github.com/discoproject/disco/) is another python-based alternative.|\n\n<br/>\n\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/aws.png\">\n</p>\n\n## aws\n\nIPython Notebook(s) demonstrating Amazon Web Services (AWS) and AWS tools functionality.\n\n\nAlso check out:\n\n* [SAWS](https://github.com/donnemartin/saws): A Supercharged AWS command line interface (CLI).\n* [Awesome AWS](https://github.com/donnemartin/awesome-aws): A curated list of libraries, open source repos, guides, blogs, and other resources.\n\n| Notebook | Description |\n|------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [boto](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#Boto) | Official AWS SDK for Python. |\n| [s3cmd](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#s3cmd) | Interacts with S3 through the command line. |\n| [s3distcp](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#s3distcp) | Combines smaller files and aggregates them together by taking in a pattern and target file.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster. |\n| [s3-parallel-put](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#s3-parallel-put) | Uploads multiple files to S3 in parallel. |\n| [redshift](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#redshift) | Acts as a fast data warehouse built on top of technology from massive parallel processing (MPP). |\n| [kinesis](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#kinesis) | Streams data in real time with the ability to process thousands of data streams per second. |\n| [lambda](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/aws/aws.ipynb#lambda) | Runs code in response to events, automatically managing compute resources. |\n\n<br/>\n<p align=\"center\">\n  <img src=\"https://raw.githubusercontent.com/donnemartin/data-science-ipython-notebooks/master/images/commands.png\">\n</p>\n\n## commands\n\nIPython Notebook(s) demonstrating various command lines for Linux, Git, etc.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [linux](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/linux.ipynb) | Unix-like and mostly POSIX-compliant computer operating system.  Disk usage, splitting files, grep, sed, curl, viewing running processes, terminal syntax highlighting, and Vim.|\n| [anaconda](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#anaconda) | Distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing, that aims to simplify package management and deployment. |\n| [ipython notebook](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#ipython-notebook) | Web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document. |\n| [git](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#git) | Distributed revision control system with an emphasis on speed, data integrity, and support for distributed, non-linear workflows. |\n| [ruby](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#ruby) | Used to interact with the AWS command line and for Jekyll, a blog framework that can be hosted on GitHub Pages. |\n| [jekyll](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#jekyll) | Simple, blog-aware, static site generator for personal, project, or organization sites.  Renders Markdown or Textile and Liquid templates, and produces a complete, static website ready to be served by Apache HTTP Server, Nginx or another web server. |\n| [pelican](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#pelican) | Python-based alternative to Jekyll. |\n| [django](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/commands/misc.ipynb#django) | High-level Python Web framework that encourages rapid development and clean, pragmatic design. It can be useful to share reports/analyses and for blogging. Lighter-weight alternatives include [Pyramid](https://github.com/Pylons/pyramid), [Flask](https://github.com/pallets/flask), [Tornado](https://github.com/tornadoweb/tornado), and [Bottle](https://github.com/bottlepy/bottle).\n\n## misc\n\nIPython Notebook(s) demonstrating miscellaneous functionality.\n\n| Notebook | Description |\n|--------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| [regex](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/misc/regex.ipynb) | Regular expression cheat sheet useful in data wrangling.|\n[algorithmia](http://nbviewer.ipython.org/github/TarrySingh/Machine-Learning-Tutorials/blob/master/misc/Algorithmia.ipynb) | Algorithmia is a marketplace for algorithms. This notebook showcases 4 different algorithms: Face Detection, Content Summarizer, Latent Dirichlet Allocation and Optical Character Recognition.|\n\n## notebook-installation\n\n### anaconda\n\nAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.\n\nFollow instructions to install [Anaconda](https://docs.continuum.io/anaconda/install) or the more lightweight [miniconda](http://conda.pydata.org/miniconda.html).\n\n### dev-setup\n\nFor detailed instructions, scripts, and tools to set up your development environment for data analysis, check out the [dev-setup](https://github.com/donnemartin/dev-setup) repo.\n\n### running-notebooks\n\nNote: If you intend to learn the hard way (preferred method)then I'd strongly advice to write as much code as you can yourself and not just run pre-written code. If you still want to test it, then do the following: \n\nTo view interactive content or to modify elements within the IPython notebooks, you must first clone or download the repository then run the notebook.  More information on IPython Notebooks can be found [here.](http://ipython.org/notebook.html)\n\n    $ git clone https://github.com/TarrySingh/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials.git\n    $ cd Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials\n    $ jupyter notebook\n    \n\nNotebooks tested with Python 3.7+\n\n## curated-list-of-deeplearning-blogs\n\n* A Blog From a Human-engineer-being http://www.erogol.com/ [(RSS)](http://www.erogol.com/feed/)\n* Aakash Japi http://aakashjapi.com/ [(RSS)](http://logicx24.github.io/feed.xml)\n* Adit Deshpande https://adeshpande3.github.io/ [(RSS)](https://adeshpande3.github.io/adeshpande3.github.io/feed.xml)\n* Advanced Analytics & R http://advanceddataanalytics.net/ [(RSS)](http://advanceddataanalytics.net/feed/)\n* Adventures in Data Land http://blog.smola.org [(RSS)](http://blog.smola.org/rss)\n* Agile Data Science http://blog.sense.io/ [(RSS)](http://blog.sense.io/rss/)\n* Ahmed El Deeb https://medium.com/@D33B [(RSS)](https://medium.com/feed/@D33B)\n* Airbnb Data blog http://nerds.airbnb.com/data/ [(RSS)](http://nerds.airbnb.com/feed/)\n* Alex Castrounis | InnoArchiTech http://www.innoarchitech.com/ [(RSS)](http://www.innoarchitech.com/feed.xml)\n* Alex Perrier http://alexperrier.github.io/ [(RSS)](http://alexperrier.github.io/feed.xml)\n* Algobeans | Data Analytics Tutorials & Experiments for the Layman https://algobeans.com [(RSS)](https://algobeans.com/feed/)\n* Amazon AWS AI Blog https://aws.amazon.com/blogs/ai/ [(RSS)](https://aws.amazon.com/blogs/amazon-ai/feed/)\n* Analytics Vidhya http://www.analyticsvidhya.com/blog/ [(RSS)](http://feeds.feedburner.com/AnalyticsVidhya)\n* Analytics and Visualization in Big Data @ Sicara https://blog.sicara.com [(RSS)](https://blog.sicara.com/feed)\n* Andreas M\u00fcller http://peekaboo-vision.blogspot.com/ [(RSS)](http://peekaboo-vision.blogspot.com/atom.xml)\n* Andrej Karpathy blog http://karpathy.github.io/ [(RSS)](http://karpathy.github.io/feed.xml)\n* Andrew Brooks http://brooksandrew.github.io/simpleblog/ [(RSS)](http://brooksandrew.github.io/simpleblog/feed.xml)\n* Andrey Kurenkov http://www.andreykurenkov.com/writing/ [(RSS)](http://www.andreykurenkov.com/writing/feed.xml/)\n* Anton Lebedevich's Blog http://mabrek.github.io/ [(RSS)](http://mabrek.github.io/feed.xml)\n* Arthur Juliani https://medium.com/@awjuliani [(RSS)](https://medium.com/feed/@awjuliani)\n* Audun M. \u00d8ygard http://www.auduno.com/ [(RSS)](http://auduno.tumblr.com/rss)\n* Avi Singh https://avisingh599.github.io/ [(RSS)](http://avisingh599.github.io/feed.xml)\n* Beautiful Data http://beautifuldata.net/ [(RSS)](http://beautifuldata.net/feed/)\n* Beckerfuffle http://mdbecker.github.io/ [(RSS)](http://mdbecker.github.io/atom.xml)\n* Becoming A Data Scientist http://www.becomingadatascientist.com/ [(RSS)](http://www.becomingadatascientist.com/feed/)\n* Ben Bolte's Blog http://benjaminbolte.com/ml/ [(RSS)](http://benjaminbolte.com/ml/)\n* Ben Frederickson http://www.benfrederickson.com/blog/ [(RSS)](http://www.benfrederickson.com/atom.xml)\n* Berkeley AI Research http://bair.berkeley.edu/blog/ [(RSS)](http://bair.berkeley.edu/blog/feed.xml)\n* Big-Ish Data http://bigishdata.com/ [(RSS)](http://bigishdata.com/feed/)\n* Blog on neural networks http://yerevann.github.io/ [(RSS)](http://yerevann.github.io/atom.xml)\n* Blogistic RegressionAbout Projects http://d10genes.github.io/blog/ [(RSS)](http://d10genes.github.io/blog/feed.xml)\n* blogR | R tips and tricks from a scientist https://drsimonj.svbtle.com/ [(RSS)](https://drsimonj.svbtle.com/)\n* Brain of mat kelcey http://matpalm.com/blog/ [(RSS)](http://matpalm.com/blog/feed)\n* Brilliantly wrong thoughts on science and programming https://arogozhnikov.github.io/ [(RSS)](http://arogozhnikov.github.io/feed.xml)\n* Bugra Akyildiz http://bugra.github.io/ [(RSS)](http://bugra.github.io/feeds/all.atom.xml)\n* Building Babylon https://building-babylon.net/ [(RSS)](http://building-babylon.net/feed/)\n* Carl Shan http://carlshan.com/ [(RSS)](http://feeds.feedburner.com/carlshan)\n* Chris Stucchio https://www.chrisstucchio.com/blog/index.html [(RSS)](http://www.chrisstucchio.com/blog/atom.xml)\n* Christophe Bourguignat https://medium.com/@chris_bour [(RSS)](https://medium.com/feed/@chris_bour)\n* Christopher Nguyen https://medium.com/@ctn [(RSS)](https://medium.com/feed/@ctn)\n* Cloudera Data Science Posts http://blog.cloudera.com/blog/category/data-science/ [(RSS)](http://blog.cloudera.com/blog/category/data-science/feed/)\n* colah's blog http://colah.github.io/archive.html [(RSS)](http://colah.github.io/rss.xml)\n* Cortana Intelligence and Machine Learning Blog https://blogs.technet.microsoft.com/machinelearning/ [(RSS)](http://blogs.technet.com/b/machinelearning/rss.aspx)\n* Daniel Forsyth http://www.danielforsyth.me/ [(RSS)](http://www.danielforsyth.me/rss/)\n* Daniel Homola http://danielhomola.com/category/blog/ [(RSS)](http://danielhomola.com/feed/)\n* Daniel Nee http://danielnee.com [(RSS)](http://danielnee.com/?feed=rss2)\n* Data Based Inventions http://datalab.lu/ [(RSS)](http://datalab.lu/atom.xml)\n* Data Blogger https://www.data-blogger.com/ [(RSS)](https://www.data-blogger.com/feed/)\n* Data Labs http://blog.insightdatalabs.com/ [(RSS)](http://blog.insightdatalabs.com/rss/)\n* Data Meets Media http://datameetsmedia.com/ [(RSS)](http://datameetsmedia.com/feed/)\n* Data Miners Blog http://blog.data-miners.com/ [(RSS)](http://blog.data-miners.com/feeds/posts/default?alt=rss)\n* Data Mining Research http://www.dataminingblog.com/ [(RSS)](http://feeds.feedburner.com/dataminingblog)\n* Data Mining: Text Mining, Visualization and Social Media http://datamining.typepad.com/data_mining/ [(RSS)](http://datamining.typepad.com/data_mining/atom.xml)\n* Data Piques http://blog.ethanrosenthal.com/ [(RSS)](http://blog.ethanrosenthal.com/feeds/all.atom.xml)\n* Data School http://www.dataschool.io/ [(RSS)](http://www.dataschool.io/rss/)\n* Data Science 101 http://101.datascience.community/ [(RSS)](http://101.datascience.community/feed/)\n* Data Science @ Facebook https://research.facebook.com/blog/datascience/ [(RSS)](https://research.facebook.com/blog/datascience/)\n* Data Science Insights http://www.datasciencebowl.com/data-science-insights/ [(RSS)](http://www.datasciencebowl.com/feed/)\n* Data Science Tutorials https://codementor.io/data-science/tutorial [(RSS)](https://www.codementor.io/data-science/tutorial/feed)\n* Data Science Vademecum http://datasciencevademecum.wordpress.com/ [(RSS)](http://datasciencevademecum.wordpress.com/feed/)\n* Dataaspirant http://dataaspirant.com/ [(RSS)](http://dataaspirant.wordpress.com/feed/)\n* Dataclysm http://blog.okcupid.com/ [(RSS)](http://blog.okcupid.com/index.php/feed/)\n* DataGenetics http://datagenetics.com/blog.html [(RSS)](http://datagenetics.com/feed/rss.xml)\n* Dataiku https://www.dataiku.com/blog/ [(RSS)](http://www.dataiku.com/feed.xml)\n* DataKind http://www.datakind.org/blog [(RSS)](http://feeds.feedburner.com/DataKin)\n* DataLook http://blog.datalook.io/ [(RSS)](http://blog.datalook.io/feed/)\n* Datanice https://datanice.wordpress.com/ [(RSS)](https://datanice.wordpress.com/feed/)\n* Dataquest Blog https://www.dataquest.io/blog/ [(RSS)](https://www.dataquest.io/blog/atom.xml)\n* DataRobot http://www.datarobot.com/blog/ [(RSS)](http://www.datarobot.com/feed/)\n* Datascope http://datascopeanalytics.com/blog [(RSS)](http://datascopeanalytics.com/rss)\n* DatasFrame http://tomaugspurger.github.io/ [(RSS)](http://tomaugspurger.github.io/feeds/all.rss.xml)\n* David Mimno http://www.mimno.org/ [(RSS)](http://mimno.infosci.cornell.edu/b/feed.xml)\n* Dayne Batten http://daynebatten.com [(RSS)](http://daynebatten.com/feed/)\n* Deep Learning http://deeplearning.net/blog/ [(RSS)](http://deeplearning.net/feed/)\n* Deepdish http://deepdish.io/ [(RSS)](http://deepdish.io/atom.xml)\n* Delip Rao http://deliprao.com/ [(RSS)](http://deliprao.com/feed)\n* DENNY'S BLOG http://blog.dennybritz.com/ [(RSS)](http://blog.dennybritz.com/feed/)\n* Dimensionless https://dimensionless.in/blog/ [(RSS)](https://dimensionless.in/feed)\n* Distill http://distill.pub/ [(RSS)](http://distill.pub/rss.xml)\n* District Data Labs http://districtdatalabs.silvrback.com/ [(RSS)](https://districtdatalabs.silvrback.com/feed)\n* Diving into data https://blog.datadive.net/ [(RSS)](http://blog.datadive.net/feed/)\n* Domino Data Lab's blog http://blog.dominodatalab.com/ [(RSS)](http://blog.dominodatalab.com/rss/)\n* Dr. Randal S. Olson http://www.randalolson.com/blog/ [(RSS)](http://www.randalolson.com/feed/)\n* Drew Conway https://medium.com/@drewconway [(RSS)](https://medium.com/feed/@drewconway)\n* Dustin Tran http://dustintran.com/blog/ [(RSS)](http://dustintran.com/blog/rss/)\n* Eder Santana https://edersantana.github.io/blog.html [(RSS)](http://edersantana.github.io/feed.xml)\n* Edwin Chen http://blog.echen.me [(RSS)](http://blog.echen.me/feeds/all.rss.xml)\n* EFavDB http://efavdb.com/ [(RSS)](http://efavdb.com/feed/)\n* Emilio Ferrara, Ph.D.  http://www.emilio.ferrara.name/ [(RSS)](http://www.emilio.ferrara.name/feed/)\n* Entrepreneurial Geekiness http://ianozsvald.com/ [(RSS)](http://ianozsvald.com/feed/)\n* Eric Jonas http://ericjonas.com/archives.html [(RSS)](http://ericjonas.com/archives.html)\n* Eric Siegel http://www.predictiveanalyticsworld.com/blog [(RSS)](http://feeds.feedburner.com/predictiveanalyticsworld/GXRy)\n* Erik Bern http://erikbern.com [(RSS)](http://erikbern.com/feed/)\n* ERIN SHELLMAN http://www.erinshellman.com/ [(RSS)](http://www.erinshellman.com/feed/)\n* Eugenio Culurciello http://culurciello.github.io/ [(RSS)](http://culurciello.github.io/feed.xml)\n* Fabian Pedregosa http://fa.bianp.net/ [(RSS)](http://fa.bianp.net/blog/feed/)\n* Fast Forward Labs http://blog.fastforwardlabs.com/ [(RSS)](http://blog.fastforwardlabs.com/rss)\n* FastML http://fastml.com/ [(RSS)](http://fastml.com/atom.xml)\n* Florian Hartl http://florianhartl.com/ [(RSS)](http://florianhartl.com/feed/)\n* FlowingData http://flowingdata.com/ [(RSS)](http://flowingdata.com/feed/)\n* Full Stack ML http://fullstackml.com/ [(RSS)](http://fullstackml.com/feed/)\n* GAB41 http://www.lab41.org/gab41/ [(RSS)](http://www.lab41.org/feed/)\n* Garbled Notes http://www.chioka.in/ [(RSS)](http://www.chioka.in/feed.xml)\n* Greg Reda http://www.gregreda.com/blog/ [(RSS)](http://www.gregreda.com/feeds/all.atom.xml)\n* Hyon S Chu https://medium.com/@adailyventure [(RSS)](https://medium.com/feed/@adailyventure)\n* i am trask http://iamtrask.github.io/ [(RSS)](http://iamtrask.github.io/feed.xml)\n* I Quant NY http://iquantny.tumblr.com/ [(RSS)](http://iquantny.tumblr.com/rss)\n* inFERENCe http://www.inference.vc/ [(RSS)](http://www.inference.vc/rss/)\n* Insight Data Science https://blog.insightdatascience.com/ [(RSS)](https://blog.insightdatascience.com/feed)\n* INSPIRATION INFORMATION http://myinspirationinformation.com/ [(RSS)](http://myinspirationinformation.com/feed/)\n* Ira Korshunova http://irakorshunova.github.io/ [(RSS)](http://irakorshunova.github.io/feed.xml)\n* I\u2019m a bandit https://blogs.princeton.edu/imabandit/ [(RSS)](https://blogs.princeton.edu/imabandit/feed/)\n* Jason Toy http://www.jtoy.net/ [(RSS)](http://jtoy.net/atom.xml)\n* Jeremy D. Jackson, PhD http://www.jeremydjacksonphd.com/ [(RSS)](http://www.jeremydjacksonphd.com/?feed=rss2)\n* Jesse Steinweg-Woods https://jessesw.com/ [(RSS)](https://jessesw.com/feed.xml)\n* Joe Cauteruccio http://www.joecjr.com/ [(RSS)](http://www.joecjr.com/feed/)\n* John Myles White http://www.johnmyleswhite.com/ [(RSS)](http://www.johnmyleswhite.com/feed/)\n* John's Soapbox http://joschu.github.io/ [(RSS)](http://joschu.github.io/feed.xml)\n* Jonas Degrave http://317070.github.io/ [(RSS)](http://317070.github.io/feed.xml)\n* Joy Of Data http://www.joyofdata.de/blog/ [(RSS)](http://www.joyofdata.de/blog/feed/)\n* Julia Evans http://jvns.ca/ [(RSS)](http://jvns.ca/atom.xml)\n* KDnuggets http://www.kdnuggets.com/ [(RSS)](http://feeds.feedburner.com/kdnuggets-data-mining-analytics)\n* Keeping Up With The Latest Techniques http://colinpriest.com/ [(RSS)](http://colinpriest.com/feed/)\n* Kenny Bastani http://www.kennybastani.com/ [(RSS)](http://www.kennybastani.com/feeds/posts/default?alt=rss)\n* Kevin Davenport http://kldavenport.com/ [(RSS)](http://kldavenport.com/feed/)\n* kevin frans http://kvfrans.com/ [(RSS)](http://kvfrans.com/rss/)\n* korbonits | Math \u2229 Data http://korbonits.github.io/ [(RSS)](http://korbonits.github.io/feed.xml)\n* Large Scale Machine Learning  http://bickson.blogspot.com/ [(RSS)](http://bickson.blogspot.com/feeds/posts/default)\n* LATERAL BLOG https://blog.lateral.io/ [(RSS)](https://blog.lateral.io/feed/)\n* Lazy Programmer http://lazyprogrammer.me/ [(RSS)](http://lazyprogrammer.me/feed/)\n* Learn Analytics Here https://learnanalyticshere.wordpress.com/ [(RSS)](https://learnanalyticshere.wordpress.com/feed/)\n* LearnDataSci http://www.learndatasci.com/ [(RSS)](http://www.learndatasci.com/feed/)\n* Learning With Data http://learningwithdata.com/ [(RSS)](http://learningwithdata.com/rss_feed.xml)\n* Life, Language, Learning http://daoudclarke.github.io/ [(RSS)](http://daoudclarke.github.io/atom.xml)\n* Locke Data https://itsalocke.com/blog/ [(RSS)](https://itsalocke.com/feed)\n* Louis Dorard http://www.louisdorard.com/blog/ [(RSS)](http://www.louisdorard.com/blog?format=rss)\n* M.E.Driscoll http://medriscoll.com/ [(RSS)](http://medriscoll.com/rss)\n* Machinalis http://www.machinalis.com/blog [(RSS)](http://www.machinalis.com/blog/feeds/rss/)\n* Machine Learning (Theory) http://hunch.net/ [(RSS)](http://hunch.net/?feed=rss2)\n* Machine Learning and Data Science http://alexhwoods.com/blog/ [(RSS)](http://alexhwoods.com/feed/)\n* Machine Learning https://charlesmartin14.wordpress.com/ [(RSS)](http://charlesmartin14.wordpress.com/feed/)\n* Machine Learning Mastery http://machinelearningmastery.com/blog/ [(RSS)](http://machinelearningmastery.com/feed/)\n* Machine Learning Blogs https://machinelearningblogs.com/ [(RSS)](https://machinelearningblogs.com/feed/)\n* Machine Learning, etc http://yaroslavvb.blogspot.com [(RSS)](http://yaroslavvb.blogspot.com/feeds/posts/default)\n* Machine Learning, Maths and Physics https://mlopezm.wordpress.com/ [(RSS)](https://mlopezm.wordpress.com/feed/)\n* Machine Learning Flashcards https://machinelearningflashcards.com/ $10, but a nicely illustrated set of 300 flash cards\n* Machined Learnings http://www.machinedlearnings.com/ [(RSS)](http://www.machinedlearnings.com/feeds/posts/default)\n* MAPPING BABEL https://jack-clark.net/ [(RSS)](https://jack-clark.net/feed/)\n* MAPR Blog https://www.mapr.com/blog [(RSS)](https://www.mapr.com/bigdata.xml)\n* MAREK REI http://www.marekrei.com/blog/ [(RSS)](http://www.marekrei.com/blog/feed/)\n* MARGINALLY INTERESTING http://blog.mikiobraun.de/ [(RSS)](http://feeds.feedburner.com/MarginallyInteresting)\n* Math \u2229 Programming http://jeremykun.com/ [(RSS)](http://jeremykun.wordpress.com/feed/)\n* Matthew Rocklin http://matthewrocklin.com/blog/ [(RSS)](http://matthewrocklin.com/blog/atom.xml)\n* Melody Wolk http://melodywolk.com/projects/ [(RSS)](http://melodywolk.com/feed/)\n* Mic Farris http://www.micfarris.com/ [(RSS)](http://www.micfarris.com/feed/)\n* Mike Tyka http://mtyka.github.io/ [(RSS)](http://mtyka.github.io//feed.xml)\n* minimaxir | Max Woolf's Blog http://minimaxir.com/ [(RSS)](http://minimaxir.com/rss.xml)\n* Mirror Image https://mirror2image.wordpress.com/ [(RSS)](http://mirror2image.wordpress.com/feed/)\n* Mitch Crowe http://www.dataphoric.com/ [(RSS)](http://www.dataphoric.com/feed.xml)\n* MLWave http://mlwave.com/ [(RSS)](http://mlwave.com/feed/)\n* MLWhiz http://mlwhiz.com/ [(RSS)](http://mlwhiz.com/atom.xml)\n* Models are illuminating and wrong https://peadarcoyle.wordpress.com/ [(RSS)](http://peadarcoyle.wordpress.com/feed/)\n* Moody Rd http://blog.mrtz.org/ [(RSS)](http://blog.mrtz.org/feed.xml)\n* Moonshots http://jxieeducation.com/ [(RSS)](http://jxieeducation.com/feed.xml)\n* Mourad Mourafiq http://mourafiq.com/ [(RSS)](http://mourafiq.com/atom.xml)\n* My thoughts on Data science, predictive analytics, Python http://shahramabyari.com/ [(RSS)](http://shahramabyari.com/feed/)\n* Natural language processing blog http://nlpers.blogspot.fr/ [(RSS)](http://nlpers.blogspot.com/feeds/posts/default)\n* Neil Lawrence http://inverseprobability.com/blog.html [(RSS)](http://inverseprobability.com/rss.xml)\n* NLP and Deep Learning enthusiast http://camron.xyz/ [(RSS)](http://camron.xyz/index.php/feed/)\n* no free hunch http://blog.kaggle.com/ [(RSS)](http://blog.kaggle.com/feed/)\n* Nuit Blanche http://nuit-blanche.blogspot.com/ [(RSS)](http://nuit-blanche.blogspot.com/feeds/posts/default)\n* Number 2147483647 https://no2147483647.wordpress.com/ [(RSS)](http://no2147483647.wordpress.com/feed/)\n* On Machine Intelligence https://aimatters.wordpress.com/ [(RSS)](https://aimatters.wordpress.com/feed/)\n* Opiate for the masses Data is our religion. http://opiateforthemass.es/ [(RSS)](http://opiateforthemass.es/feed.xml)\n* p-value.info http://www.p-value.info/ [(RSS)](http://www.p-value.info/feeds/posts/default)\n* Pete Warden's blog http://petewarden.com/ [(RSS)](http://feeds.feedburner.com/typepad/petewarden)\n* Plotly Blog http://blog.plot.ly/ [(RSS)](http://blog.plot.ly/rss)\n* Probably Overthinking It http://allendowney.blogspot.ca/ [(RSS)](http://allendowney.blogspot.com/feeds/posts/default)\n* Prooffreader.com http://www.prooffreader.com [(RSS)](http://www.prooffreader.com/feeds/posts/default)\n* ProoffreaderPlus http://prooffreaderplus.blogspot.ca/ [(RSS)](http://prooffreaderplus.blogspot.ca/feeds/posts/default)\n* Publishable Stuff http://www.sumsar.net/ [(RSS)](http://www.sumsar.net/atom.xml)\n* PyImageSearch http://www.pyimagesearch.com/ [(RSS)](http://feeds.feedburner.com/Pyimagesearch)\n* Pythonic Perambulations https://jakevdp.github.io/ [(RSS)](http://jakevdp.github.com/atom.xml)\n* quintuitive http://quintuitive.com/ [(RSS)](http://quintuitive.com/feed/)\n* R and Data Mining https://rdatamining.wordpress.com/ [(RSS)](http://rdatamining.wordpress.com/feed/)\n* R-bloggers http://www.r-bloggers.com/ [(RSS)](http://feeds.feedburner.com/RBloggers)\n* R2RT http://r2rt.com/ [(RSS)](http://r2rt.com/feeds/all.atom.xml)\n* Ramiro G\u00f3mez http://ramiro.org/notebooks/ [(RSS)](http://ramiro.org/notebook/rss.xml)\n* Random notes on Computer Science, Mathematics and Software Engineering http://barmaley-exe.github.io/ [(RSS)](http://feeds.feedburner.com/barmaley-exe-blog-feed)\n* Randy Zwitch http://randyzwitch.com/ [(RSS)](http://randyzwitch.com/feed.xml)\n* RaRe Technologies http://rare-technologies.com/blog/ [(RSS)](http://rare-technologies.com/feed/)\n* Rayli.Net http://rayli.net/blog/ [(RSS)](http://rayli.net/blog/feed/)\n* Revolutions http://blog.revolutionanalytics.com/ [(RSS)](http://blog.revolutionanalytics.com/atom.xml)\n* Rinu Boney http://rinuboney.github.io/ [(RSS)](http://rinuboney.github.io/feed.xml)\n* RNDuja Blog http://rnduja.github.io/ [(RSS)](http://rnduja.github.io/feed.xml)\n* Robert Chang https://medium.com/@rchang [(RSS)](https://medium.com/feed/@rchang)\n* Rocket-Powered Data Science http://rocketdatascience.org [(RSS)](http://rocketdatascience.org/?feed=rss2)\n* Sachin Joglekar's blog https://codesachin.wordpress.com/ [(RSS)](https://codesachin.wordpress.com/feed/)\n* samim https://medium.com/@samim [(RSS)](https://medium.com/feed/@samim)\n* Sean J. Taylor http://seanjtaylor.com/ [(RSS)](http://seanjtaylor.com/rss)\n* Sebastian Raschka http://sebastianraschka.com/blog/index.html [(RSS)](http://sebastianraschka.com/rss_feed.xml)\n* Sebastian Ruder http://sebastianruder.com/ [(RSS)](http://sebastianruder.com/rss/)\n* Sebastian's slow blog http://www.nowozin.net/sebastian/blog/ [(RSS)](http://www.nowozin.net/sebastian/blog/feeds/all.atom.xml)\n* SFL Scientific Blog https://sflscientific.com/blog/ [(RSS)](http://sflscientific.com/blog/?format=rss)\n* Shakir's Machine Learning Blog http://blog.shakirm.com/ [(RSS)](http://blog.shakirm.com/feed/)\n* Simply Statistics http://simplystatistics.org [(RSS)](http://simplystatistics.org/feed/)\n* Springboard Blog http://springboard.com/blog\n* Startup.ML Blog http://startup.ml/blog [(RSS)](http://www.startup.ml/blog?format=RSS)\n* Statistical Modeling, Causal Inference, and Social Science http://andrewgelman.com/ [(RSS)](http://andrewgelman.com/feed/)\n* Stigler Diet http://stiglerdiet.com/ [(RSS)](http://stiglerdiet.com/feeds/all.atom.xml)\n* Stitch Fix Tech Blog http://multithreaded.stitchfix.com/blog/ [(RSS)](http://multithreaded.stitchfix.com/feed.xml)\n* Stochastic R&D Notes http://arseny.info/ [(RSS)](http://arseny.info/feeds/all.rss.xml)\n* Storytelling with Statistics on Quora http://datastories.quora.com/ [(RSS)](http://datastories.quora.com/rss)\n* StreamHacker http://streamhacker.com/ [(RSS)](http://feeds.feedburner.com/StreamHacker)\n* Subconscious Musings http://blogs.sas.com/content/subconsciousmusings/ [(RSS)](http://feeds.feedburner.com/advanalytics)\n* Swan Intelligence http://swanintelligence.com/ [(RSS)](http://swanintelligence.com/feeds/all.rss.xml)\n* TechnoCalifornia http://technocalifornia.blogspot.se/ [(RSS)](http://technocalifornia.blogspot.com/feeds/posts/default)\n* TEXT ANALYSIS BLOG | AYLIEN http://blog.aylien.com/ [(RSS)](http://blog.aylien.com/rss)\n* The Angry Statistician http://angrystatistician.blogspot.com/ [(RSS)](http://angrystatistician.blogspot.com/feeds/posts/default)\n* The Clever Machine https://theclevermachine.wordpress.com/ [(RSS)](http://theclevermachine.wordpress.com/feed/)\n* The Data Camp Blog https://www.datacamp.com/community/blog [(RSS)](http://blog.datacamp.com/feed/)\n* The Data Incubator http://blog.thedataincubator.com/ [(RSS)](http://blog.thedataincubator.com/feed/)\n* The Data Science Lab https://datasciencelab.wordpress.com/ [(RSS)](http://datasciencelab.wordpress.com/feed/)\n* THE ETZ-FILES http://alexanderetz.com/ [(RSS)](http://nicebrain.wordpress.com/feed/)\n* The Science of Data http://www.martingoodson.com [(RSS)](http://www.martingoodson.com/rss/)\n* The Shape of Data https://shapeofdata.wordpress.com [(RSS)](https://shapeofdata.wordpress.com/feed/)\n* The unofficial Google data science Blog http://www.unofficialgoogledatascience.com/ [(RSS)](http://www.unofficialgoogledatascience.com/feeds/posts/default)\n* Tim Dettmers http://timdettmers.com/ [(RSS)](http://timdettmers.com/feed/)\n* Tombone's Computer Vision Blog http://www.computervisionblog.com/ [(RSS)](http://www.computervisionblog.com/feeds/posts/default)\n* Tommy Blanchard http://tommyblanchard.com/category/projects [(RSS)](http://tommyblanchard.com/feeds/all.atom.xml)\n* Trevor Stephens http://trevorstephens.com/ [(RSS)](http://trevorstephens.com/feed.xml)\n* Trey Causey http://treycausey.com/ [(RSS)](http://treycausey.com/feeds/all.atom.xml)\n* UW Data Science Blog http://datasciencedegree.wisconsin.edu/blog/ [(RSS)](http://datasciencedegree.wisconsin.edu/feed/)\n* Wellecks http://wellecks.wordpress.com/ [(RSS)](http://wellecks.wordpress.com/feed/)\n* Wes McKinney http://wesmckinney.com/archives.html [(RSS)](http://wesmckinney.com/feeds/all.atom.xml)\n* While My MCMC Gently Samples http://twiecki.github.io/ [(RSS)](http://twiecki.github.io/atom.xml)\n* WildML http://www.wildml.com/ [(RSS)](http://www.wildml.com/feed/)\n* Will do stuff for stuff http://rinzewind.org/blog-en [(RSS)](http://rinzewind.org/feed-en)\n* Will wolf http://willwolf.io/ [(RSS)](http://willwolf.io/feed/)\n* WILL'S NOISE http://www.willmcginnis.com/ [(RSS)](http://www.willmcginnis.com/feed/)\n* William Lyon http://www.lyonwj.com/ [(RSS)](http://www.lyonwj.com/atom.xml)\n* Win-Vector Blog http://www.win-vector.com/blog/ [(RSS)](http://www.win-vector.com/blog/feed/)\n* Yanir Seroussi http://yanirseroussi.com/ [(RSS)](http://yanirseroussi.com/feed/)\n* Zac Stewart http://zacstewart.com/ [(RSS)](http://zacstewart.com/feed.xml)\n* \u0177hat http://blog.yhat.com/ [(RSS)](http://blog.yhat.com/rss.xml)\n* \u211auantitative \u221aourney http://outlace.com/ [(RSS)](http://outlace.com/feed.xml)\n* \u5927\u30c8\u30ed http://blog.otoro.net/ [(RSS)](http://blog.otoro.net/feed.xml)\n\n\n## credits\n\n* [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython/dp/1449319793) by Wes McKinney\n* [PyCon 2015 Scikit-learn Tutorial](https://github.com/jakevdp/sklearn_pycon2015) by Jake VanderPlas\n* [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook) by Jake VanderPlas\n* [Parallel Machine Learning with scikit-learn and IPython](https://github.com/ogrisel/parallel_ml_tutorial) by Olivier Grisel\n* [Statistical Interference Using Computational Methods in Python](https://github.com/AllenDowney/CompStats) by Allen Downey\n* [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples) by Aymeric Damien\n* [TensorFlow Tutorials](https://github.com/pkmital/tensorflow_tutorials) by Parag K Mital\n* [TensorFlow Tutorials](https://github.com/nlintz/TensorFlow-Tutorials) by Nathan Lintz\n* [TensorFlow Tutorials](https://github.com/alrojo/tensorflow-tutorial) by Alexander R Johansen\n* [TensorFlow Book](https://github.com/BinRoot/TensorFlow-Book) by Nishant Shukla\n* [Summer School 2015](https://github.com/mila-udem/summerschool2015) by mila-udem\n* [Keras tutorials](https://github.com/leriomaggio/deep-learning-keras-tensorflow) by Valerio Maggio\n* [Kaggle](https://www.kaggle.com/)\n* [Yhat Blog](http://blog.yhat.com/)\n\n## contributing\n\nContributions are welcome!  For bug reports or requests please [submit an issue](https://github.com/tarrysingh/Machine-Learning-Tutorials//issues).\n\n## contact-info\n\nFeel free to contact me to discuss any issues, questions, or comments.\n\n* Email: [tarry.singh@gmail.com](mailto:tarry.singh@gmail.com)\n* Twitter: [@tarrysingh](https://twitter.com/tarrysingh)\n* GitHub: [tarrysingh](https://github.com/tarrysingh.com)\n* LinkedIn: [Tarry Singh](https://www.linkedin.com/in/tarrysingh)\n* Website: [tarrysingh.com](https://tarrysingh.com)\n* Medium: [tarry@Medium](https://medium.com/@tarrysingh)\n* Quora : [Answers from Tarry on Quora](https://www.quora.com/profile/Tarry-Singh)\n\n## license\n\nThis repository contains a variety of content; some developed by Tarry Singh and some from third-parties and a lot will be maintained by me. The third-party content is distributed under the license provided by those parties.\n\nThe content was originally started by Donne Martin is distributed under the following license in 2017. I have been further developing and maintaining it by adding PyTorch, Torch/Lua, MXNET and much more:\n\n*I am providing code and resources in this repository to you under an open source license.*\n\n    Copyright 2017 Tarry Singh\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
 },
 {
  "repo": "msasnur/Healthcare-Analytics",
  "language": "Jupyter Notebook",
  "readme_contents": "# Healthcare-Analytics\nContents of Repository:\n- Python Notebook file contains project code for Data Exploration, Feature Engineering, and Machine Learning models (Naive Bayes, XGBoost, Neural Networks).\n- PDF Report file contains overview of the project, predicitions and results.\n- Datasets.zip contains both the test and train data used in the project.\n- HTML file is a markdown of the jupyter notebook with alll the outputs to View without python or its IDE.\n\nIntroduction:\n\n  Healthcare organizations are under increasing pressure to improve patient care outcomes and achieve better care. While this situation represents a challenge, it also offers organizations an opportunity to dramatically improve the quality of care by leveraging more value and insights from their data. Health care analytics refers to the analysis of data using quantitative and qualitative techniques to explore trends and patterns in the acquired data. While healthcare management uses various metrics for performance, a patient\u2019s length of stay is an important one.\n\n  Being able to predict the length of stay (LOS) allows hospitals to optimize their treatment plans to reduce LOS, to reduce infection rates among patients, staff, and visitors.\n\nProject Highligths:\n\nHospital admission data was analyzed to accurately predict the patient\u2019s Length of Stay at the time of admit so that the hospitals can optimize resources and function better. Built 3 models in Python to predict the length of stay,\n\n-\tA supervised algorithm Na\u00efve Bayes which was classifying with an accuracy of 34.55%.\n\n-\tAn ensemble method XGBoost which was predicting with an accuracy of 43.05%.\n\n-\tA dense neural network with 6 layers which yields an accuracy of 42.5%.\n\n"
 },
 {
  "repo": "glungu/udacity-healthcare-ai",
  "language": "Jupyter Notebook",
  "readme_contents": "# udacity-healthcare-ai\nUdacity AI for Healthcare Nanodegree Projects\n"
 },
 {
  "repo": "popsolutions/openventilator",
  "language": "OpenSCAD",
  "readme_contents": "# OpenVentilator\n\n> Welcome to the OpenVentilator project. This is an Open Source Ventilator / Mechanical Respirator for the Covid-19 Crisis.\n> Specially Designed for countries in Africa / South America / Middle East and other poor regions in the world\n\n[![N|Solid](https://popsolutions.co/web/image/65243/CommunitySupport.png)](https://popsolutions.co/forum/openventilator-5)   [![N|Solid](https://popsolutions.co/web/image/65245/t_logo.png)](https://t.me/openventilator) \n\n<img src=\"images/OpenVentilatorLogo.png\" height=300> <img src=\"images/OpenVentilatorSpartanModel.png\" height=300>\n\n# :heavy_exclamation_mark::heavy_exclamation_mark:DISCLAIMER \n**PROJECT STATUS:** We still need validation with health regulatory institutions and compliance with clinical requirements. - The tests on Lung Simulators are Sucessfull, limited on application depending on hardware availability but viable on emergency cases\n\n**PLEASE DO NOT USE THIS MACHINE IF NOT NEEDED**, WE DO NOT GUARANTEE THE OPERATION OF THIS MACHINE | THIS MACHINE IS FOR EMERGENCY and HEALTH SYSTEM COLLAPSE SCENARIOS :warning: \n\n# Main Goal\n\nDesign, Build, Validate a reliable Ventilation Medical Equipment Project for people, regions, countries in difficult economical situations with a component-agnostic philosophy. This is why we didn't continue putting efforts into the projects being developed by other teams, who have a different society and economic reality.\n\n**The equipment must have as few industrial parts as possible. If necessary, parts must be easily accessible, even in small towns and villages.**\n\n## Technical highlights\n\nOpenVentilor has adjustable PEEP and maximum pressure threshold, with a wide range. It has adjustable timing for the inspiratory and expiratory phases, allowing a wide range of BPM and I/E. The volume per breath is adjustable mechanically.\n\n# Getting Started\nSummary\n\n- [Join the Team](https://t.me/openventilator)\n- Documentation\n  - [Specifications](Specifications.md)\n  - [System description](SystemDescription.md)\n  - [Part List](00_Documentation/TheSpartanModel/PartsList.md)\n  - [Assembly Guide](Assembly.md)\n  - [Testing Your Machine]()\n  - [Regulations on your Country IYBITB]()\n- [Project philosophy](projectPhilosophy.md)\n- [Research we based on](00_Documentation/Research)\n- [Test Results](/00_Documentation/SimulatorTest/SpartanV1.0/SpartanV1.0.zip)\n\n## Community\n\n- [Website](https://www.popsolutions.co/openventilator)\n- [Forum](https://popsolutions.co/forum/openventilator-5)\n- [Social Media]()\n- [Problem / Issue Report]()\n\n## If you want to help\n\n[First, ** CLICK HERE ** to complete the form please, so we can organize everybody](https://www.popsolutions.co/openventilator-jointheteam) \n\nThen join the Whatsapp group and talk with Amanda (+55 11 99735-5042 ): https://chat.whatsapp.com/HRMx9xzVdt8Gpmwgm7ZVZ3\n\nThis initiative as other projects was born on the [Open Source COVID19 Medical Supplies](https://web.facebook.com/groups/opensourcecovid19medicalsupplies/) Facebook Group by the awareness of the need to create a Ventilator solution for the scarcity plaguing our society worldwide. I contacted Jeremias Almada from Argentina who by that time had presented an Ambu solution and a Cad Design. The idea was interesting but needed improvements.\n\nSince then we tried to establish some development and community standards and evolved the project several times.\n\n## Special thanks to\n\n - [Jose Ign\u00e1cio M\u00e9ndez](https://www.linkedin.com/in/jos%C3%A9-ignacio-m%C3%A9ndez-0ba3ab53/)\n - [Jeremias Almada](https://www.linkedin.com/in/almada-jerem%C3%ADas-43888680)\n - Fabian Franz\n - [Washington Perez](https://www.linkedin.com/in/washingtonperez/) \n - [Jaqueline Passos](https://www.linkedin.com/in/jaquelinepassos/)\n - [Amanda Pellini](https://www.linkedin.com/in/amanda-cristina-maciel-pellini-9177226a/)\n - [Marguel Gutierrez](https://www.linkedin.com/in/marguelgtz/)\n - [Henrique Aguilar](https://www.linkedin.com/in/henriaguilar/)\n - [Vandeir Soares](https://www.facebook.com/vandeir.soares.7)\n - [F\u00e1bio Soares](https://www.linkedin.com/in/fabio-julio-sores-soares-58852630/)\n - [GlobaltTech](http://www.globaltechc.com.br/)\n - [Samtronic](http://www.samtronic.com.br/)\n - [Ethan Moses](https://www.cameradactyl.com/)\n - [Joris Robijn](https://www.linkedin.com/in/jorisrobijn/)\n - [Ramon Bastos]\n - [Faizan Shaikh](https://www.linkedin.com/in/faizanzshaikh)\n - [Matheus Prado]\n - [Leonardo Automni]\n - [Diego Sangiorgi]\n - [Rodrigo Song]\n - [Wendell Mendes](https://www.linkedin.com/in/1endell)\n - [Rodrigo Borges](http://linkedin.com/in/rborges111)\n - [Henrique Nery](https://www.linkedin.com/in/henrique-nery-650216a2/) \n - [Duit](https://www.duit.com.br/)\n - [Carlos Delfino](https://github.com/CarlosDelfino)\n - [Marcio Dultra](https://www.linkedin.com/in/marciodultra)\n - [Ba\u00fa da Eletr\u00f4nica](https://www.baudaeletronica.com.br/)\n - [Tr\u00eas meninas hardware store](https://www.google.com/maps/place/Casa+das+3+Meninas/@-23.5391312,-46.6524764,19.5z/data=!4m5!3m4!1s0x0:0x377232460c40d90d!8m2!3d-23.5391706!4d-46.6524278)\n - [Rogers Guedes]\n - [The MIT guys from this paper](https://web.mit.edu/2.75/projects/DMD_2010_Al_Husseini.pdf): Abdul Mohsen Al Husseini, Heon Ju Lee, Justin Negrete, Stephen Powelson, Amelia Servil, Alexander Slocum, Jussi Saukkonen. \n - [Draeger for supplying parts to test with]\n\nAll our families, wives and husbands that for the last days have been supporting us on our craziness.\n\nAll the doctors, nurses and paramedics in the field fighting this common enemy.\n"
 },
 {
  "repo": "awslabs/fhir-works-on-aws-deployment",
  "language": "TypeScript",
  "readme_contents": "# FHIR Works on AWS deployment\n\n# This GitHub repository has been migrated. You can now find FHIR Works on AWS at https://github.com/aws-solutions/fhir-works-on-aws.\n\n## Upgrade notice\n\nVersions 3.1.1 and 3.1.2 of the `fhir-works-on-aws-authz-smart` package have been deprecated for necessary security updates. Please upgrade to version 3.1.3 or higher. For more information, see [the fhir-works-on-aws-authz-smart security advisory](https://github.com/awslabs/fhir-works-on-aws-authz-smart/security/advisories/GHSA-vv7x-7w4m-q72f).\n\n## Summary\n\nFHIR Works on AWS is a framework that can be used to deploy a [FHIR server](https://www.hl7.org/fhir/overview.html) on AWS. Using this framework, you can customize and add different FHIR functionality to best serve your use cases. When deploying this framework, by default [Cognito and role based access control](https://github.com/awslabs/fhir-works-on-aws-authz-rbac) is used. However, if preferred, you can be authenticated and authorized to access the FHIR server\u2019s resources by using [SMART](https://github.com/awslabs/fhir-works-on-aws-authz-smart) instead of Cognito. Cognito is the default AuthN/AuthZ provider because it is easier to configure than SMART. It doesn\u2019t require setting up a separate IDP server outside of AWS as compared to SMART. However, Cognito authentication is not granular. When a new user is created, it is assigned into the auditor, practitioner, or non-practitioner groups. Depending on the group, the user gets access to different groups of FHIR resources.\nThe AuthN/Z providers are defined in `package.json` and `config.ts`. You can choose  appropriate providers. SMART allows greater granularity into authentication than Cognito and is the FHIR standard. It allows you to access a FHIR record only if that record has reference to the user.\n\n## FHIR Works on AWS features\n\nFHIR Works on AWS utilizes AWS Lambda, Amazon DynamoDB, Amazon S3 and Amazon Elasticsearch Service to provide the following FHIR features:\n\n+ Create, Read, Update, Delete (CRUD) operations for all R4 or STU3 base FHIR resources\n+ Search capabilities per resource type\n+ Ability to do versioned reads ([vread](https://www.hl7.org/fhir/http.html#vread))\n+ Ability to post a transaction bundle of 25 entries or less. Presently, transaction bundles with only 25 entries or less are supported.\n\n## Accessing FHIR Works on AWS\n\nThe easiest and quickest way to access FHIR Works on AWS is by using [AWS solution](https://aws.amazon.com/solutions/implementations/fhir-works-on-aws/). To modify the code and set up a developer environment, follow the steps below:\n\n**Note**: AWS Solution provides an earlier version(See Solutions [CHANGELOG](https://github.com/awslabs/fhir-works-on-aws-deployment/blob/aws-solution/CHANGELOG.md) for more details) of FWoA install. Please follow the instruction below to install from GitHub repository if you wish to install the latest version.\n\n1. Clone or download the repository to a local directory.\n\nExample:\n\n```sh\ngit clone https://github.com/awslabs/fhir-works-on-aws-deployment.git\n```\n\n**Note**: To modify FHIR Works on AWS, create your own fork of the GitHub repository. This allows you to check in any changes you make to your private copy of the code.\n\n2. Use one of the following links to download FHIR Works on AWS:\n\n- [Linux/macOS](./INSTALL.md#linux-or-macos-installation)\n- [Windows](./INSTALL.md#windows-installation)\n- [Docker](./INSTALL.md#docker-installation)\n\n\n3. Refer to these [instructions](./DEVELOPMENT.md) for making code changes.\n\nIf you intend to use FHIR Implementation Guides read the [Using Implementation Guides](./USING_IMPLEMENTATION_GUIDES.md) documentation first.\n\nIf you intend to do a multi-tenant deployment read the [Using Multi-Tenancy](./USING_MULTI_TENANCY.md) documentation first.\n\nIf you intend to use FHIR Subscriptions read the [Using Subscriptions](./USING_SUBSCRIPTIONS.md) documentation first.\n\n## Architecture\n\nThe system architecture consists of multiple layers of AWS serverless services. The endpoint is hosted using API Gateway. The database and storage layer consists of Amazon DynamoDB and S3, with Elasticsearch as the search index for the data written to DynamoDB. The endpoint is secured by API keys and Cognito for user-level authentication and user-group authorization. The diagram below shows the FHIR server\u2019s system architecture components and how they are related.\n\n![Architecture](resources/architecture.png)\n\n## Components overview\n\nFHIR Works on AWS is powered by single-function components. These functions provide you the flexibility to plug your own implementations, if needed. The components used in this deployment are:\n+ [Interface](https://github.com/awslabs/fhir-works-on-aws-interface) - Defines communication between the components.\n+ [Routing](https://github.com/awslabs/fhir-works-on-aws-routing) - Accepts HTTP FHIR requests, routes it to the other components, logs the errors, transforms output to HTTP responses and generates the [Capability Statement](https://www.hl7.org/fhir/capabilitystatement.html).\n+ [Authorization](https://github.com/awslabs/fhir-works-on-aws-authz-rbac) - Accepts the access token found in HTTP header and the action the request is trying to perform. It then determines if that action is permitted.\n+ [Persistence](https://github.com/awslabs/fhir-works-on-aws-persistence-ddb) - Contains the business logic for creating, reading, updating, and deleting the FHIR record from the database. FHIR also supports \u2018conditional\u2019 CRUD actions and patching.\n   + Bundle - Supports multiple incoming requests as one request. Think of someone wanting to create five patients at once instead of five individual function calls. There are two types of bundles, batch and transaction. We currently only support transaction bundles of size 25 entries or fewer, but support batch bundles of up to 750 entries. This 750 limit was drawn from the Lambda payload limit of 6MB and an average request size of 4KB, divided in half to allow for flexibility in request size. This limit can also be configured in the `config.ts`, by specifying the `maxBatchSize` when constructing the `DynamoDBBundleService`.\n+ [Search](https://github.com/awslabs/fhir-works-on-aws-search-es) - Enables system-wide searching (/?name=bob) and type searching (/Patient/?name=bob).\n+ History - (*Not implemented*) Searches all archived/older versioned resources. This can be done at a system, type or instance level.\n\n## License\n\nThis project is licensed under the Apache-2.0 license.\n\n## Setting variables for FHIR on AWS\n\n### Retrieving user variables\n\nAfter installation, all user-specific variables (such as `USER_POOL_APP_CLIENT_ID`) can be found in the `Info_Output.log` file. You can also retrieve these values by running the following command:\n```\nserverless info --verbose --region <REGION> --stage <STAGE>.\n```\n**Note**: The default stage is `dev` and region is `us-west-2`.\n\nIf you are receiving `Error: EACCES: permission denied` when running a command, try re-running it using `sudo`.\n\n### Accessing the FHIR API\n\nThe FHIR API can be accessed through `API_URL` using the following REST syntax:\n```sh\ncurl -H \"Accept: application/json\" -H \"Authorization: Bearer <COGNITO_AUTH_TOKEN>\" -H \"x-api-key:<API_KEY>\" <API_URL>\n```\nFor more information, click [here](http://hl7.org/fhir/http.html).\n\n### Using Postman to make API requests\n\nTo access APIs, you can use Postman as well.  [Postman](https://www.postman.com/) is an API Client for RESTful services that can run on your development desktop for making requests to the FHIR Server. Postman is highly suggested and enables easier access to the FHRI API. You can use Postman to make API requests by following the steps below:\n\n**Importing the collection file**\n\nUnder the Postman folder, you can access the JSON definitions for some API requests that you can make against the server. To import these requests into your Postman application, click [here](https://kb.datamotion.com/?ht_kb=postman-instructions-for-exporting-and-importing).\n\n**Note**: Ensure that you import the [Fhir.postman_collection.json](./postman/Fhir.postman_collection.json) collection file.\n\nAfter you import the collection, set up your environment. You can set up a local environment, a development environment, and a production environment. Each environment should have the correct values configured. For example, the value for `API_URL` for the local environment might be `localhost:3000` while the `API_URL` for the development environment would be your API gateway\u2019s endpoint.\n\n**Setting up environment variables**\n\nSet up the following three Postman environments:\n\n+ `Fhir_Local_Env.json`\n+ `Fhir_Dev_Env.json`\n+ `Fhir_Prod_Env.json`\n\nFor instructions on importing the environment JSON, click [here](https://thinkster.io/tutorials/testing-backend-apis-with-postman/managing-environments-in-postman).\n\nThe `COGNITO_AUTH_TOKEN` required for each of these files can be obtained by following the instructions under [Authorizing a user](#authorizing-a-user).\n\nThe following variables required in the Postman collection can be found in `Info_Output.log`:\n+ API_URL: from Service Information:endpoints: ANY\n+ API_KEY: from Service Information: api keys: developer-key\n\nTo find what FHIR Server supports, use the `GET Metadata` Postman request to retrieve the [Capability Statement](https://www.hl7.org/fhir/capabilitystatement.html)\n\n**Authorizing a user**\n\nFHIR Works on AWS uses Role-Based Access Control (RBAC) to determine what operations and what resource types a user can access. The default rule set can be found in [RBACRules.ts](https://github.com/awslabs/fhir-works-on-aws-deployment/blob/mainline/src/RBACRules.ts). To access the API, you must use the ID token. This ID token must include scopes of either `openid`, `profile` or `aws.cognito.signin.user.admin`.\n\nUsing either of these scopes provide information about users and their group. It helps determine what resources/records they can access.\n\n+ The `openid` scope returns all user attributes in the ID token that are readable by the client. The ID token is not returned if the openid scope is not requested by the client.\n+ The `profile` scope grants access to all user attributes that are readable by the client. This scope can only be requested with the openid scope.\n+ The `aws.cognito.signin.user.admin` scope grants access to [Amazon Cognito User Pool](https://docs.aws.amazon.com/cognito-user-identity-pools/latest/APIReference/Welcome.html) API operations that require access tokens, such as `UpdateUserAttributes` and `VerifyUserAttribute`.\n\nFor more information, click [here](https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pools-app-idp-settings.html).\n\n**Retrieving an ID token using aws.cognito.signin.user.admin**\n\nTo access the FHIR API, an ID token is required. A Cognito ID token can be obtained using the following command substituting all variables with their values from `Info_Output.log`.\n+\tFor Windows, enter:\n```sh\nscripts/init-auth.py <CLIENT_ID> <REGION>\n```\n+\tFor Mac, enter:\n```sh\npython3 scripts/init-auth.py <CLIENT_ID> <REGION>\n```\nThe return value is the `COGNITO_AUTH_TOKEN` (found in the postman collection) to be used for access to the FHIR APIs.\n\n### Accessing binary resources\n\nBinary resources are FHIR resources that consist of binary/unstructured data of any kind. This could be X-rays, PDF, video or other files. This implementation of the FHIR API has a dependency on the API Gateway and Lambda services, which currently have limitations in request/response sizes of 10 MB and 6 MB respectively. The workaround for this limitation is a hybrid approach of storing a binary resource\u2019s metadata in DynamoDB and using S3's get/putPreSignedUrl APIs. So in your requests to the FHIR API, you will store/get the Binary's metadata from DynamoDB and in the response object it will also contain a pre-signed S3 URL, which should be used to interact directly with the binary file.\n\n### Testing binary resources\n\n**Using Postman**\n\nTo test, use Postman.  For steps, click [here](https://github.com/awslabs/fhir-works-on-aws-deployment/blob/mainline/README.md#using-postman-to-make-api-requests).\n\n**Note**: We recommend you to test binary resources by using the `Binary` folder in Postman.\n\n**Using cURL**\n\nTo test this with cURL, follow these steps:\n1.\tPOST a binary resource to FHIR API using the following command:\n```sh\ncurl -H \"Accept: application/json\" -H \"Authorization: Bearer <COGNITO_AUTH_TOKEN>\" -H \"x-api-key:<API_KEY>\" --request POST \\\n  --data '{\"resourceType\": \"Binary\", \"contentType\": \"image/jpeg\"}' \\\n  <API_URL>/Binary\n```\n2. Check the POST's response. There will be a presignedPutUrl parameter. Use that pre-signed url to upload your file. See below for command\n```sh\ncurl -v -T \"<LOCATION_OF_FILE_TO_UPLOAD>\" \"<PRESIGNED_PUT_URL>\"\n```\n\n### Testing bulk data export\n\nBulk Export allows you to export all of your data from DDB to S3. We currently support the [System Level](https://hl7.org/fhir/uv/bulkdata/export/index.html#endpoint---system-level-export) export. For more information about bulk export, refer to the FHIR [Implementation Guide](https://hl7.org/fhir/uv/bulkdata/export/index.html).\n\nTo test this feature on FHIR Works on AWS, make API requests using the [Fhir.postman_collection.json](./postman/Fhir.postman_collection.json) file by following these steps:\n1.\tIn the FHIR Examples collection, under the **Export** folder, use `GET System Export` request to initiate an export request.\n2.\tIn the response, check the Content-Location header field for a URL. The URL should be in the `<base-url>/$export/<jobId>` format.\n3.\tTo get the status of the export job, in the **Export** folder, use the GET System Job Status request. Enter the `jobId` value from step 2 in that request.\n4.\tCheck the response that is returned from `GET System Job Status`. If the job is in progress, the response header will have the field `x-progress: in-progress`. Keep polling that URL every 10 seconds until the job is complete. Once the job is complete, you'll get a JSON body with presigned S3 URLs of your exported data. You can download the exported data using those URLs.\nExample:\n```sh\n{\n    \"transactionTime\": \"2021-03-29T16:49:00.819Z\",\n    \"request\": \"https://xyz.execute-api.us-west-2.amazonaws.com/$export?_outputFormat=ndjson&_since=1800-01-01T00%3A00%3A00.000Z&_type=Patient\",\n    \"requiresAccessToken\": false,\n    \"output\":\n    [\n        {\n            \"type\": \"Patient\",\n            \"url\": \"https://fhir-service-dev-bulkexportresultsbucket-.com/abc\"\n        }\n    ],\n    \"error\": []\n}\n```\n**Note**: To cancel an export job, use the `Cancel Export Job` request in the \"Export\" folder located in the Postman collections.\n\n## Troubleshooting FHIR Works on AWS\n\n+ If changes are required for the Elasticsearch instances, you may have to redeploy. Redeployment deletes the Elasticsearch cluster and creates a new one. Redeployment also deletes the data inside your cluster. In future releases, we will create a one-off lambda instance that can retrieve the data from DynamoDB to Elasticsearch. To do this, you can currently use either of the following options:\n   + You can manually push the DynamoDB data to Elasticsearch by creating a lambda instance.\n   + You can refresh your DynamoDB table with a backup.\n   + You can remove all data from the DynamoDB table and that will create parity between Elasticsearch and DynamoDB.\n\n+ Support for STU3 and [R4](https://www.hl7.org/fhir/validation.html) releases of FHIR is based on the JSON schema provided by HL7. The schema for R4 is more restrictive than the schema for [STU3](http://hl7.org/fhir/STU3/validation.html). The STU3 schema doesn\u2019t restrict appending additional fields into the POST/PUT requests of a resource, whereas the R4 schema has a strict definition of what is permitted in the request. You can access the schema [here](https://github.com/awslabs/fhir-works-on-aws-routing/blob/mainline/src/router/validation/schemas/fhir.schema.v3.json).\n\n**Note**: We are using the official schema provided by [HL7](https://www.hl7.org/fhir/STU3/downloads.html).\n\n+ When making a `POST`/`PUT` request to the server, if you get an error that includes the text `Failed to parse request body as JSON resource`, check that you've set the request headers correctly. The header for `Content-Type` should be either `application/json` or `application/fhir+json`. If you're using Postman for making requests, in the **Body** tab, make sure to change the setting to `raw` and `JSON`.\n![Postman Body Request Settings](resources/postman_body_request_settings.png)\n\n## Feedback\nWe'd love to hear from you! Please reach out to our team via [Github Issues](https://github.com/awslabs/fhir-works-on-aws-deployment/issues) for any feedback.\n"
 },
 {
  "repo": "GoogleCloudPlatform/healthcare-nlp-visualizer-demo",
  "language": "JavaScript",
  "readme_contents": "# Healthcare NLP Visualizer Demo\n\nThe demo application is a Node.js and React.js system to visualize the \nGoogle Cloud [Healthcare Natural Language API](https://cloud.google.com/healthcare/docs/how-tos/nlp).\nYou can upload your own sample medical text to visualize the output such as medical dictionaries,\nentity extraction and relationships, context assessment and more. We have also provided sample\ntexts for a a medical record, research paper and lab form. \n\n![screencast](screencast-short.gif)\n\n## Prerequisites \n\n1. A GCP Project with billing and the Healthcare NLP API enabled.\n1. Complete the Healthcare NLP [How-to Guide](https://cloud.google.com/healthcare/docs/how-tos/nlp).\n1. Familiarity with Google Cloud Functions and Vue.js.\n\n## Set Up Instructions\n\n### Backend\n\nThe HTTP Cloud Function can be found in the `/visualizer` directory. Please note, this code is NOT\nmeant for production use.\n\n1. Download the service account key for your project.\n1. Deploy the Cloud Function, you can follow the instructions [here](https://cloud.google.com/functions/docs/deploying).\n1. Copy the endpoint for your Cloud Function.\n\n### Frontend\n\nThe Vue.js app is found in the `/app` directory.\n\n1. ```cd app/```\n1. Paste your Cloud Function endpoint in to the placeholder in index.html. \n1. Start a local server of your choice and open the application in your \nbrowser.\n"
 },
 {
  "repo": "informatici/openhospital-gui",
  "language": "Java",
  "readme_contents": "# Open Hospital - GUI\n[![Java CI](https://github.com/informatici/openhospital-gui/workflows/Java%20CI%20with%20Maven/badge.svg)](https://github.com/informatici/openhospital-gui/actions?query=workflow%3A%22Java+CI+with+Maven%22)\n\nThis is the GUI component of [Open Hospital][openhospital]: it contains a graphical user interface (GUI) made with Java Swing. \nThis project depends on the [Core component][openhospital-core] for the business logic and the data abstraction layer. \nAn alternative user interface based on React and currently still work-in-progress is available [here][openhospital-ui].\n\n## How to build\n\nTo build this project you'll need Java JDK 8+ and Maven. \nAdditionally, you'll need to build and install locally the [Core component][openhospital-core] of Open Hospital.\nOnce you do that, to build this project just issue:\n\n  mvn package\n  \nTo run the tests simply issue:\n\n  mvn test\n  \n## How to launch Open Hospital\n\nTo launch Open Hospital GUI, use the scripts `oh.sh` (on Linux) or `oh.bat` (on Windows) from the maven `target` folder.\nYou will need a MySQL database running locally (e.g. the Docker container available in the Core project),\nor any similar SQL database (e.g. MariaDB).\n\n### Launch within IDE\n\nBe sure to have configured properly the classpath for the project (see [5 Installing Open Hospital 1.13.0 in Eclipse EE](https://github.com/informatici/openhospital-doc/blob/develop/doc_admin/AdminManual.adoc#5-installing-open-hospital-1-13-0-in-eclipse-ee))\n\nBefore running the application, you should generate the config files with the `g)` option, or manually copying and renaming the files `*.dist` files in `rsc/` folder and edit them accordingly:\n\n| Dist file                | Property file       | Properties to fill in                                         |\n|--------------------------|---------------------|---------------------------------------------------------------|\n| database.properties.dist | database.properties | DBSERVER, DBPORT, DBNAME, DBUSER, DBPASS                      |\n| dicom.properties.dist    | dicom.properties    | OH_PATH_SUBSTITUTE/DICOM_DIR, DICOM_SIZE                      |\n| log4j.properties.dist    | log4j.properties    | LOG_DEST, DBSERVER, DBPORT, DBNAME, DBUSER, DBPASS, LOG_LEVEL |\n| settings.properties.dist | settings.properties | OH_LANGUAGE,(SINGLEUSER=)YES_OR_NO, PHOTO_DIR, OH_DOC_DIR     |\n\n*For further information, please refer to the Admin and User manuals in the [Doc project][openhospital-doc].*\n\n## How to contribute\n\nYou can find the contribution guidelines in the [Open Hospital wiki][contribution-guide]. \nA list of open issues is available on [Jira][jira].\n\n## Community\n\nYou can reach out to the community of contributors by joining \nour [Slack workspace][slack] or by subscribing to our [mailing list][ml].\n\n## Code style\n\nThis project uses a consistent code style and provides definitions for use in both IntelliJ and Eclipse IDEs.\n\n<details><summary>IntelliJ IDEA instructions</summary>\n\nFor IntelliJ IDEA the process for importing the code style is:\n\n* Select *Settings* in the *File* menu\n* Select *Editor*\n* Select *Code Style*\n* Expand the menu item and select *Java*\n* Go to *Scheme* at the top, click on the setting button by the side of the drop-down list\n* Select *Import Scheme*\n* Select *IntelliJ IDE code style XML*\n* Navigate to the location of the file which relative to the project root is: `.ide-settings/idea/OpenHospital-code-style-configuration.xml`\n* Select *OK* \n* At this point the code style is stored as part of the IDE and is used for **all** projects opened in the editor. To restrict the settings to just this project again select the setting button by the side of the *Scheme* list and select *Copy to Project...*. If successful a notice appears in the window that reads: *For current project*.\n\n</details>\n\n<details><summary>Eclipse instructions</summary>\n\nFor Eclipse the process requires loading the formatting style and the import order separately.\n\n* Select *Preferences* in the *Window* menu\n* Select *Java*\n* Select *Code Style* and expand the menu\n* Select *Formatter*\n* Select the *Import...* button\n* Navigate to the location of the file which relative to the project root is: `.ide-settings/eclipse/OpenHospital-Java-CodeStyle-Formatter.xml`\n* Select *Open*\n* At this point the code style is stored and is applicable to all projects opened in the IDE. To restrict the settings just to this project select *Configure Project Specific Settings...* in the upper right. In the next dialog select the *openhospital* repository and select *OK*. In the next dialog select the *Enable project specific settings* checkbox. Finally select *Apply and Close*.\n* Back in the *Code Style* menu area, select *Organize Imports*\n* Select *Import...*\n* Navigate to the location of the file which relative to the project root is: `.ide-settings/eclipse/OpenHospital.importorder`\n* Select *Open*\n* As with the formatting styles the import order is applicable to all projects. In order to change it just for this project repeat the same steps as above for *Configure Project Specific Settings...*\n \n</details> \n\n [openhospital]: https://www.open-hospital.org/\n [openhospital-core]: https://github.com/informatici/openhospital-core\n [openhospital-ui]: https://github.com/informatici/openhospital-ui\n [openhospital-doc]: https://github.com/informatici/openhospital-doc\n [contribution-guide]: https://openhospital.atlassian.net/wiki/display/OH/Contribution+Guidelines\n [jira]: https://openhospital.atlassian.net/jira/software/c/projects/OP/issues/\n [database.prop]: https://github.com/informatici/openhospital-core/blob/develop/src/test/resources/database.properties\n [slack]: https://join.slack.com/t/openhospitalworkspace/shared_invite/enQtOTc1Nzc0MzE2NjQ0LWIyMzRlZTU5NmNlMjE2MDcwM2FhMjRkNmM4YzI0MTAzYTA0YTI3NjZiOTVhMDZlNWUwNWEzMjE5ZDgzNWQ1YzE\n [ml]: https://sourceforge.net/projects/openhospital/lists/openhospital-devel\n"
 },
 {
  "repo": "Susmita-Dey/Sukoon",
  "language": "HTML",
  "readme_contents": "# Sukoon \nThis is a stress-relieving website project made for the hackathon [Hackofiesta](https://hack.iiitl.ac.in/). \nThis project is under the theme **Healthcare.**\nThis was our first hackathon.\n\n## \ud83d\udcc3 Description \n\"Welcome to our stress-relieving website : [Sukoon](https://sukoon-stress-free.netlify.app/)! Here, you'll find a variety of tools and resources to help you manage and reduce stress in your daily life. From carefully crafted playlists and relaxing podcasts, to articles and tips on stress management techniques, our goal is to provide you with a one-stop-shop for all of your stress-relief needs. Whether you're looking for a quick break during a hectic workday, or a longer practice to unwind at night, we've got you covered. Take a look around, try out some of our resources, and let us know if there's anything we can do to improve your experience. Remember, taking care of yourself is just as important as taking care of your work and projects, so don't hesitate to make time for stress relief in your busy schedule.\"\n\n## Website Link-\n<a href=\"https://sukoon-stress-free.netlify.app/\">Sukoon</a>\n\n## \ud83d\udd4a Our Tagline \nThe one step solution to get relief from your stress.\nLive a stress-free life.\n\n## \ud83d\udcdd Table of Contents\n- [Problem it Solves](#problem_statement)\n- [Services](#services)\n- [Get Started](#getStarted)\n- [Logo](#logo)\n- [Screenshots](#screenshots)\n- [Technology Stack](#tech_stack)\n- [Open-Source program](#open_source_programs)\n- [Project Admin](#admin)\n- [Contributors](#contributors)\n\n## \ud83d\udd0e Problems it Solves: <a name = \"problem_statement\"></a>\n- Gives mental peace \ud83e\uddd8\u200d\u2640\ufe0f\n- Reduces stress\n- Refreshes mood\n- Entertains people\n- Motivates people\n- Helps people to lead a healthy and succesful life.\n\n## \ud83d\udcbc Our Services <a name = \"services\"></a>\n- Audio Therapy\n- Reading Therapy\n- Yoga Therapy\n- Laughing Therapy\n- Talking Therapy\n- Consult A Doctor\n\n## \ud83d\ude80  Get Started <a name = \"getStarted\"></a>\nEvery contribution counts.\n1. Ensure that you have Git installed and working properly.\n2. Fork the repo by clicking on 'Fork' above.\n3. Clone the project by running git clone <forked_project_url>.\n4. Confused about where to start? Check out [good-first-issue](https://github.com/Susmita-Dey/Sukoon/labels/good%20first%20issue).\n5. Make a separate branch with the issue name ex. issue#485.\n6. You are good to go. Change the code and we will be waiting for your exciting PRs.\n\nFor contributing guidelines and standards, visit [contributing.md](https://github.com/Susmita-Dey/Sukoon/blob/main/CONTRIBUTING.md).\n\n## Our Logo <a name = \"logo\"></a>\n<img src=\"./logo.png\" width=140px height=110px alt=\"logo\">\n\n## \ud83d\udcf8 Screenshots <a name = \"screenshots\"></a>\n![readmeBanner](https://user-images.githubusercontent.com/98955085/184510782-3f699206-4768-4b3a-aa6d-40c924e13578.png)\n\n## Tech Stack <a name = \"tech_stack\"></a>\n<img alt=\"HTML5\" src=\"https://img.shields.io/badge/html5-%23fca9ae.svg?style=for-the-badge&logo=html5&logoColor=140200\"/>\n<img alt=\"CSS3\" src=\"https://img.shields.io/badge/css3-%23ffd2ce.svg?style=for-the-badge&logo=css3&logoColor=140200\"/>\n<img alt=\"JavaScript\" src=\"https://img.shields.io/badge/javascript-%23e4626b.svg?style=for-the-badge&logo=javascript&logoColor=%23F7DF1E\"/>\n\n## Open Source Programs  <a name = \"open_source_programs\"></a>\n \n<table>\n<tr>\n <td align=\"center\">\n<a href=\"https://ssoc.devfolio.co/\"><img src=\"https://user-images.githubusercontent.com/72400676/182021806-e7439fdd-8f9b-46a6-a1da-0bf731bbe379.png\" width=100px height=100px /><br /><sub><b>Social Summer Of Code 2022</b></sub></a>\n </td>\n <td align=\"center\">\n<a href=\"https://hacktoberfest.com/\"><img src=\"https://user-images.githubusercontent.com/79099734/195970153-ee19d55b-20fc-4ddb-a91d-000773699c37.png\" width=100px height=100px /><br /><sub><b>Hacktoberfest</b></sub></a>\n </td>\n </tr>\n</table>\n\n## \ud83d\ude0e Project Admin <a name = \"admin\"></a>\n\n<table>\n  <tr>\n<td align=\"center\"><a href=\"https://github.com/Susmita-Dey\"><img src=\"https://avatars.githubusercontent.com/u/79099734?v=4\" width=\"100px;\" alt=\"\"/><br /><sub><b>Susmita Dey</b></sub></a></td>\n  </tr>\n</table>\n\n<h2>Project Contributors\u2b50</h2>   <a name = \"contributors\"></a>\n<table align=\"center\">\n<tr>\n<td>\n<a href=\"https://github.com/Susmita-Dey/Sukoon/graphs/contributors\" align=\"center\">\n  <img src=\"https://contrib.rocks/image?repo=Susmita-Dey/Sukoon\" /> \n</a>\n</td>\n</tr>\n</table>\n\n---\n\n<p align=\"center\">\n  Made with \u2764 from India.\n</p>\n"
 },
 {
  "repo": "informatici/openhospital-api",
  "language": "Java",
  "readme_contents": "# Open Hospital API\n\n[![Java CI with Maven](https://github.com/informatici/openhospital-api/workflows/Java%20CI%20with%20Maven/badge.svg)](https://github.com/informatici/openhospital-api/actions?query=workflow%3A%22Java+CI+with+Maven%22)\n\nThis is the API project of [Open Hospital][openhospital]: it exposes a REST API of the business logic implemented in the [openhospital-core project][core].\n\n## Summary\n\n  * [How to build [WIP]](#how-to-build-wip)\n    + [Using Swagger-UI](#using-swagger-ui)\n    + [Using Postman](#using-postman)\n  * [How to deploy backend in docker environment](#how-to-deploy-backend-in-docker-environment)\n  * [Cleaning](#cleaning)\n  * [How to contribute](#how-to-contribute)\n  * [Community](#community)\n  * [Code style](#code-style)\n\n<small>Table of contents generated with <i><a href='http://ecotrust-canada.github.io/markdown-toc/'>markdown-toc</a></i></small>\n\n\n## How to build [WIP]\n\nFor the moment, to build this project you should \n\n 1. fetch and build the [core] project\n    \n        git clone https://github.com/informatici/openhospital-core.git\n        cd openhospital-core\n        mvn clean install -DskipTests=true\n        \n 2. clone and build this project\n \n        git clone https://github.com/informatici/openhospital-api\n        cd openhospital-api\n        mvn clean install -DskipTests=true\n        \n 3. prepare settings from each rsc/*.dist file\n \n        rsc/application.properties <- set a SHA-256 jwt token\n        rsc/database.properties\n        rsc/log4j.properties\n        rsc/...\n \n 4. set target/rsc/database.properties\n \n        DB can be created with `docker-compose up` from `openhospital-core` or using a dedicated MySQL server\n        \n 5. start openhospital-api (in `target` folder)\n \n        # Windows\n        java -cp \"openhospital-api-0.0.2.jar;rsc/;static/\" org.springframework.boot.loader.JarLauncher\n\n        # Linux\n        java -cp \"openhospital-api-0.0.2.jar:rsc/:static/\" org.springframework.boot.loader.JarLauncher\n        \n 6. call services\n    - URL base: http://localhost:8080\n    - URL login: http://localhost:8080/auth/login\n    - URL patients: http://localhost:8080/patients\n    - URL swagger: http://localhost:8080/swagger-ui/\n\nYou can see Swagger Api Documentation at: http://localhost:8080/swagger-ui/\n\n![image](https://user-images.githubusercontent.com/2938553/215335720-73d59075-f0df-44c4-93ed-eae79945bb71.png)\n   \n### Using Swagger-UI\n\n 1. use endpoint /auth/login to login and get the token\n \n![image](https://user-images.githubusercontent.com/2938553/228294801-4d27dd2c-9053-4f62-9497-690706232c9f.png)\n![image](https://user-images.githubusercontent.com/2938553/228294867-79d6a326-9e7d-4ca0-93cd-ce34c7b7373f.png)\n \n 2. use the Authorize button at the top of the Swagger-UI and paste the token form step #1 prefixed by the string \"Bearer \" and click Authorize\n\n![image](https://user-images.githubusercontent.com/2938553/228296149-64905464-441f-4b20-80af-4dcfb40aef4c.png)\n \n 3. close the dialog\n\n![image](https://user-images.githubusercontent.com/2938553/228294994-56c1ae3b-f7cb-49b6-94d4-c899fa20374e.png)\n\n 4. now all the endpoints are automatically secured and the token will be added to the request\n\n![image](https://user-images.githubusercontent.com/2938553/228295504-910a6036-4656-4645-8756-3dec0154eed4.png)\n![image](https://user-images.githubusercontent.com/2938553/228295166-d1948976-fbdb-4f7e-ab12-8f0621b21373.png)\n\n\n### Using Postman\n\n 1. import postman_collection.json in your Postman installation\n \n## How to deploy backend in Docker environment\n\nMake sure you have docker with docker-compose installed, then run the following commands:\n\n- copy `dotenv` file into `.env` and set variables as needed (the SHA-256 jwt token is needed)\n- run `make`\n- run `docker compose up -d database` (wait for some seconds the very first time to build the DB)\n- (optional - demo data) run `docker compose run --rm oh-database-init`\n- run `docker compose up backend`\n\nWhen done successfully, head over at http://localhost:[API_PORT]/swagger-ui/\n\nYou can change the deployment branch using an .env file.\n\n## Cleaning\n\n\tdocker compose rm --stop --volumes --force\n\tmake clean\n\n\n## How to contribute\n\nYou can find the contribution guidelines in the [Open Hospital wiki][contribution-guide].  \nA list of open issues is available on [Jira][jira].\n\n## Community\n\nYou can reach out to the community of contributors by joining \nour [Slack workspace][slack] or by subscribing to our [mailing list][ml].\n\n\n## Code style\n\nThis project uses a consistent code style and provides definitions for use in both IntelliJ and Eclipse IDEs.\n\n<details><summary>IntelliJ IDEA instructions</summary>\n\nFor IntelliJ IDEA the process for importing the code style is:\n\n* Select *Settings* in the *File* menu\n* Select *Editor*\n* Select *Code Style*\n* Expand the menu item and select *Java*\n* Go to *Scheme* at the top, click on the setting button by the side of the drop-down list\n* Select *Import Scheme*\n* Select *IntelliJ IDE code style XML*\n* Navigate to the location of the file which relative to the project root is:  `.ide-settings/idea/OpenHospital-code-style-configuration.xml`\n* Select *OK* \n* At this point the code style is stored as part of the IDE and is used for **all** projects opened in the editor.  To restrict the settings to just this project again select the setting button by the side of the *Scheme* list and select *Copy to Project...*. If successful a notice appears in the window that reads: *For current project*.\n\n</details>\n\n<details><summary>Eclipse instructions</summary>\n\nFor Eclipse the process requires loading the formatting style and the import order separately.\n\n* Select *Preferences* in the *Window* menu\n* Select *Java*\n* Select *Code Style* and expand the menu\n* Select *Formatter*\n* Select the *Import...* button\n* Navigate to the location of the file which relative to the project root is:  `.ide-settings/eclipse/OpenHospital-Java-CodeStyle-Formatter.xml`\n* Select *Open*\n* At this point the code style is stored and is applicable to all projects opened in the IDE.  To restrict the settings just to this project select *Configure Project Specific Settings...* in the upper right.  In the next dialog select the *openhospital* repository and select *OK*.  In the next dialog select the *Enable project specific settings* checkbox.  Finally select *Apply and Close*.\n* Back in the *Code Style* menu area, select *Organize Imports*\n* Select *Import...*\n* Navigate to the location of the file which relative to the project root is:  `.ide-settings/eclipse/OpenHospital.importorder`\n* Select *Open*\n* As with the formatting styles the import order is applicable to all projects.  In order to change it just for this project repeat the same steps as above for *Configure Project Specific Settings...*\n \n</details> \n\n[openhospital]: https://www.open-hospital.org/\n[core]: https://github.com/informatici/openhospital-core\n[contribution-guide]: https://openhospital.atlassian.net/wiki/display/OH/Contribution+Guidelines\n[jira]: https://openhospital.atlassian.net/jira/software/c/projects/OP/issues/\n[slack]: https://join.slack.com/t/openhospitalworkspace/shared_invite/enQtOTc1Nzc0MzE2NjQ0LWIyMzRlZTU5NmNlMjE2MDcwM2FhMjRkNmM4YzI0MTAzYTA0YTI3NjZiOTVhMDZlNWUwNWEzMjE5ZDgzNWQ1YzE\n[ml]: https://sourceforge.net/projects/openhospital/lists/openhospital-devel\n"
 },
 {
  "repo": "brishi19791/HealthCare-WebApplication",
  "language": "Java",
  "readme_contents": "# HealthCare-WebApplication\nHealth care web application using Spring MVC, Hibernate, MySQL\n\nIn order to run the project, we have to create few useraccounts for login.\n\nShould run java file present in below path which will create admin,doctor,nurse,lab assistant and pharmacist:\nsrc\\test\\java\\com\\neu\\project\\Mapping.java\n\nFlow of the Project:\n======================\n1) Admin can create doctor, nurse,lab assistant and pharmacist\n2)Patient should register and select a primary doctor.\n3)Nurse will take vitalSigns of the patient and send those to respective primary doctor\n4)Doctor will see the vitalSigns of the patient and decides to diagnose or give medication to the patient.\n\t4.1) If doctor wants to diagnose he can send a work request to the lab assistant by selecting kind of lab test.\n\t4.2) If doctor wants to give medication, he can select a drug which should be manufactured by the pharmacist\n5)Pharmacist will manufacture the drugs.\n6)Lab assistant will get work request from the doctor to send the lab test reports for a patient.\n7) Patient can login to see there encounterList, Lab test reports and medication provided by doctor.\n\n"
 },
 {
  "repo": "baoxuliang/healthcaredatastandard",
  "language": null,
  "readme_contents": "\u4e2d\u56fd\u536b\u751f\u4fe1\u606f\u6807\u51c6\n======================\n\n## \u6982\u8ff0\n\n\u572809\u5e74\u65b0\u533b\u6539\u4e4b\u540e\uff0c\u56fd\u5bb6\u5c42\u9762\u4e0a\u5bf9\u536b\u751f\u4fe1\u606f\u6807\u51c6\u7684\u91cd\u89c6\u7a0b\u5ea6\u4e5f\u7b97\u662f\u63d0\u9ad8\u4e86\uff0c\u4e5f\u6709\u4e00\u4e9b\u4e0d\u9519\u7684\u8fdb\u5c55\uff0c\u6574\u4f53\u4e0a\u8fd8\u662f\u504f\u6162\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65b0\u5f62\u52bf\u4e0b\u7684\u533b\u7597\u7c7b\u5e94\u7528\u548c\u7cfb\u7edf\u7684\u5f00\u53d1\u3002\n\n\u5f15\u7528\u6c64\u5904\u7684\u4e00\u5f20\u56fe\uff0c\u76ee\u524d\u56fd\u5bb6\u5c42\u9762\u4e0a\u7684\u536b\u751f\u4fe1\u606f\u6807\u51c6\u4f53\u7cfb\u5982\u4e0b\n![](overview.png)\n\n\u76ee\u524d\u5df2\u5b58\u5728\u7684\u6570\u91cf\n![](standard number -201506.png)\n\n\n## \u57fa\u7840\u7c7b\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS/T 303-2009  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u6807\u51c6\u5316\u89c4\u5219 | \n|  WS/T 304-2009  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u6a21\u5f0f\u63cf\u8ff0\u6307\u5357 | \n|  WS/T 305-2009  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u96c6\u5143\u6570\u636e\u89c4\u8303 | \n|  WS/T 306-2009  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u96c6\u5206\u7c7b\u4e0e\u7f16\u7801\u89c4\u5219 | \n|  WS/T 370-2012  | \u536b\u751f\u4fe1\u606f\u57fa\u672c\u6570\u636e\u96c6\u7f16\u5236\u89c4\u8303 | \n|  WS/T 482-2016  | \u536b\u751f\u4fe1\u606f\u5171\u4eab\u6587\u6863\u7f16\u5236\u89c4\u8303 | \n\n\n## \u6570\u636e\u7c7b\n             \n### \u6570\u636e\u5143\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS 363-2011  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55 \u7b2c1-17\u90e8\u5206 | \n| WS 364-2011  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u503c\u57df\u4ee3\u7801 \u7b2c1-17\u90e8\u5206 | \n\n| \u6807\u51c6\u7f16\u53f7 |   \u6807\u51c6\u4e2d\u6587\u540d\u79f0 |  \u6807\u8bc6\u7b26\u8303\u56f4 |  \u6570\u636e\u5143\u6570\u76ee | \n| ----  | ---- |  ---- |  ---- | \n| WS363.1-2011| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c1\u90e8\u5206:\u603b\u5219 \t| \t\t| \t| \n| WS363.2-2011| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c2\u90e8\u5206:\u6807\u8bc6\t| DE01.00.001.00-DE01.00.015.00\t| 13\n| WS363.3-2011| \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c3\u90e8\u5206:\u4eba\u53e3\u5b66\u53ca\u793e\u4f1a\u7ecf\u6d4e\u5b66\u7279\u5f81\t| DE02.01.001.00-DE02.01.058.00\t| 62\n| WS363.4-2011 |  \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c4\u90e8\u5206:\u5065\u5eb7\u53f2\t| DE02.10.001.00-DE02.10.096.00\t| 90\n| WS363.5-2011 | \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c5\u90e8\u5206:\u5065\u5eb7\u5371\u9669\u56e0\u7d20\t| DE03.00.001.00-DE03.00.099.00\t| 98\n| WS363.6-2011 | \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c6\u90e8\u5206:\u4e3b\u8bc9\u4e0e\u75c7\u72b6\t| DE04.01.001.00-DE04.01.120.00\t| 119\n| WS363.7-2011 | \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c7\u90e8\u5206:\u4f53\u683c\u68c0\u67e5\t| DE04.10.001.00-DE04.10.243.00\t| 241\n| WS363.8-2011 | \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c8\u90e8\u5206:\u4e34\u5e8a\u8f85\u52a9\u68c0\u67e5\t| DE04.30.001.00-DE04.30.051.00\t| 51\n| WS363.9-2011 | \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c9\u90e8\u5206:\u5b9e\u9a8c\u5ba4\u68c0\u67e5\t| DE04.50.001.00-DE04.50.129.00\t| 129\n| WS363.10-2011| \t\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c10\u90e8\u5206:\u533b\u5b66\u8bca\u65ad\t| DE05.01.001.00-DE05.01.073.00\t| 73\n| WS363.11-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c11\u90e8\u5206:\u533b\u5b66\u8bc4\u4f30\t| DE05.10.001.00-DE05.10.128.00\t| 127\n| WS363.12-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c12\u90e8\u5206:\u8ba1\u5212\u4e0e\u5e72\u9884\t| DE06.00.001.00-DE06.00.177.00\t| 175\n| WS363.13-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c13\u90e8\u5206:\u536b\u751f\u8d39\u7528\t| DE07.00.001.00-DE07.00.010.00\t| 10\n| WS363.14-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c14\u90e8\u5206:\u536b\u751f\u673a\u6784\t| DE08.10.001.00-DE08.10.053.00\t| 53\n| WS363.15-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c15\u90e8\u5206:\u536b\u751f\u4eba\u5458\t| DE08.30.001.00-DE08.30.031.00\t| 31\n| WS363.16-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c16\u90e8\u5206:\u836f\u54c1\u3001\u8bbe\u5907\u4e0e\u6750\u6599\t| DE08.50.001.00-DE08.50.025.00\t| 25\n| WS363.17-2011\t| \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55\u7b2c17\u90e8\u5206:\u536b\u751f\u7ba1\u7406\t| DE09.00.001.00-DE09.00.102.00\t| 102\n\n### \u4ee3\u7801\u4e0e\u7f16\u7801\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS 364-2011  | \u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u503c\u57df\u4ee3\u7801 \u7b2c1-17\u90e8\u5206 | \n| WS xxx-2013  | \u536b\u751f\u7edf\u8ba1\u6307\u6807\u76ee\u5f55 \u7b2c1-10\u90e8\u5206 | \n| GB xxx-2013  | \u75be\u75c5\u5206\u7c7b\u4e0e\u4ee3\u7801 | \n| WS 446-2014  |  \u5c45\u6c11\u5065\u5eb7\u6863\u6848\u533b\u5b66\u68c0\u9a8c\u9879\u76ee\u5e38\u7528\u4ee3\u7801 | \n| WS xxx-2013  |  \u533b\u7597\u670d\u52a1\u9879\u76ee\u5206\u7c7b\u4e0e\u7f16\u7801 | \n\n\n### \u6570\u636e\u96c6\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS 365-2011  | \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6 | \n| WS 371-2012  | \u57fa\u672c\u4fe1\u606f\u57fa\u672c\u6570\u636e\u96c6 \u4e2a\u4eba\u4fe1\u606f | \n| WS 372-2012  | \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6  \u7b2c1-6\u90e8\u5206 | \n| WS 373-2012  | \u533b\u7597\u670d\u52a1\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-3\u90e8\u5206 | \n| WS 374-2012  | \u536b\u751f\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-4\u90e8\u5206 | \n| WS 375-2012  | \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-23\u90e8\u5206 | \n| WS 376-2013  | \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-5\u90e8\u5206 | \n| WS 377-2013  | \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-7\u90e8\u5206 | \n| WS xxx-2013  | \u536b\u751f\u5e94\u6025\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1-5\u90e8\u5206 | \n| WS xxx-2013  | \u533b\u5b66\u6570\u5b57\u5f71\u50cf\u901a\u4fe1\u57fa\u672c\u6570\u636e\u96c6 | \n| WS xxx-2013  | \u65b0\u578b\u519c\u6751\u5408\u4f5c\u533b\u7597\u57fa\u672c\u6570\u636e\u96c6 | \n| WS xxx-2013  | \u5c45\u6c11\u5065\u5eb7\u5361\u6570\u636e\u96c6 | \n| WS xxx-2013  | \u5c45\u6c11\u5065\u5eb7\u5361\u6ce8\u518c\u7ba1\u7406\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u6570\u636e\u96c6 | \n\n\n| \u6570\u636e\u96c6\u540d\u79f0\t| \u6240\u5c5e\u7c7b\u522b\t| \u6240\u5c5e\u6807\u51c6\u7f16\u53f7\t| \u6240\u5c5e\u6807\u51c6\u540d\u79f0   |   \n| ----  | ---- |  ---- |  ---- |       \n| \u4e2a\u4eba\u57fa\u672c\u4fe1\u606f\t| \u57fa\u672c\u4fe1\u606f\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6      | \n| \u5065\u5eb7\u4f53\u68c0\u4fe1\u606f\t| \u5065\u5eb7\u4f53\u68c0\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6      | \n| \u65b0\u751f\u513f\u5bb6\u5ead\u8bbf\u89c6\u4fe1\u606f\t| \u513f\u7ae5\u4fdd\u5065\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6         | \n| \u513f\u7ae5\u5065\u5eb7\u68c0\u67e5\u4fe1\u606f\t| \u513f\u7ae5\u4fdd\u5065\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6       | \n| \u4ea7\u524d\u968f\u8bbf\u670d\u52a1\u4fe1\u606f\t| \u5987\u5973\u4fdd\u5065\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6     | \n| \u4ea7\u540e\u8bbf\u89c6\u670d\u52a1\u4fe1\u606f\t| \u5987\u5973\u4fdd\u5065\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6      | \n| \u4ea7\u540e42\u5929\u5065\u5eb7\u4f53\u68c0\u4fe1\u606f\t| \u5987\u5973\u4fdd\u5065\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6|       \n| \u9884\u9632\u63a5\u79cd\u5361\u4fe1\u606f\t| \u75be\u75c5\u63a7\u5236\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6  |     \n| \u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\t| \u75be\u75c5\u63a7\u5236\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6| \n| \u804c\u4e1a\u75c5\u62a5\u544a\u5361\u4fe1\u606f\t| \u75be\u75c5\u63a7\u5236\t| WS365| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6       |  \n| \u98df\u6e90\u6027\u75be\u75c5\u62a5\u544a\u5361\u4fe1\u606f\t| \u75be\u75c5\u63a7\u5236\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6 |       \n| \u9ad8\u8840\u538b\u60a3\u8005\u968f\u8bbf\u4fe1\u606f\t| \u75be\u75c5\u7ba1\u7406\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6 |      \n| 2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u968f\u8bbf\u4fe1\u606f\t| \u75be\u75c5\u7ba1\u7406\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6  |      \n| \u91cd\u6027\u7cbe\u795e\u75be\u75c5\u60a3\u8005\u7ba1\u7406\u4fe1\u606f\t| \u75be\u75c5\u7ba1\u7406\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6   |        \n| \u95e8\u8bca\u6458\u8981\u4fe1\u606f\t| \u533b\u7597\u670d\u52a1\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6   |    \n| \u4f4f\u9662\u6458\u8981\u4fe1\u606f\t| \u533b\u7597\u670d\u52a1\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6  |    \n| \u4f1a\u8bca\u4fe1\u606f\t| \u533b\u7597\u670d\u52a1\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6      | \n| \u8f6c\u9662(\u8bca)\u4fe1\u606f\t| \u533b\u7597\u670d\u52a1\t| WS365\t| \u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6  | \n\n\u5f3a\u5236\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6\n\n| \u6807\u51c6\u7f16\u53f7 |   \u6807\u51c6\u4e2d\u6587\u540d\u79f0 | \u6570\u636e\u5143\u6570\u76ee | \u6027\u8d28 | \n| ----  | ---- |  ---- | \u5f3a\u5236\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6|  \n| WS 371-2012 \t\t| \u57fa\u672c\u4fe1\u606f\u57fa\u672c\u6570\u636e\u96c6 \u4e2a\u4eba\u4fe1\u606f\t\t| 68\n| WS 376.1-2013\t\t| \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206 \u51fa\u751f\u533b\u5b66\u8bc1\u660e\t\t| 39\n| WS 376.2-2013\t\t| \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u513f\u7ae5\u5065\u5eb7\u4f53\u68c0\t\t| 61\n| WS 376.3-2013\t\t| \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u65b0\u751f\u513f\u75be\u75c5\u7b5b\u67e5\t\t| 41\n| WS 376.4-2013\t\t| \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\uff1a\u4f53\u5f31\u513f\u7ae5\u7ba1\u7406\t\t| 33\n| WS 376.5-2013\t\t| \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c5\u90e8\u5206\uff1a5\u5c81\u4ee5\u4e0b\u513f\u7ae5\u6b7b\u4ea1\u62a5\u544a\t\t| 27\n| WS 377.1-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u5a5a\u524d\u4fdd\u5065\u670d\u52a1\t\t| 122\n| WS 377.2-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206 \u5987\u5973\u75c5\u666e\u67e5\t\t| 74\n| WS 377.3-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\u8ba1\u5212\u751f\u80b2\u6280\u672f\u670d\u52a1\t\t| 131\n| WS 377.4-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\u5b55\u4ea7\u671f\u4fdd\u5065\u670d\u52a1\u4e0e\u9ad8\u5371\u7ba1\u7406\t\t| 244\n| WS 377.5-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c5\u90e8\u5206\u4ea7\u524d\u7b5b\u67e5\u4e0e\u8bca\u65ad\t\t| 31\n| WS 377.6-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206\u51fa\u751f\u7f3a\u9677\u76d1\u6d4b\t\t| 48\n| WS 377.7-2013\t\t| \u5987\u5973\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c7\u90e8\u5206\u5b55\u4ea7\u5987\u6b7b\u4ea1\u62a5\u544a\t\t| 39\n| WS 375.1-2012 \t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u827e\u6ecb\u75c5\u7efc\u5408\u9632\u6cbb\t\t| 71\n| WS 375.2-2012 \t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u8840\u5438\u866b\u75c5\u75c5\u4eba\u7ba1\u7406\t\t| 115\n| WS 375.3-2012\t\t| \u00a0\u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u6162\u6027\u4e1d\u866b\u75c5\u75c5\u4eba\u7ba1\u7406\t\t| 73\n| WS 375.4-2012\t\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\uff1a\u804c\u4e1a\u75c5\u62a5\u544a\t\t| 63\n| WS 375.5-2012\t    | \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c5\u90e8\u5206\uff1a\u804c\u4e1a\u6027\u5065\u5eb7\u76d1\u62a4\t\t| 261\n| WS 375.6-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206\uff1a\u4f24\u5bb3\u76d1\u6d4b\u62a5\u544a\t\t| 41\n| WS 375.7-2012 \t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c7\u90e8\u5206\uff1a\u519c\u836f\u4e2d\u6bd2\u62a5\u544a\t\t| 33\n| WS 375.8-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c8\u90e8\u5206\uff1a\u884c\u4e3a\u5371\u9669\u56e0\u7d20\u76d1\u6d4b\t\t| 56\n| WS 375.9-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c9\u90e8\u5206\uff1a\u6b7b\u4ea1\u533b\u5b66\u8bc1\u660e\t\t| 49\n| WS 375.10-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c10\u90e8\u5206\uff1a\u4f20\u67d3\u75c5\u62a5\u544a\t\t| 33\n| WS 375.11-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c11\u90e8\u5206\uff1a\u7ed3\u6838\u75c5\u62a5\u544a\t\t| 78\n| WS 375.12-2012\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c12\u90e8\u5206\uff1a\u9884\u9632\u63a5\u79cd\t\t| 42\n| WS 375.14-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c14\u90e8\u5206\uff1a\u5b66\u6821\u7f3a\u52e4\u7f3a\u8bfe\u76d1\u6d4b\u62a5\u544a |          \n| WS 375.15-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c15\u90e8\u5206\uff1a\u6258\u5e7c\u673a\u6784\u7f3a\u52e4\u76d1\u6d4b\u62a5\u544a |          \n| WS 375.18-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c18\u90e8\u5206\uff1a\u7591\u4f3c\u9884\u9632\u63a5\u79cd\u5f02\u5e38\u53cd\u5e94\u62a5\u544a |          \n| WS 375.19-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c19\u90e8\u5206\uff1a\u75ab\u82d7\u7ba1\u7406 |        \n| WS 375.20-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c20\u90e8\u5206\uff1a\u8111\u5352\u4e2d\u767b\u8bb0\u62a5\u544a |    \n| WS 375.21-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c21\u90e8\u5206\uff1a\u8111\u5352\u4e2d\u75c5\u4eba\u7ba1\u7406 |    \n| WS 375.22-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c22\u90e8\u5206\uff1a\u5bab\u9888\u764c\u7b5b\u67e5\u767b\u8bb0 |    \n| WS 375.23-2016\u00a0\t| \u75be\u75c5\u63a7\u5236\u57fa\u672c\u6570\u636e\u96c6 \u7b2c23\u90e8\u5206\uff1a\u5927\u80a0\u764c\u7b5b\u67e5\u767b\u8bb0 |    \n| WS 372.1-2012 \t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u4e59\u809d\u60a3\u8005\u7ba1\u7406\t\t| 106\n| WS 372.2-2012\t\t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u9ad8\u8840\u538b\u60a3\u8005\u5065\u5eb7\u7ba1\u7406\t| 106\n| WS 372.3-2012 \t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u91cd\u6027\u7cbe\u795e\u75be\u75c5\u60a3\u8005\u7ba1\u7406\t\t| 118\n| WS 372.4-2012 \t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\uff1a\u8001\u5e74\u4eba\u5065\u5eb7\u7ba1\u7406\t\t| 102\n| WS 372.5-2012 \t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c5\u90e8\u5206\uff1a2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u5065\u5eb7\u7ba1\u7406\t\t| 113\n| WS 372.6-2012 \t| \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206\uff1a\u80bf\u7624\u75c5\u4f8b\u7ba1\u7406\t\t| 72\n| WS 373.1-2012 \t| \u533b\u7597\u670d\u52a1\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u95e8\u8bca\u6458\u8981\t\t| 62\n| WS 373.2-2012 \t| \u533b\u7597\u670d\u52a1\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u4f4f\u9662\u6458\u8981\t\t| 72\n| WS 373.3-2012\t   | \u533b\u7597\u670d\u52a1\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u6210\u4eba\u5065\u5eb7\u4f53\u68c0\t\t| 182\n| WS 374.1-2012 \t\t| \u536b\u751f\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u536b\u751f\u76d1\u7763\u68c0\u67e5\u4e0e\u884c\u653f\u5904\u7f5a\t\t| 62\n| WS 374.2-2012 \t\t| \u536b\u751f\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u536b\u751f\u76d1\u7763\u884c\u653f\u8bb8\u53ef\u4e0e\u767b\u8bb0\t\t|92\n| WS 374.3-2012 \t\t| \u536b\u751f\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u536b\u751f\u76d1\u7763\u76d1\u6d4b\u4e0e\u8bc4\u4ef7\t\t|22\n| WS 374.4-2012\u00a0\t\t| \u536b\u751f\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\uff1a\u536b\u751f\u76d1\u7763\u673a\u6784\u4e0e\u4eba\u5458\t\t|105\n\n\n\n\n| \u6807\u51c6\u7f16\u53f7 |   \u6807\u51c6\u4e2d\u6587\u540d\u79f0 | \u6570\u636e\u5143\u6570\u76ee | \u6027\u8d28 | \n| ----  | ---- |  ---- | \u5f3a\u5236\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6|     \n| WS 445.1-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206\uff1a\u75c5\u5386\u6982\u8981  \t\t|  xx               \n| WS 445.2-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206\uff1a\u95e8\uff08\u6025\uff09\u8bca\u75c5\u5386  \t\t|  xx               \n| WS 445.3-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c3\u90e8\u5206\uff1a\u95e8\uff08\u6025\uff09\u8bca\u5904\u65b9  \t\t|  xx               \n| WS 445.4-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206\uff1a\u68c0\u67e5\u68c0\u9a8c\u8bb0\u5f55  \t\t|  xx               \n| WS 445.5-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c5\u90e8\u5206\uff1a\u4e00\u822c\u6cbb\u7597\u5904\u7f6e\u8bb0\u5f55  \t\t|  xx               \n| WS 445.6-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206\uff1a\u52a9\u4ea7\u8bb0\u5f55  \t\t|  xx               \n| WS 445.7-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c7\u90e8\u5206\uff1a\u62a4\u7406\u64cd\u4f5c\u8bb0\u5f55  \t\t|  xx               \n| WS 445.8-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c8\u90e8\u5206\uff1a\u62a4\u7406\u8bc4\u4f30\u4e0e\u8ba1\u5212  \t\t|  xx               \n| WS 445.9-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c9\u90e8\u5206\uff1a\u77e5\u60c5\u544a\u77e5\u4fe1\u606f  \t\t|  xx               \n| WS 445.10-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c10\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u6848\u9996\u9875  \t\t|  xx               \n| WS 445.11-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c11\u90e8\u5206\uff1a\u4e2d\u533b\u4f4f\u9662\u75c5\u6848\u9996\u9875  \t\t|  xx               \n| WS 445.12-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c12\u90e8\u5206\uff1a\u5165\u9662\u8bb0\u5f55  \t\t|  xx               \n| WS 445.13-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c13\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \t\t|  xx               \n| WS 445.14-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c14\u90e8\u5206\uff1a\u4f4f\u9662\u533b\u5631  \t\t|  xx               \n| WS 445.15-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c15\u90e8\u5206\uff1a\u51fa\u9662\u5c0f\u7ed3  \t\t|  xx               \n| WS 445.16-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c16\u90e8\u5206\uff1a\u8f6c\u8bca(\u9662)\u8bb0\u5f55  \t\t|  xx               \n| WS 445.17-2014 \t\t| \u7535\u5b50\u75c5\u5386\u57fa\u672c\u6570\u636e\u96c6 \u7b2c17\u90e8\u5206\uff1a\u533b\u7597\u673a\u6784\u4fe1\u606f  \t\t|  xx               \n\n\n\n### \u5171\u4eab\u6587\u6863\n\n## \u6280\u672f\u7c7b\n\n### \u529f\u80fd\u89c4\u8303\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS/T 452-2014  | \u536b\u751f\u76d1\u7763\u4fe1\u606f\u7cfb\u7edf\u529f\u80fd\u89c4\u8303 | \n| WS/T xxx-2013  | \u5987\u5e7c\u4fdd\u5065\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \n| WS/T xxx-2013  | \u57fa\u5c42\u533b\u7597\u536b\u751f\u4fe1\u606f\u7cfb\u7edf\u529f\u80fd\u89c4\u8303 | \n| WS/T 451-2014  | \u9662\u524d\u533b\u7597\u6025\u6551\u6307\u6325\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \n| WS/T 450-2014  | \u65b0\u578b\u519c\u6751\u5408\u4f5c\u533b\u7597\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \n| WS/T 449-2014  | \u6162\u6027\u75c5\u76d1\u6d4b\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \n| WS/T 529-2016  | \u8fdc\u7a0b\u533b\u7597\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |\n| WS/T xxx-2013  | \u533b\u9662\u611f\u67d3\u7ba1\u7406\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303 | \n| WS/T xxx-2013  | \u514d\u75ab\u89c4\u5212\u4fe1\u606f\u7cfb\u7edf\u57fa\u672c\u529f\u80fd\u89c4\u8303(\u5f81\u6c42\u610f\u89c1\u7a3f) | \n\n### \u6280\u672f\u89c4\u8303\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS/T 448-2014  | \u57fa\u4e8e\u5065\u5eb7\u6863\u6848\u7684\u533a\u57df\u536b\u751f\u4fe1\u606f\u5e73\u53f0\u6280\u672f\u89c4\u8303 | \n| WS/T 447-2014  | \u57fa\u4e8e\u7535\u5b50\u75c5\u5386\u7684\u533b\u9662\u4fe1\u606f\u5e73\u53f0\u6280\u672f\u89c4\u8303 | \n| WS/T xxx-2013  | \u5c45\u6c11\u5065\u5eb7\u5361\u6280\u672f\u89c4\u8303 | \n| WS/T xxx-2013  | \u8fdc\u7a0b\u533b\u7597\u4fe1\u606f\u7cfb\u7edf\u6280\u672f\u89c4\u8303 | \n| WS/T 526-2016  | \u5987\u5e7c\u4fdd\u5065\u670d\u52a1\u4fe1\u606f\u7cfb\u7edf\u6280\u672f\u89c4\u8303 | \n| WS/T xxx-2013  | \u533b\u5b66\u6570\u5b57\u5f71\u50cf\u4e2d\u6587\u5c01\u88c5\u4e0e\u901a\u4fe1\u89c4\u5219 | \n| WS/T xxx-2013  | \u533a\u57df\u75be\u75c5\u63a7\u5236\u4e1a\u52a1\u5e94\u7528\u5b50\u5e73\u53f0\u6280\u672f\u89c4\u8303 | \n| WS/T 517-2016  | \u57fa\u5c42\u533b\u7597\u536b\u751f\u4fe1\u606f\u7cfb\u7edf\u6280\u672f\u89c4\u8303 | \n| WS/T xxx-2013  | \u8fdc\u7a0b\u533b\u7597\u8bbe\u5907\u53ca\u7edf\u4e00\u901a\u8baf\u4ea4\u4e92\u89c4\u8303(\u5f81\u6c42\u610f\u89c1\u7a3f) | \n| WS/T xxx-2013  | \u533a\u57df\u536b\u751f\u4fe1\u606f\u5e73\u53f0\u4ea4\u4e92\u89c4\u8303(\u5f81\u6c42\u610f\u89c1\u7a3f) | \n| WS/T xxx-2013  | \u533b\u9662\u4fe1\u606f\u5e73\u53f0\u4ea4\u4e92\u89c4\u8303(\u5f81\u6c42\u610f\u89c1\u7a3f) | \n\n### \u5b89\u5168\u4e0e\u9690\u79c1\n\n### \u4f20\u8f93\u4e0e\u4ea4\u6362\n\n| \u7f16\u53f7  | \u540d\u79f0 |  \u6027\u8d28 | \n| ----  | ---- |  ---- | \n| WS/T 483.1-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c1\u90e8\u5206\uff1a\u4e2a\u4eba\u57fa\u672c\u5065\u5eb7\u4fe1\u606f\u767b\u8bb0 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.2-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c2\u90e8\u5206\uff1a\u51fa\u751f\u533b\u5b66\u8bc1\u660e | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.3-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c3\u90e8\u5206\uff1a\u65b0\u751f\u513f\u5bb6\u5ead\u8bbf\u89c6 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.4-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c4\u90e8\u5206\uff1a\u513f\u7ae5\u5065\u5eb7\u4f53\u68c0 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.5-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c5\u90e8\u5206\uff1a\u9996\u6b21\u4ea7\u524d\u968f\u8bbf\u670d\u52a1 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.6-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c6\u90e8\u5206\uff1a\u4ea7\u524d\u968f\u8bbf\u670d\u52a1 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.7-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c7\u90e8\u5206\uff1a\u4ea7\u540e\u8bbf\u89c6 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.8-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c8\u90e8\u5206\uff1a\u4ea7\u540e42\u5929\u5065\u5eb7\u68c0\u67e5 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.9-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c9\u90e8\u5206\uff1a\u9884\u9632\u63a5\u79cd\u62a5\u544a | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |  \n| WS/T 483.10-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c10\u90e8\u5206\uff1a\u4f20\u67d3\u75c5\u62a5\u544a | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.11-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c11\u90e8\u5206\uff1a\u6b7b\u4ea1\u533b\u5b66\u8bc1\u660e | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.12-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c12\u90e8\u5206\uff1a\u9ad8\u8840\u538b\u60a3\u8005\u968f\u8bbf\u670d\u52a1 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.13-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c13\u90e8\u5206\uff1a2\u578b\u7cd6\u5c3f\u75c5\u60a3\u8005\u968f\u8bbf\u670d\u52a1 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.14-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c14\u90e8\u5206\uff1a\u91cd\u6027\u7cbe\u795e\u75be\u75c5\u60a3\u8005\u4e2a\u4eba\u4fe1\u606f\u767b\u8bb0 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.15-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c15\u90e8\u5206\uff1a\u91cd\u6027\u7cbe\u795e\u75be\u75c5\u60a3\u8005\u968f\u8bbf\u670d\u52a1 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.16-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c16\u90e8\u5206\uff1a\u6210\u4eba\u5065\u5eb7\u4f53\u68c0 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.17-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c17\u90e8\u5206\uff1a\u95e8\u8bca\u6458\u8981 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.18-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c18\u90e8\u5206\uff1a\u4f4f\u9662\u6458\u8981 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.19-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c19\u90e8\u5206\uff1a\u4f1a\u8bca\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 483.20-2016  | \u5065\u5eb7\u6863\u6848\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c20\u90e8\u5206\uff1a\u8f6c\u8bca(\u9662)\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n\n| \u7f16\u53f7  | \u540d\u79f0 |  \u6027\u8d28 | \n| ----  | ---- |  ---- | \n| WS/T 500.1-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c1\u90e8\u5206\uff1a\u75c5\u5386\u6982\u8981 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.2-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c2\u90e8\u5206\uff1a\u95e8(\u6025)\u8bca\u75c5\u5386 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.3-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c3\u90e8\u5206\uff1a\u6025\u8bca\u7559\u89c2\u75c5\u5386 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.4-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c4\u90e8\u5206\uff1a\u897f\u836f\u5904\u65b9 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.5-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c5\u90e8\u5206\uff1a\u4e2d\u836f\u5904\u65b9 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.6-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c6\u90e8\u5206\uff1a\u68c0\u67e5\u62a5\u544a | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.7-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c7\u90e8\u5206\uff1a\u68c0\u9a8c\u62a5\u544a | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.8-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c8\u90e8\u5206\uff1a\u6cbb\u7597\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.9-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c9\u90e8\u5206\uff1a\u4e00\u822c\u624b\u672f\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.10-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c10\u90e8\u5206\uff1a\u9ebb\u9189\u672f\u524d\u8bbf\u89c6\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.11-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c11\u90e8\u5206\uff1a\u9ebb\u9189\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.12-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c12\u90e8\u5206\uff1a\u9ebb\u9189\u672f\u540e\u8bbf\u89c6\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.13-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c13\u90e8\u5206\uff1a\u8f93\u8840\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.14-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c14\u90e8\u5206\uff1a\u5f85\u4ea7\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.15-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c15\u90e8\u5206\uff1a\u9634\u9053\u5206\u5a29\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.16-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c16\u90e8\u5206\uff1a\u5256\u5bab\u4ea7\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.17-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c17\u90e8\u5206\uff1a\u4e00\u822c\u62a4\u7406\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.18-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c18\u90e8\u5206\uff1a\u75c5\u91cd\uff08\u75c5\u5371\uff09\u62a4\u7406\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.19-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c19\u90e8\u5206\uff1a\u624b\u672f\u62a4\u7406\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.20-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c20\u90e8\u5206\uff1a\u751f\u547d\u4f53\u5f81\u6d4b\u91cf\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.21-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c21\u90e8\u5206\uff1a\u51fa\u5165\u91cf\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.22-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c22\u90e8\u5206\uff1a\u9ad8\u503c\u8017\u6750\u4f7f\u7528\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.23-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c23\u90e8\u5206\uff1a\u5165\u9662\u8bc4\u4f30 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.24-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c24\u90e8\u5206\uff1a\u62a4\u7406\u8ba1\u5212 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.25-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c25\u90e8\u5206\uff1a\u51fa\u9662\u8bc4\u4f30\u4e0e\u6307\u5bfc | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.26-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c26\u90e8\u5206\uff1a\u624b\u672f\u77e5\u60c5\u540c\u610f\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.27-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c27\u90e8\u5206\uff1a\u9ebb\u9189\u77e5\u60c5\u540c\u610f\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.28-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c28\u90e8\u5206\uff1a\u8f93\u8840\u6cbb\u7597\u540c\u610f\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.29-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c29\u90e8\u5206\uff1a\u7279\u6b8a\u68c0\u67e5\u53ca\u7279\u6b8a\u6cbb\u7597\u540c\u610f\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.30-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c30\u90e8\u5206\uff1a\u75c5\u5371(\u91cd)\u901a\u77e5\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.31-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c31\u90e8\u5206\uff1a\u5176\u4ed6\u77e5\u60c5\u544a\u77e5\u540c\u610f\u4e66 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.32-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c32\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u6848\u9996\u9875 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.33-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c33\u90e8\u5206\uff1a\u4e2d\u533b\u4f4f\u9662\u75c5\u6848\u9996\u9875 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.34-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c34\u90e8\u5206\uff1a\u5165\u9662\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.35-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c35\u90e8\u5206\uff1a24\u5c0f\u65f6\u5185\u5165\u51fa\u9662\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.36-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c36\u90e8\u5206\uff1a24\u5c0f\u65f6\u5185\u5165\u9662\u6b7b\u4ea1\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.37-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c37\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \u9996\u6b21\u75c5\u7a0b\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.38-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c38\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \u65e5\u5e38\u75c5\u7a0b\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.39-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c39\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \u4e0a\u7ea7\u533b\u5e08\u67e5\u623f\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.40-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c40\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \u7591\u96be\u75c5\u4f8b\u8ba8\u8bba\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.41-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c41\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55  \u4ea4\u63a5\u73ed\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.42-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c42\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u8f6c\u79d1\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.43-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c43\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u9636\u6bb5\u5c0f\u7ed3 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.44-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c44\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u62a2\u6551\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.45-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c45\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u4f1a\u8bca\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.46-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c46\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u672f\u524d\u5c0f\u7ed3 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |           \n| WS/T 500.47-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c47\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u672f\u524d\u8ba8\u8bba | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.48-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c48\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55\u672f\u540e \u9996\u6b21\u75c5\u7a0b\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.49-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c49\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u51fa\u9662\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.50-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c50\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u6b7b\u4ea1\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.51-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c51\u90e8\u5206\uff1a\u4f4f\u9662\u75c5\u7a0b\u8bb0\u5f55 \u6b7b\u4ea1\u75c5\u4f8b\u8ba8\u8bba\u8bb0\u5f55 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.52-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c52\u90e8\u5206\uff1a\u4f4f\u9662\u533b\u5631 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n| WS/T 500.53-2016  | \u7535\u5b50\u75c5\u5386\u5171\u4eab\u6587\u6863\u89c4\u8303 \u7b2c53\u90e8\u5206\uff1a\u51fa\u9662\u5c0f\u7ed3 | \u63a8\u8350\u6027\u536b\u751f\u884c\u4e1a\u6807\u51c6 |            \n          \n\n\n## \u7ba1\u7406\u7c7b\n\n### \u5efa\u8bbe\u6307\u5357\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS/T xxx-2009  | \u7efc\u5408\u536b\u751f\u7ba1\u7406\u4fe1\u606f\u5e73\u53f0\u5efa\u8bbe\u6307\u5357\uff08\u5f81\u6c42\u610f\u89c1\u7a3f\uff09 | \n\n\n### \u6d4b\u8bd5\u4e0e\u8bc4\u4ef7\n\n| \u7f16\u53f7  | \u540d\u79f0 | \n| ----  | ---- | \n| WS/T 502-2016  | \u7535\u5b50\u5065\u5eb7\u6863\u6848\u4e0e\u533a\u57df\u536b\u751f\u4fe1\u606f\u5e73\u53f0\u6807\u51c6\u7b26\u5408\u6027\u6d4b\u8bd5\u89c4\u8303 | \n| WS/T 501-2016  | \u7535\u5b50\u75c5\u5386\u4e0e\u533b\u9662\u4fe1\u606f\u5e73\u53f0\u6807\u51c6\u7b26\u5408\u6027\u6d4b\u8bd5\u89c4\u8303 | \n\n### \u8fd0\u7ef4\u7ba1\u7406\n\n### \u76d1\u7406\u4e0e\u9a8c\u6536\n\n## \u53c2\u8003          \n\n1\u3001[\u56fd\u5bb6\u536b\u751f\u4fe1\u606f\u6807\u51c6\u4e0e\u5b9e\u65bd\u8bc4\u4ef7 \u6c64\u5b66\u519b CHINC2015]()\n2\u3001[\u4e2d\u56fd\u536b\u751f\u4fe1\u606f\u6807\u51c6\u7f51](http://www.chiss.org.cn/hism/wcmpub/hism1029/index/)\n## \u66f4\u65b0\u65e5\u5fd7\n\n2017-03-07\u65e5\u66f4\u65b0\n1.\u5bf9\u4e8e README.md \u4e2d\u7684\u6807\u51c6\u7f16\u53f7\u8fdb\u884c\u4e86\u66f4\u65b0          \n\u53c2\u8003\u56fd\u536b\u901a[2016]12\u53f7    \n\n\n2014-11-13\u65e5\u66f4\u65b0\n1.\u5bf9\u4e8e \"\u6570\u636e\u96c6\u6807\u51c6/Excel/WS365\u57ce\u4e61\u5c45\u6c11\u5065\u5eb7\u6863\u6848\u57fa\u672c\u6570\u636e\u96c6.xlsx\"      \n* 5.2.03\u65b0\u751f\u513f\u5bb6\u5ead\u8bbf\u89c6\u4fe1\u606f\u7b2c56\u884c HDSD00.01,261 \u4fee\u6539\u4e3a 5.2.03\u65b0\u751f\u513f\u5bb6\u5ead\u8bbf\u89c6\u4fe1\u606f\u7b2c56\u884c HDSD00.01.261(\u9017\u53f7\u6539\u4e3a\u70b9)       \n* 5.2.01\u4e2a\u4eba\u57fa\u672c\u4fe1\u606f \u7b2c\u4e8c\u884c\u4e3a\u7a7a\u884c \u79fb\u9664       \n* 5.2.04\u513f\u7ae5\u5065\u5eb7\u68c0\u67e5\u4fe1\u606f \u7b2c\u4e00\u5217\u5220\u9664 \u4e0e\u5176\u4ed6\u8868\u5355\u4fdd\u6301\u683c\u5f0f\u4e00\u81f4        \n* 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c23\u884c Hr)SD00.01.379  \u4fee\u6539\u4e3a 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c23\u884c HDSD00.01.379                 \n* 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c33\u884c HDSD00. 01.407 \u4fee\u6539\u4e3a 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c33\u884c HDSD00.01.407(00.01.407 \u539f\u676501\u524d\u9762\u6709\u7a7a\u683c)     \n* 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c38\u884c HDSD00. 01.412 \u4fee\u6539\u4e3a 5.2.09\u4f20\u67d3\u75c5\u62a5\u544a\u5361\u4fe1\u606f\u7b2c38\u884c HDSD00.01.412 (00.01.412\u539f\u676501\u524d\u9762\u6709\u7a7a\u683c)      \n* 5.2.18\u8f6c\u8bca(\u9662)\u4fe1\u606f\u7b2c49\u884c HDSD00,01.571 \u4fee\u6539\u4e3a 5.2.18\u8f6c\u8bca(\u9662)\u4fe1\u606f\u7b2c49\u884c HDSD00.01.571(\u9017\u53f7\u6539\u4e3a\u70b9)      \n2.\u5bf9\u4e8e\"WS369-375\u536b\u751f\u4fe1\u606f\u57fa\u672c\u6570\u636e\u96c6.xlsx\"      \n* \u513f\u7ae5\u4fdd\u5065\u57fa\u672c\u6570\u636e\u96c6 \u7b2c2\u90e8\u5206 \u513f\u7ae5\u5065\u5eb7\u4f53\u68c0\u7b2c62\u884c 29  \u5220\u9664\u7b2c62\u884c                \n* \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206 \u4e59\u809d\u60a3\u8005\u7ba1\u7406\u7b2c37\u884c HDSB04,01.037 \u4fee\u6539\u4e3a \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206 \u4e59\u809d\u60a3\u8005\u7ba1\u7406\u7b2c37\u884c HDSB04.01.037(\u9017\u53f7\u6539\u4e3a\u70b9)     \n* \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c1\u90e8\u5206 \u4e59\u809d\u60a3\u8005\u7ba1\u7406\u7b2c97\u884c  HDSB04,01.097  \u4fee\u6539\u4e3a   HDSB04.01.097  (\u9017\u53f7\u6539\u4e3a\u70b9)       \n* \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c4\u90e8\u5206 \u8001\u5e74\u4eba\u5065\u5eb7\u7ba1\u7406\u7b2c90\u884c  HDSB04,04.090 \u4fee\u6539\u4e3a  HDSB04.04.090       \n* \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206 \u80bf\u7624\u75c5\u4f8b\u7ba1\u7406\u7b2c30\u884c  HDSB04,06.030 \u4fee\u6539\u4e3a  HDSB04.06.030     \n* \u75be\u75c5\u7ba1\u7406\u57fa\u672c\u6570\u636e\u96c6 \u7b2c6\u90e8\u5206 \u80bf\u7624\u75c5\u4f8b\u7ba1\u7406\u7b2c61\u884c  HDSB04.06.06] \u4fee\u6539\u4e3a  HDSB04.06.061(\u62ff\u6389] )       \n3.\u5bf9\u4e8e\"\u6570\u636e\u5143\u6807\u51c6/Excel/\u536b\u751f\u4fe1\u606f\u6570\u636e\u5143\u76ee\u5f55.xlsx\"          \n*                   \n*\n*\n*\n\n\n\n\n"
 },
 {
  "repo": "abhishek-ch/streamlit-healthcare-ML-App",
  "language": "Python",
  "readme_contents": "\n![](https://assets.website-files.com/5dc3b47ddc6c0c2a1af74ad0/5e0a328bedb754beb8a973f9_logomark_website.png)\n\n# Streamlit Healthcare Machine Learning Data App\n\n![](extra/StreamlitML.gif)\n\n## Objective\n1. How easy is it to create a Web App using Streamlit\n2. Integrating multiple #machinelearning technologies in the same app\n3. Code reusability\n4. Streamlit functions & feature usage\n\n>  of-course Dockerize!\n\n## Running the App\n\n1. Checkout the code\n ```\n git checkout\n``` \n2. Build the docker image\n```buildoutcfg\ndocker build --tag streamlit-healthcare:1.0 .\n```\n3. Run the docker image\n```buildoutcfg\ndocker run -it -p 8501:8501 streamlit-healthcare:1.0\n```\n4. Browse the [url](http://localhost:8501)\n\n## Features\n* Load Healthcare data from Kaggle https://www.kaggle.com/sulianova/cardiovascular-disease-dataset\n* Use __scikit-learn__ ML lib to run classification.\n* Provide Tuning param options in the UI \n* Provide Switch to enable __PySpark__\n* Provide Pyspark MLlib options over the same data, technically one can compare \nthe result between 2 seperate libraries\n* Plotting using Seaborn chart\n\n## Conclusion\n\nStreamlit is essentially a very straightforward easy library to create\npython based Webapp. \nI am Convinced \ud83d\udc4f\ud83d\udc4f\ud83d\udc4f"
 },
 {
  "repo": "GoogleCloudPlatform/healthcare-api-dicomweb-cli",
  "language": "Python",
  "readme_contents": "# DICOMweb command line tool\nDICOMweb command line tool is a command line utility for interacting with DICOMweb servers.\n\n## Requirements\n\n- python (3.5+)\n- pip\n\n## Installation\n\n### Using GitHub:\n\n```bash\npip install https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/archive/v1.0.2.zip\n```\n\nNOTE: Getting errors due to not having Python3? See [instructions below](#running-on-machine-with-python2).\n\n## Interface\n\n### dcmweb [-m] \\<host> \\<store|retrieve|search|delete> [parameters]\n\n* **-m**\n\\\n Whether to perform batch operations in parallel or sequentially, default is sequentially.\n\n* **host**\n\\\n The full DICOMweb endpoint URL. E.g. `https://healthcare.googleapis.com/v1/projects/<project_id>/locations/<location_id>/datasets/<dataset_id>/dicomStores/<dicom_store_id>/dicomWeb`\n\n* **store**\n\\\n Stores one or more files by posting multiple StoreInstances requests. Requests will be sent in sequence or in parallel based on the -m flag.\n \n \t* --masks \\*string\n\t\\\n\tPositional argument, contains list of file paths or masks to upload, mask support wildcard(\\*) and cross directory boundaries wildcard(\\*\\*) char, \n\n\n* **retrieve**\n\\\n Retrieves one or more studies, series, instances, or frames from the server. Outputs the instances to the directory specified by the --output option.\n\n\t* --path string\n\t\\\n\tPositional argument, can either be empty (indicates downloading of all studies) or specify a resource path (studies/<uid>[/series/<uid>[/instances/<uid>[/frames/<frame_num]]]) to download from the server\n\n\t* --type string\n\t\\\n\tControls what format to request the files in (defaults to application/dicom; transfer-syntax=*). The tool will use this as the part content type in the multipart accept header being sent to the server. \n\n\t* --output string\n\t\\\n\tControls where to write the files to (defaults to current directory).\n\tThe following folder structure will be created:\n\t\\\n\t\t```\n\t\t- study_uid\n\t\t\t- series_uid\n\t\t\t\t- instance_uid[_frame_X].<ext>\n\t\t```\n\n\n\n* **search**\n\\\nPerforms a search over studies, series, or instances and outputs the result to stdout, limited to 5000 items by default. You can specify limit/offset parameters to change this.\n\n    * --path string\n\t\\\n\tPositional argument, specifies a path (studies/[<uid>/series/[<uid>/instances/]]) to search on the server, default is \"/studies\"\n\n    * --parameters string\n\t\\\n\tQIDO search parameters formatted as URL query parameters.\n\n* **delete**\n\\\n Deletes the given study, series, or instance from the server. Uses an un-standardized extension to the DICOMweb spec.\n\n    * --path string\n    \\\n\tPositional argument, specifies a resource path (studies/<uid>[/series/<uid>[/instances/<uid>[/frames/<frame_num]]]) to delete from the server\n\n## Examples\n\n**search**\n\n```bash\n# will return json list of instances in dicomstore with date==1994.10.13\ndcmweb $host search instances StudyDate=19941013 \n```\n\n```bash\n# will return list of studies without any filter\ndcmweb $host search \n```\n\nSince search returns JSON data it can be redirected into parse tools like [jq](https://stedolan.github.io/jq/).\n\n```bash\n# will parse StudyUIDs/PatientNames for each study in search results\ndcmweb $host search | jq '.[] | .[\"0020000D\"].Value[0],.[\"00100010\"].Value[0]'\n```\n\nOutput of jq may be redirected as well:\n```bash\n# will parse StudyUIDs for each study in search results\n# and count lines of jq output by wc\ndcmweb $host search | jq '.[] | .[\"0020000D\"].Value[0]' | wc -l\n```\nThe list of DICOM tags can be found in this [page](https://dicom.innolitics.com/ciods/).\n\n**store**\n\n```bash\n# will upload list of files generated from current folder by shell\ndcmweb $host store ./* \n```\n\n```bash\n# will upload list of files generated from current folder by python\ndcmweb $host store \"./*\" \n```\n\n```bash\n# will upload list of files generated from current folder recursively by python\ndcmweb $host store \"./**\" \n```\n\n```bash\n# will upload list of files in parallel\ndcmweb -m $host store \"./**\" \n```\n**retrieve**\n\n```bash\n# will download all instances from dicomstore into current folder\ndcmweb $host retrieve \n```\n\n```bash\n# will download all instances from dicomstore into current folder in parallel\ndcmweb -m $host retrieve \n```\n\n```bash\n# will download all instances from dicomstore into ./data folder\ndcmweb $host retrieve --output ./data \n```\n\n```bash\n# will download all instances from dicomstore into ./data folder as png images,\n# in instance is multiframe, frames will be saved as separate files\ndcmweb $host retrieve --output ./data --type \"image/png\" \n```\n\n```bash\n# will download all instances from study 1 into ./data folder\ndcmweb $host retrieve studies/1 --output ./data \n```\n\n**delete**\n\n```bash\n# will delete study 1\ndcmweb $host delete studies/1\n```\n\n## Build\n\n```bash\npython ./setup.py sdist bdist_wheel \n```\n## Run tests\n\n```bash\npip install tox\ntox\n```\n\n## Developing\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md)\n\n## Running on machine with Python2\n\nThis tool requires Python3 to be run which may cause issues for environments that\nhave Python2 installed and can't upgrade due to other dependencies. Here are 2 options\nfor getting the tool to work in this enviornment:\n\n### Using pip local install\n\n1.  Install python3 & python3-pip (e.g. `sudo apt install python3 python3-pip`)\n1.  Download zip file from latest release from\n    [GitHub](https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/releases)\n1.  Run `python3 -m pip install healthcare-api-dicomweb-cli-X.Y.zip --user`\n1.  Run `export PATH=$PATH:\"${HOME}/.local/bin\"` -- this is where pip --user\n    installs things to\n1.  Then you should be able to run `dcmweb`\n\n### Using virtualenv\n\n1.  Install python3 & python3-venv (e.g. `sudo apt install python3 python3-venv`)\n1.  Start a virtualenv `python3 -m venv py3-env && cd py3-env && source ./bin/activate`\n1.  Download zip file from latest release from\n    [GitHub](https://github.com/GoogleCloudPlatform/healthcare-api-dicomweb-cli/releases)\n1.  Install within virtualenv`pip install healthcare-api-dicomweb-cli-X.Y.zip`\n1.  Then you should be able to run `dcmweb`\n\n## Apache License 2.0\nProject License can be found [here](LICENSE).\n\n"
 },
 {
  "repo": "techshot25/HealthCare",
  "language": "Jupyter Notebook",
  "readme_contents": "# HealthCare\n\nThis includes a dataset representing insurance costs for individuals. It uses information such as age, sex, bmi, and smoking habits to determine the cost on the insurance company for that person.\n\nThe idea is to present a machine learning model that predicts the cost with the highest accuracy.\n\nDocumentation and code provided in the file: `HealthCare.ipynb`. If GitHub cannot render the file [click here](https://nbviewer.jupyter.org/github/techshot25/HealthCare/blob/master/HealthCare.ipynb).\n"
 },
 {
  "repo": "simpledotorg/simple-server",
  "language": "Ruby",
  "readme_contents": "# Simple Server\n\n[![Build Status](https://simple.semaphoreci.com/badges/simple-server/branches/master.svg)](https://simple.semaphoreci.com/projects/simple-server)\n[![Ruby Style Guide](https://img.shields.io/badge/code_style-standard-brightgreen.svg)](https://github.com/testdouble/standard)\n\nThis is the backend for the Simple app to help track hypertensive patients across a population.\n\n## Development\n\nWe have a `bin/setup` script that does most of the work of getting things setup, but you need a few things in place first.\nIf you are on a Mac, install [homebrew](https://brew.sh) and then install rbenv, redis, postgres@14, and yarn:\n\n```\nbrew install rbenv ruby-build redis yarn postgresql@14\n```\n\nAfter this is done, it is highly recommended to tune your local PostgreSQL installation, otherwise your server will get bogged down when doing things like refreshing materialized views.\nYou can use [PGTune](https://pgtune.leopard.in.ua) to do this, it takes about 2 minutes. You can find your local postgresql.conf file at `/opt/homebrew/var/postgresql@14/postgresql.conf` on M1 Macs,\nand `/usr/local/var/postgresql@14/postgresql.conf` on Intel Macs.\n\n### bin/setup\n\nTo set up the Simple server for local development, clone the git repository and run the setup script included:\n\n```\n$ git clone git@github.com:simpledotorg/simple-server.git\n$ cd simple-server\n$ bin/setup\n```\n\nNote: If you already have a previous dev environment you're trying to refresh, it's easiest to drop your database run setup again.\n```\n$ rails db:drop\n$ rails parallel:drop\n$ bin/setup\n```\n\nIf you encounter issues with this script, please open [a new issue with details](https://github.com/simpledotorg/simple-server/issues/new?title=Problems+with+bin/setup). Please include the entire log from bin/setup, as well as your computer / OS details.\n\n### Setup Troubleshooting\n\n#### Setup fails when bundler tries to resolve `nokogiri`\n\nThe error message will look like:\n```\nExtracting libxml2-2.9.13.tar.xz into <...>\n========================================================================\ntar (child): xz: Cannot exec: No such file or directory\ntar (child): Error is not recoverable: exiting now\n/bin/tar: Child returned status 2\n/bin/tar: Error is not recoverable: exiting now\n========================================================================\n```\n\nEnsure that you have `xz` installed and *linked* with `brew link xz`.\n\n#### Apple Silicon M1 Macs\n\nWith recent gem updates, all of our gems and dependencies now build ARM native on m1 macs. This means you do **not** need to use Rosetta to set up simple-server, and in fact using Rosetta will make things more complicated and confusing in day to day dev experience, and also hurts performance.\n\nThere is one possible caveat to this -- if you see any problems with google-protobuf, run the following:\n\n```\ngem uninstall google-protobuf\ngem install google-protobuf -v 3.21.5 --platform=ruby\n```\n\nThen rerun bundler and everything will work. This is being tracked over in https://github.com/protocolbuffers/protobuf/issues/8682, hopefully there will be a better fix soon.\n\nBeyond that, the setup instructions are now the same for Intel or M1 macs, as you can install homebrew normally and go from there.\n\n#### Docker Compose\nDev environment setup using docker and docker-compose\n\n##### Prerequisite\n- [Docker](https://docs.docker.com/engine/install/)\n- [Docker compose](https://docs.docker.com/compose/install/)\n\n###### Install docker and docker-compose on Mac\n```\nbrew install docker\nbrew install docker-compose\n```\n\n##### Setup\n```\nbin/docker-up\n```\n\nAfter a successful docker-compose initialisation, an admin dashboard account is automatically created.\n```\nusername: admin@simple.org\npassword: Resolve2SaveLives\n```\n\nOpen http://localhost:3000 in your browser to view the simple dashboard\n\nUse below Ngrok [guide](#developing-with-the-android-app) for Android development setup\n\n##### Teardown (delete docker containers and volumes)\n```\nbin/docker-down\n```\n\n#### Manual Setup\n\nIf the included `bin/setup` script fails for some reason, you can also manually\nset up the application step by step. You can do so as follows.\n\nFirst, you need to [install ruby](https://www.ruby-lang.org/en/documentation/installation). It is recommended to use [rbenv](https://github.com/rbenv/rbenv) to manage ruby versions. Note that we currently use Bundler version 2.3.22, so that is also hardcoded below.\n\nNext, [install NodeJS v18.11.0](https://nodejs.org/en/) using [nvm](https://github.com/nvm-sh/nvm).\n\n```bash\ngem install bundler -v 2.3.22\nbundle _2.3.22_ install\nbrew install nvm\nnvm install 18.11.0\nrake yarn:install\nrails db:setup\n```\n\nWe cleanup old migration files every once in a while and so running `db:migrate` would not work for the initial setup.\nWhen setting up a new database, `db:setup` will take care of everything (it runs `db:structure:load` under the hood).\n\n#### Developing with the Android app\n\nTo run [simple-android](https://github.com/simpledotorg/simple-android/) app with the server running locally, you can\nuse [ngrok](https://ngrok.com).\n\n```bash\nbrew install --cask ngrok\nrails server\nngrok http 3000\n```\n\nThe output of the ngrok command is HTTP and HTTPS URLs that can be used to access your local server. The HTTP URL cannot\nbe used since HTTP traffic will not be supported by the emulator. Configure the following places with the HTTPS URL.\n\nIn the `gradle.properties` file in the `simple-android` repository, set:\n```\nmanifestEndpoint=<HTTPS URL>/api/\nfallbackApiEndpoint=<HTTPS URL>/api/\n```\n\nIn the `.env.development.local` (you can create this file if it doesn't exist),\n```\nSIMPLE_SERVER_HOST=<URL>  # i.e. without https://\nSIMPLE_SERVER_HOST_PROTOCOL=https\n```\n\nAlternatively, you can make the change on the server side. In the server repo, open `app/views/api/manifests/show.json.jbuilder`. Change:\n```\njson.endpoint \"#{ENV[\"SIMPLE_SERVER_HOST_PROTOCOL\"]}://#{ENV[\"SIMPLE_SERVER_HOST\"]}/api/\"\n```\nto:\n```\njson.endpoint \"<HTTPS URL>/api/\"\n```\n\n#### Workers\n\nWe use [sidekiq](https://github.com/mperham/sidekiq) to run async tasks. To run them locally you need to start redis:\n\n```bash\nredis-server -v\n```\n\n### Testing Email\n\nWe use [Mailcatcher](https://mailcatcher.me/) for testing email in development. Please use the\nfollowing to set it up on your machine.\n\n_Note: Please don't add Mailcatcher to the `Gemfile`, as it causes conflicts._\n\n```bash\ngem install mailcatcher\nmailcatcher\n```\n\nNow you should be able to see test emails at http://localhost:1080\n\n### Testing Web Views\n\nWhen testing web views like the progress tab or help screens, you will need to authenticate yourself with specific\nrequest headers. You can run the following command to get a set of request headers for a user that you can attach to\nyour requests.\n\n```\n$ bundle exec rails get_user_credentials\n```\n\nThe command will output a set of request headers that you can attach to your requests using tools like\n[Postman](https://www.postman.com/) or [ModHeader](https://bewisse.com/modheader/).\n\n```\nAttach the following request headers to your requests:\nAuthorization: Bearer 9b54814d4b422ee37dad46e7ebee673c59eed088c264e479880cbe7fb5ac1ce7\nX-User-ID: 452b96c2-e0cf-49e7-ab73-c328acd3f1e5\nX-Facility-ID: dcda7d9d-48f9-47d2-b1cc-93d90c94386e\n```\n\nHere are two Simple App pages you can test on your browser:\n* \"Progress Tab\": `http://localhost:3000/api/v3/analytics/user_analytics.html`\n* \"Help Page\": `http://localhost:3000/api/v3/help.html`\n\n### Review Apps\n\nEvery pull request opened on the `simple-server` repo creates a [Heroku review app](https://devcenter.heroku.com/articles/github-integration-review-apps)\nwith the branch's code deployed to it. The review app is hosted at the URL [https://simple-review-pr-<PR number>.herokuapp.com](#).\nThis temporary environment can be used to test your changes in a production-like environment easily.\n\nIf you need to test your changes with a mobile app build as well, you can generate a mobile app build that points to\nyour review app. To do so:\n\n* Navigate to the GitHub Actions page on the `simple-server` repository\n* Select the \"Mobile Review App Build\" action\n* Trigger a \"workflow dispatch\" at the top of the screen. You can keep the branch as `master` (it doesn't matter) and\n  enter your PR number in the required input\n* Once the Action is complete, its page will contain the APK as an artifact.\n\n![trigger-mobile-review-app](https://user-images.githubusercontent.com/4241399/139230709-1604df1f-ad7d-4690-8bae-80d2a48cab37.gif)\n\n<img width=\"1557\" alt=\"Screen Shot 2021-10-28 at 3 11 44 PM\" src=\"https://user-images.githubusercontent.com/4241399/139230802-39a38e26-7a96-4e00-9599-c8f7ce48d62d.png\">\n\n\n#### Testing messages\n\nMessages sent through Twilio are currently fixed to specific countries. To override this setting, go to the [heroku console](https://dashboard.heroku.com/pipelines/30a12deb-f419-4dca-ad4a-6f26bf192e6f) and [add/update](https://devcenter.heroku.com/articles/config-vars#managing-config-vars) the `DEFAULT_COUNTRY` config variable on your review app to your desired country. The supported country codes are listed [here](https://github.com/simpledotorg/simple-server/blob/master/config/initializers/countries.rb).\n\n```\n# for US/Canada\nDEFAULT_COUNTRY = US\n\n# for UK\nDEFAULT_COUNTRY = UK\n```\n\nUpdating this config will automatically restart the review app and should allow one to receive messages in their appropriate ISD codes.\n\n### Configuration\n\nThe app uses a base development configuration using `.env.development`. To add or override any configurations during\nlocal development, create a `.env.development.local` file and add your necessary configurations there. If a\nconfiguration change is applicable to all dev environments, ensure that it is added to `.env.development` and checked\ninto the codebase.\n\n### Running the application locally\n\nForeman can be used to run the application locally. First, install foreman.\n\n```bash\n$ gem install foreman\n```\n\nThen, run the following command to start the Rails and Sidekiq together.\n\n```bash\n$ foreman start -f Procfile.dev\n```\n\n**Note:** Foreman will also execute the `whenever` gem in trial mode. This will validate that the `whenever`\nconfiguration is valid, but will not actually schedule any cron jobs.\n\nAlternatively, you can start these services locally _without_ foreman by using the following commands individually.\n\n* Rails: `bundle exec rails server` or `bundle exec puma`\n* Sidekiq: `bundle exec sidekiq`\n\n### Running the tests\n\n```bash\nbin/rspec\n```\n\nRun tests interactively quickly while developing:\n\n```\nbin/guard\n```\n\n### Code\n\nWe use the [standard](https://github.com/testdouble/standard#how-do-i-run-standard-in-my-editor) gem as our default formatter and linter. To enable it directly in your editor, follow [this](https://github.com/testdouble/standard#how-do-i-run-standard-in-my-editor).\n\nTo check all the offenses throughout the codebase:\n\n```bash\n$ bundle exec standardrb\n```\n\nTo fix any offenses that standard can autofix, run\n\n```bash\n$ bundle exec standardrb --fix\n```\n\n### Generating seed data\n\nNOTE: Its highly recommended to tune your local PostgreSQL before generating new seed data, especially large seed data sets. See the docs for that under [Development](#development).\nTo generate a full set of seed data, including facilities, users, patients with BPs, etc, run the following:\n\n```bash\nbin/rails db:seed\n```\n\nYou can always do a full reset to get back to a working dataset locally - note that reset clears all DBs, recreates them, runs seed, and refreshes matviews.\n\n```bash\nbin/rails db:reset\n```\n\nNeed a larger dataset? Try adding the `SEED_TYPE` ENV variable. Available sizes are `small`, `medium`, and `large`, and `profiling`. Large and profiling take a long time to run (20 mins to an hour), but they are very helpful for performance testing.\n\n```bash\nSEED_TYPE=medium bin/rails db:reset\n# You also may want an entirely new large dataset, with more facilities and regions, and more patients per facility.\nSEED_TYPE=large bin/rails db:reset\n```\n\nTo purge the generated patient data _only_, run the following. Note that you usually don't want this, and a full `db:reset` is safer in terms of generating a valid data set.\n\n```bash\n$ bin/rails db:purge_users_data\n```\n\n### Creating an admin user\n\nIf you need new admin users, you can run the following command from the project root. Note that the standard seed process already creates various admins for you, so you probably don't need this for typical dev.\n\n```bash\n$ bin/rails 'create_admin_user[<name>,<email>,<password>]'\n```\n\n### View Sandbox data in your local environment\n\nNOTE: generating seed data locally is the recommended way to get data in your env. Sandbox data is actually just generated via `db:seed`, so the below\nprocess really just adds SCP overhead to the process.\n\n1. Follow the steps in the \"How to add an SSH key...\" section [here](https://github.com/simpledotorg/deployment) to add your SSH key to the deployment repo\n2. Ask someone from the Simple team to add you as an admin to Sandbox\n3. Create a password for your Sandbox account and use that to log into the Sandbox dashboard on https://api-sandbox.simple.org\n4. Run `ssh deploy@ec2-13-235-33-14.ap-south-1.compute.amazonaws.com` to verify that your SSH access from step 1 was completed successfully.\n5. Run `bundle exec cap sandbox db:pull` to sync Sandbox data with your local machine.\n6. Use your Sandbox email and password to log into your local environment (http://localhost:3000).\n\n### Profiling\n\nWe use the [vegeta](https://github.com/tsenart/vegeta) utility to run performance benchmarks. The suite and additional instructions are [here](./profiling/README.md).\n\n### Security audits\n\nSecurity audits generally require some test data to be set up in a specific way, and account credentials and other\ninformation to be shared with the auditor. Run the following command to set up the necessary test data and print out\nan information sheet to be shared.\n\n```\n$ bin/rails 'prepare_security_environment'\n```\n\nThis task can only be executed in development and security environments.\n\n## Documentation\n\n### API\n\nAPI Documentation can be accessed at `/api-docs` on local server and hosted at https://api.simple.org/api-docs\n\nTo regenerate the Swagger API documentation, run the following command.\n\n```\n$ bundle exec rake docs\n```\n\n### ADRs\n\nArchitecture decisions are captured in ADR format and are available in `/doc/arch`\n\n### Wiki\n\nGuides, instructions and long-form maintenance documentation can go in `/doc/wiki`\n\n### ERD (Entity-Relationship Diagram)\n\nThese are not actively committed into the repository. But can be generated by running `bundle exec erd`\n\n## Deployment\n\nSimple Server is continuously deployed from master to all environments via [Semaphore Workflows](https://docs.semaphoreci.com/essentials/modeling-complex-workflows/) as long as the build passes. We use a mixture of tools under the hood for deployments:\n\n* Ansible: Server management and configuration is done using Ansible. See the [deployment repository](https://github.com/simpledotorg/deployment/tree/master/ansible)\n  for more information.\n* Capistrano: Application code is deployed to servers for a specific country and environment using Capistrano.\n* SemaphoreCI: Continuous deployment - all merges to master are auto-deployed to all environments.\n\nIf you need to make a manual production release, run the release script from master:\n\n```\nbin/release\n```\n\nThis will create a git release tag and automatically trigger a deployment to all environments through Semaphore. You can monitor the deployment progress [in Semaphore](https://simple.semaphoreci.com/projects/simple-server) via the tagged release's workflow. Please make sure to copy / paste the changelog from `bin/release` so you can post it in the #releases channel.\n\n### Deployment to a specific environment\n\n* We use Capistrano [multi-config](https://github.com/railsware/capistrano-multiconfig) to do multi-country deploys.\n* Most `cap` commands are namespaced with the country name. For eg: `bundle exec cap india:staging deploy` to deploy to India staging. Note that some (like sandbox) are do not have a country, so the command would be `bundle exec cap sandbox deploy`.\n* The available country names are listed under `config/deploy`. The subsequent envs, under the country directory, like\n  `config/deploy/india/staging.rb`\n\nSimple Server can be deployed to a specific environment and/or specific country via `bundle exec cap <country>:<enviroment> deploy`.\nNote that Sandbox does _not_ have a country prefix:\n\n```bash\n# Sandbox (deploys master)\nbundle exec cap sandbox deploy\n# Sandbox from a specific branch\nBRANCH=my-branch-name bundle exec cap sandbox deploy\n# Bangladesh demo\nbundle exec cap bangladesh:demo deploy\n```\n\nRake tasks can be run on the deployed server using Capistrano as well. For example,\n\n```bash\nbundle exec cap india:staging deploy:rake task=db:seed\n```\n\n### Deployment to a new environment\n\nWhen setting up a new environment to deploy Simple Server to, follow these steps.\n\n#### 1. Create a config file\n\nCreate a new file in `config/deploy/<env_name>.rb` for the new environment. It can be placed inside a subdirectory if\ndesired. Populate the new config file with relevant IP address info. Use an existing file for reference. For example,\nthe configuration for a deployment with two EC2 instances may look like:\n```\nserver \"ec2-12-111-34-45.ap-south-1.compute.amazonaws.com\", user: \"deploy\", roles: %w[web app db cron whitelist_phone_numbers seed_data]\nserver \"ec2-12-222-67-89.ap-south-1.compute.amazonaws.com\", user: \"deploy\", roles: %w[web sidekiq]\n```\n\nThe first server runs the web application and cron tasks, the second server runs Sidekiq to process background jobs.\n\n#### 2. Install Sidekiq\n\nA one-time installation of Sidekiq is required in new environments. Run the following command:\n\n```bash\nbundle exec cap <environment> sidekiq:install\n```\n\n#### 2. Deploy\n\nYou can now run a regular Capistrano deployment:\n\n```bash\nFIRST_DEPLOY=true bundle exec cap <environment> deploy\n```\n\nThis may take a long time for the first deployment, since several dependencies (like Ruby) need to be installed.\nSubsequent deployments will be much faster.\n\nNote that `FIRST_DEPLOY=true` only needs to be specified on the first run. Any deployments afterwards don't need the flag.\n\n### Deployment Resources\n\nThe infrastructure setup including the ansible and terraform scripts are documented in the [deployment repository](https://github.com/simpledotorg/deployment).\n\n## Contributing\n\nIf you're working on a project that will affect any of the indicators listed in [this document](https://docs.simple.org/), please contact the product / design team.\n"
 },
 {
  "repo": "greggersh/healthcare.gov",
  "language": "JavaScript",
  "readme_contents": "# HealthCare.gov-Open-Source-Release\n\nThis project includes the source code and content for the healthcare.gov website. For more information, please visit https://www.healthcare.gov/developers\n\n## Local Installation Requirements\n\n- Linux, Unix, Windows or Mac OS X\n- [Ruby](http://www.ruby-lang.org/en/downloads/)\n- [RubyGems](http://rubygems.org/pages/download)\n- [Jekyll](http://jekyllrb.com)\n\n\n## Ruby\n\n### To install ruby on unix:\n\n`yum install ruby` (or `sudo apt-get install ruby1.9.1`)\n\n\n### To install ruby on Mac OS X:\n\n`curl -L https://get.rvm.io | bash -s stable --ruby`\n\nVisit the following links for more detailed information on how to set up Ruby using a method applicable to your environment:\n\nThree Ways of Installing Ruby (Linux/Unix)\nhttp://www.ruby-lang.org/en/downloads/\n \nRubyInstaller for Windows\nhttp://rubyinstaller.org/\n\nHow to Install Ruby on a Mac\nhttp://net.tutsplus.com/tutorials/ruby/how-to-install-ruby-on-a-mac/\n\n\n## Install rubygems: \n\n- `cd ~/`\n- `wget http://production.cf.rubygems.org/rubygems/rubygems-1.8.24.tgz`\n- `tar xzvf rubygems-1.8.24.tgz`\n- `cd rubygems-1.8.24`\n- `ruby setup.rb`\n\n\n## Managing Dependencies Using Bundler\n\nWe recommend using Bundler to manage dependencies. Once you have Ruby installed, install Bundler by running the following command: 'gem install bundler'\n\nOnce Bundler is installed, you install/update depencies by simply running 'bundle install' within your project folder.\n\nMore information on Bundler may be found here: http://gembundler.com/\n\n\n## Install Jekyll\n\n- `cd healthcare.gov` (or the location of your cloned repository)\n- `bundle install`\n\nFor more information and detailed documentation on Jekyll, visit the following sites:\n\nJekyll Project Home\nhttp://jekyllrb.com\n\nJekyll on GitHub\nhttps://github.com/mojombo/jekyll\n\n\n## Clone the repository\n\n- `cd /var/www/html` (or the location you would like the compiled site to live)\n- `git clone https://github.com/CMSgov/HealthCare.gov-Open-Source-Release.git healthcare.gov`\n\n\n## Generate the site and serve\n\n- `jekyll serve`\n- Browse to [localhost:4000](http://localhost:4000) to view the site"
 },
 {
  "repo": "UCDS/health4all_v3",
  "language": "PHP",
  "readme_contents": "# Health4All\n\nContact On [gunaranjan@yousee.one]\n\nSoftwares to be installed\n    - WAMP / XAMPP Server\n    - Recommendation: SQLYog Community IDE [Instead of phpmyadmin if available and familiar for the programmer]\n\n\nSteps to be followed : \n    1. Create a Github account .\n    2. Fork (health4all_v3) from https://github.com/UCDS/health4all_v3.\n    3. Clone health4all_v3 from your forked respository to your system \n    4. Move this app to either www folder if you are using WAMP, or to htdocs in case of XAMPP server\n    5. Navigate to http://localhost/phpmyadmin/index.php  and create a new database with name 'health4all'.\n    6. Import the sql file from health4all_v3/db/health4all.sql into 'health4all' database created in above step.\n    7. Run the script SET GLOBAL sql_mode=''; in phpmyadmin sql editor\n    8. Open health4all_v3 app in browser (http://localhost/health4all_v3/) and login with below credentials\n        credentials-\n                username  : admin\n                password  : password  \n\nNote :  To hide the errors  , change environment from 'development' to 'production' health4all_v3/index.php line number 20 \n    \n \n\nDB Migration,\n    - After taking code pull, run all the files after the version mentioned in the table db_version\n    - To add new migration, create new version file & at the end of the sql file add db_version update query, rever 1.0.2.sql for reference.\n    - NOTE: DO NOT ADD DATABASE IN THE QUERIES.\n\n\nTODOs,\n    - DB Migration needs to be automated --> https://www.youtube.com/watch?v=i07XXM37VFk\n\nReferences,\n    - Database Migrations\n        - https://codeinphp.github.io/post/database-migrations-in-codeigniter/\n"
 }
]